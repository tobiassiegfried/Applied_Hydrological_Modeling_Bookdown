[["Chapter-DATA.html", "Chapter 3 Data Retrieval, Preparation &amp; Analysis", " Chapter 3 Data Retrieval, Preparation &amp; Analysis In this Chapter, we will discuss how to retrieve, prepare and process the data that is required for modeling. This includes in-situ station data, geospatial data, climate reanalysis data, and climate projections data. As will become clear, the preparation of these data requires a substantial amount of work, local storage space and, in some instances, computational power. With a focus on the generation of input files for hydrological-hydraulic modeling with RS Minerve, the flow diagram Figure ?? shows the required steps for the individual data types. These steps will be discussed in the individual Sections below in detail. "],["station-data.html", "3.1 Station Data", " 3.1 Station Data Much of key data visualization techniques have already been presented in the Chapter 2. Here, we are demonstrating important data preparation steps that should always precede modeling. These preparatory steps focus on data cleaning and gap filling. 3.1.1 Available Data The riversCentralAsia Package provides available data of the gauging and meteorological stations in the Chirchik River Basin1. Before starting any type of modeling, it is important to get a good understanding of the data that we are dealing with and whether there exist problems with the raw data that need to be addressed prior to modeling. Problems usually include data gaps and outliers as data records that one obtains are usually ever complete nor clean of errors. The steps performed here are thus required steps for any type of successful modeling and should be performed with great care prior to starting hydrological modeling. We concentrate our efforts here on discharge records and data from meteorological stations in the Chirchik River Basin. The techniques shown here for decadal (10-days) data naturally extend to monthly data and also, to data from other basins. 3.1.2 Gap Filling Discharge Data In the following, we will work with decadal discharge data from the two main tributaries, i.e. the Chatkal and (Gauge 16279) Pskem rivers (Gauge 16290) and the data of the inflow to the Charvak reservoir (Gauge 16924). The goal is to analyze the data and prepare for modeling. First, let us load the relevant discharge data. data &lt;- ChirchikRiverBasin # load data q_dec_tbl &lt;- data %&gt;% filter(code == &#39;16279&#39; | code == &#39;16290&#39; | code == &#39;16924&#39;) # Note for the new name of the object, we choose to add periodicity (_dec_) and data type (_tbl for tibble/dataframe) to the data name. This just helps to stay organized and is good practice in R programming. q_dec_tbl ## # A tibble: 9,072 x 14 ## date data norm units type code station river basin resolution ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; ## 1 1932-01-10 48.8 38.8 m3s Q 16279 Khudaydod Chatkal Chirch… dec ## 2 1932-01-20 48.4 37.5 m3s Q 16279 Khudaydod Chatkal Chirch… dec ## 3 1932-01-31 42.4 36.6 m3s Q 16279 Khudaydod Chatkal Chirch… dec ## 4 1932-02-10 43.7 36.4 m3s Q 16279 Khudaydod Chatkal Chirch… dec ## 5 1932-02-20 44.2 36.3 m3s Q 16279 Khudaydod Chatkal Chirch… dec ## 6 1932-02-29 47.7 36.9 m3s Q 16279 Khudaydod Chatkal Chirch… dec ## 7 1932-03-10 54.1 39.4 m3s Q 16279 Khudaydod Chatkal Chirch… dec ## 8 1932-03-20 63.2 47.6 m3s Q 16279 Khudaydod Chatkal Chirch… dec ## 9 1932-03-31 103 60.5 m3s Q 16279 Khudaydod Chatkal Chirch… dec ## 10 1932-04-10 103 86.4 m3s Q 16279 Khudaydod Chatkal Chirch… dec ## # … with 9,062 more rows, and 4 more variables: lon_UTM42 &lt;dbl&gt;, ## # lat_UTM42 &lt;dbl&gt;, altitude_masl &lt;dbl&gt;, basinSize_sqkm &lt;dbl&gt; You can get more information about the available data by typing ?ChirchikRiverBasin. Note that the original time series data has been packaged in this format by the riversCentralAsia::loadTabularData() function which takes a simple .csv file as input. It is advisable to check at this stage for missing data in time series and to fill gaps where present. Are there missing data? How can these be filled so as to arrive at complete time series that are required for hydrological modeling? As can be seen in Figure 3.1 , close inspection of the time series indeed reveals some missing data in the 1940ies. q_dec_tbl %&gt;% plot_time_series(date,data, .facet_vars = code, .smooth = FALSE, .interactive = TRUE, .x_lab = &quot;year&quot;, .y_lab = &quot;m^3/s&quot;, .title = &quot;&quot; ) Figure 3.1: Discharge data of selected gauges in the upstream zone of runoff formation in the Chirchik River Basin. Data Source: Uzbek Hydrometeorological Service. Note, Figure 3.1 and the following Figures are interactive, so you can zoom in to regions of interest. Missing data are also confirmed by the warning that the function timetk::plot_time_series() throws (suppressed here). Statistics of the missing data can be easily obtained. As the Table below shows, we can do this analysis for each discharge station separately. q_dec_tbl %&gt;% group_by(code) %&gt;% summarize(n.na = sum(is.na(data)), na.perc = n.na/n()*100) ## # A tibble: 3 x 3 ## code n.na na.perc ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 16279 15 0.496 ## 2 16290 39 1.29 ## 3 16924 42 1.39 Summarizing the number of observation with missing data reveals 15 data points for station 16279 (0.5 % of total record length) and 39 for station 16290 (1.3 % of total record length). As there are only very few gaps in the existing time series, we use a simple method to fill these. Wherever there is a gap, we fill in the corresponding decadal norm as stored in the norm column in the object q_dec_tbl. The visualization of the results confirms that our simple gap filling approach is indeed satisfactory (Figure 3.2). q_dec_filled_tbl &lt;- q_dec_tbl q_dec_filled_tbl$data[is.na(q_dec_filled_tbl$data)] = q_dec_filled_tbl$norm[is.na(q_dec_filled_tbl$data)] # Gap filling step q_dec_filled_tbl %&gt;% plot_time_series(date, data, .facet_vars = code, .smooth = FALSE, .interactive = TRUE, .x_lab = &quot;year&quot;, .y_lab = &quot;m^3/s&quot;, .title = &quot;&quot; ) Figure 3.2: Gap filled Pskem and Chatkal river discharges. All missing data are gone now. q_dec_filled_tbl %&gt;% group_by(code) %&gt;% summarize(n.na = sum(is.na(data)), na.perc = n.na/n()*100) ## # A tibble: 3 x 3 ## code n.na na.perc ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 16279 0 0 ## 2 16290 0 0 ## 3 16924 0 0 A note of caution here. This simple gap filling technique reduces variance in the time series. It should only be used when the percentage of missing data is low. As will be discussed in the next Section 3.1.3 below, better techniques have to be utilized when there exist substantial gaps and in the case of less regular data. Finally, we discard the norm data which we used for gap filling of the missing discharge data and convert the data to wide format (see the Table below) to add to it meteorological data in the next Section. q_dec_filled_wide_tbl &lt;- q_dec_filled_tbl %&gt;% # again we use the name convention of objects as introduced above mutate(code = paste0(&#39;Q&#39;,code %&gt;% as.character())) %&gt;% # Since we convert everything to long form, we want to keep information as compact as possible. Hence, we paste the type identifier (Q for discharge here) in from of the 5 digit station code. dplyr::select(date,data,code) %&gt;% # ... and then ditch all the remainder information pivot_wider(names_from = &quot;code&quot;,values_from = &quot;data&quot;) # in order to pivot to the long format, we need to make a small detour via the wide format. q_dec_filled_long_tbl &lt;- q_dec_filled_wide_tbl %&gt;% pivot_longer(-date) # and then pivot back q_dec_filled_wide_tbl ## # A tibble: 3,024 x 4 ## date Q16279 Q16290 Q16924 ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1932-01-10 48.8 38.3 87.1 ## 2 1932-01-20 48.4 37.7 86.1 ## 3 1932-01-31 42.4 36.2 78.6 ## 4 1932-02-10 43.7 35.6 79.3 ## 5 1932-02-20 44.2 35 79.2 ## 6 1932-02-29 47.7 37.1 84.8 ## 7 1932-03-10 54.1 43.1 97.2 ## 8 1932-03-20 63.2 47 110 ## 9 1932-03-31 103 72.1 175 ## 10 1932-04-10 103 73.2 176 ## # … with 3,014 more rows As a result, we now have a complete record of decadal discharge data for the two main tributaries of the Chirchik river and the inflow time series to Charvak Reservoir from the beginning of 1932 until and including 2015, i.e. 84 years. The same type of preparatory analysis will now be carried out for the meteorological data. 3.1.3 Gap Filling Meteorological Data Here, we use precipitation and temperature data from Pskem (38462), Chatkal (38471) and Charvak Reservoir (38464) Meteorological Stations (see Chapter ?? for more information on these stations). We also have data from Oygaing station (Station Code 38339) but the record only starts in 1962 and the time resolution is monthly. Therefore, we do not take this station into account here for the time being. We start with precipitation and plot the available data. p_dec_tbl &lt;- data %&gt;% filter(type==&quot;P&quot; &amp; code!=&quot;38339&quot;) p_dec_tbl %&gt;% plot_time_series(date,data, .facet_vars = code, .interactive = TRUE, .smooth = FALSE, .title = &quot;&quot;, .y_lab = &quot;mm/decade&quot;, .x_lab = &quot;year&quot; ) Figure 3.3: Raw decadal precipitation data from Pskem (38462), Charvak Reservoir (38471) and Chatkal Meteo Station (38471). The precipitation data from these 3 stations shows some significant data gaps. The Chatkal Meteorological Station that is located in Kyrgyzstan apparently did not work in the post-transition years and continuous measurements were only resumed there in 1998. Let us see what happens if we were to use the same simple gap filling technique that we introduced above for discharge. p_dec_filled_tbl &lt;- p_dec_tbl p_dec_filled_tbl$data[is.na(p_dec_filled_tbl$data)] = p_dec_filled_tbl$norm[is.na(p_dec_filled_tbl$data)] p_dec_filled_tbl %&gt;% plot_time_series(date,data, .facet_vars = code, .interactive = TRUE, .smooth = FALSE, .title = &quot;&quot;, .y_lab = &quot;mm/decade&quot;, .x_lab = &quot;year&quot; ) Figure 3.4: Precipitation Data gap-filled with norms. The filled values from 1990 - 2000 in the case of the Station 38471 indicate that the norm-filling technique is not good. Closely inspect the significant data gap in the 1990ies at Station 38741 (tip: play around and zoom into the time series in the 1990ies in Figure 3.3 and comparing it with the resulting gap-filled timeseries in Figure ??. We see that our technique of gap filling with long-term norms is not suitable for this type of data and the significant gap size. The effect of variance reduction is also clearly visible. Hence, we resort to a more powerful gap filling technique that uses a (regression) model to impute the missing values from existing ones at the neighboring stations, i.e. Stations 38462 and 38464. To do so, we utilize the simputation R package that is tightly integrated in the tidyverse2. library(simputation) # First, we bring the data into the suitable format. p_dec_wide_tbl &lt;- p_dec_tbl %&gt;% mutate(code = paste0(&#39;P&#39;,code %&gt;% as.character())) %&gt;% dplyr::select(date,data,code) %&gt;% pivot_wider(names_from = &quot;code&quot;,values_from = &quot;data&quot;) # Second, we impute missing values. p_dec_filled_wide_tbl &lt;- p_dec_wide_tbl %&gt;% impute_rlm(P38471 ~ P38462 + P38464) %&gt;% # Imputing precipitation at station 38471 using a robust linear regression model impute_rlm(P38462 ~ P38471 + P38464) %&gt;% # Imputing precipitation at station 38462 using a robust linear regression model impute_rlm(P38464 ~ P38462 + P38471) # Imputing precipitation at station 38464 using a robust linear regression model p_dec_filled_long_tbl &lt;- p_dec_filled_wide_tbl %&gt;% pivot_longer(c(&#39;P38462&#39;,&#39;P38464&#39;,&#39;P38471&#39;)) p_dec_filled_long_tbl%&gt;% plot_time_series(date,value, .facet_vars = name, .interactive = TRUE, .smooth = FALSE, .title = &#39;&#39;, .y_lab = &quot;mm/decade&quot;, .x_lab = &quot;year&quot; ) (#fig:rawData_P_rlm)Precipitation Data gap filled with a robust linear regression modeling approach As you can see, we use simple linear regression models to impute missing value in the target time series using observations from the neighboring stations. Through simple visual inspection, it becomes clear that this type of regression model for gap filling is better suited than the previous approach chosen. Let us check whether we could successfully fill all gaps with this robust linear regression approach. p_dec_filled_long_tbl %&gt;% group_by(name) %&gt;% summarize(n.na = sum(is.na(value)), n.na.perc = n.na/n()*100) ## # A tibble: 3 x 3 ## name n.na n.na.perc ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 P38462 12 0.402 ## 2 P38464 12 0.402 ## 3 P38471 3 0.100 It turns out that we still have very few gaps to deal with. We can see them by simply visualizing the wide tibble. The problem persisted at times when two or more values were missing across the available stations at the same time and where thus the linear regression could not be carried out. p_dec_filled_wide_tbl %&gt;% head(10) ## # A tibble: 10 x 4 ## date P38462 P38464 P38471 ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1933-01-10 NA NA 2 ## 2 1933-01-20 NA NA 10 ## 3 1933-01-31 NA NA 5 ## 4 1933-02-10 NA NA 33 ## 5 1933-02-20 NA NA 8 ## 6 1933-02-28 NA NA 10 ## 7 1933-03-10 NA NA 31 ## 8 1933-03-20 NA NA 50 ## 9 1933-03-31 NA NA 6 ## 10 1933-04-10 23 21.3 13 p_dec_filled_wide_tbl %&gt;% tail() ## # A tibble: 6 x 4 ## date P38462 P38464 P38471 ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2015-11-10 72 81 19 ## 2 2015-11-20 122 76 43 ## 3 2015-11-30 7 2 3 ## 4 2015-12-10 NA NA NA ## 5 2015-12-20 NA NA NA ## 6 2015-12-31 NA NA NA We can solve the issues related to the missing values at the start of the observation record by using the same technique as above and by only regressing P38462 and P38464 on P38471. p_dec_filled_wide_tbl &lt;- p_dec_filled_wide_tbl %&gt;% impute_rlm(P38462 ~ P38471) %&gt;% # Imputing precipitation at station 38462 using a robust linear regression model impute_rlm(P38464 ~ P38471) # Imputing precipitation at station 38464 using a robust linear regression model p_dec_filled_wide_tbl %&gt;% head(10) ## # A tibble: 10 x 4 ## date P38462 P38464 P38471 ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1933-01-10 5.60 5.08 2 ## 2 1933-01-20 18.3 16.7 10 ## 3 1933-01-31 10.4 9.46 5 ## 4 1933-02-10 54.9 50.3 33 ## 5 1933-02-20 15.2 13.8 8 ## 6 1933-02-28 18.3 16.7 10 ## 7 1933-03-10 51.8 47.3 31 ## 8 1933-03-20 82.0 75.0 50 ## 9 1933-03-31 12.0 10.9 6 ## 10 1933-04-10 23 21.3 13 Converse to this, the complete set of observations is missing for December 2015. We will thus remove these non-observations from our tibble. p_dec_filled_wide_tbl &lt;- p_dec_filled_wide_tbl %&gt;% na.omit() p_dec_filled_wide_tbl %&gt;% tail() ## # A tibble: 6 x 4 ## date P38462 P38464 P38471 ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2015-10-10 5 1 0 ## 2 2015-10-20 89 108 58 ## 3 2015-10-31 34 40 12 ## 4 2015-11-10 72 81 19 ## 5 2015-11-20 122 76 43 ## 6 2015-11-30 7 2 3 p_dec_filled_long_tbl &lt;- p_dec_filled_wide_tbl %&gt;% pivot_longer(-date) Inspecting the temperature data, we see similar data issues as in the precipitation data set. t_dec_tbl &lt;- data %&gt;% filter(type==&quot;T&quot;) t_dec_tbl %&gt;% plot_time_series(date,data, .facet_vars = code, .interactive = TRUE, .smooth = FALSE, .title = &#39;&#39;, .y_lab = &quot;deg. Celsius&quot;, .x_lab = &quot;year&quot; ) (#fig:rawData_T)Raw temperature data from the meteorological stations Pskem (38462) and Chatkal (38471) # First, we bring the data into the suitable format. t_dec_wide_tbl &lt;- t_dec_tbl %&gt;% mutate(code = paste0(&#39;T&#39;,code %&gt;% as.character())) %&gt;% dplyr::select(date,data,code) %&gt;% pivot_wider(names_from = &quot;code&quot;,values_from = &quot;data&quot;) # Second, we impute missing values. t_dec_filled_wide_tbl &lt;- t_dec_wide_tbl %&gt;% impute_rlm(T38471 ~ T38462) %&gt;% # Imputing precipitation at station 38471 using a robust linear regression model impute_rlm(T38462 ~ T38471) # Imputing precipitation at station 38462 using a robust linear regression model t_dec_filled_long_tbl &lt;- t_dec_filled_wide_tbl %&gt;% pivot_longer(c(&#39;T38462&#39;,&#39;T38471&#39;)) t_dec_filled_long_tbl%&gt;% plot_time_series(date,value, .facet_vars = name, .interactive = TRUE, .smooth = FALSE, .title = &#39;&#39;, .y_lab = &quot;deg. Celsius&quot;, .x_lab = &quot;year&quot; ) (#fig:rawData_T_rlm)Temperature data gap filled with robust linear regression modeling. There are some irregularities in the temperature time series of Chatkal Meteorological Station in the first decade of the 20th century (tip: zoom in to see these more clearly). Note that these were not introduced by the gap filling technique that we used but are most likely wrong temperature readings. We will return to these in the outlier analysis below in Section 3.1.4. t_dec_filled_long_tbl %&gt;% group_by(name) %&gt;% summarize(n.na = sum(is.na(value)), n.na.perc = n.na/n()*100) ## # A tibble: 2 x 3 ## name n.na n.na.perc ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 T38462 3 0.100 ## 2 T38471 3 0.100 To see where the missing value are, we find them easily again by looking at the head and tail of the tibble. t_dec_filled_wide_tbl %&gt;% head() ## # A tibble: 6 x 3 ## date T38462 T38471 ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1933-01-10 -6.9 -16.6 ## 2 1933-01-20 -6.1 -15.5 ## 3 1933-01-31 -6.3 -15.6 ## 4 1933-02-10 -2 -8.6 ## 5 1933-02-20 -3.3 -12.5 ## 6 1933-02-28 -0.1 -8.5 t_dec_filled_wide_tbl %&gt;% tail() ## # A tibble: 6 x 3 ## date T38462 T38471 ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2015-11-10 2.4 -2.5 ## 2 2015-11-20 2 -2.2 ## 3 2015-11-30 4.6 -3.7 ## 4 2015-12-10 NA NA ## 5 2015-12-20 NA NA ## 6 2015-12-31 NA NA Finally, we remove the non observations again as above with the function na.omit. t_dec_filled_wide_tbl &lt;- t_dec_filled_wide_tbl %&gt;% na.omit() t_dec_filled_long_tbl &lt;- t_dec_filled_wide_tbl %&gt;% pivot_longer(-date) To deal with the missing values at the end of the observational record, we could also have used any other technique. Using the norm values however would have artificially reduced the variance in both cases as explained above. Furthermore and at least in the case of temperature, it is also questionable to what extent a norm calculated over the last 84 years is still representative given global warming. We will look in this important and interesting topic in the next section. 3.1.4 Anomalies and Outliers We use the function timetk::plot_anomaly_diagnostics to investigate anomalies in the time series. For discharge, we first log-transform the raw data with the following transformation to reduce the variance of the original data. \\[ \\hat{q}(t) = log(q(t) + 1) \\] where \\(\\hat{q}(t)\\) denotes the transformed discharge. Prior to the log transformation, 1 is added so as to avoid cases where discharge would be 0 and the logarithmic transform thus undefined. The transformation can easily be done with the log1p() function in R. Backtransformation via the function expm1() simply involves taking the exponent and subtracting 1 from the result. Figure ?? shows the result. The exceptionally wet year 19169 shows up as anomalous in the Chatkal River Basin and at the downstream Charvak Reservoir inflow gauge. , ?? and ?? show anomalies diagnostics of the available data. q_dec_filled_long_tbl %&gt;% plot_anomaly_diagnostics(date, value %&gt;% log1p(), .facet_vars = name, .frequency = 36, .interactive = TRUE, .title = &quot;&quot;) Figure 3.5: Anomaly diagnostics of discharge data. The transparent grey band shows the width of the normal range. The highly anomalous wet year of 1969 is clearly visible in the discharge record of the Chatkal river basin (Station 16279). The investigation of precipitation anomalies shows a succession of regular anomalous wet events over time. It is interesting to see that the winter 1968/69 regularly anomalous at all three stations (Figure 3.6, zoom in to investigate). p_dec_filled_long_tbl %&gt;% plot_anomaly_diagnostics(date, value, .facet_vars = name, .interactive = TRUE, .title = &quot;&quot;) Figure 3.6: Anomaly diagnostics of precipitation data. While intuitively, we would have expected an eceptionally mild winter in 1968/69 due to the precipitation excess, the corresponding anomaly does not show up in the temperature record (Figure 3.7). t_dec_filled_long_tbl %&gt;% plot_anomaly_diagnostics(date,value, .facet_vars = name, .interactive = TRUE, .title = &quot;&quot;) Figure 3.7: Anomaly diagnostics of temperature data. Apart from the identification of extremal periods since as the 1969 discharge year in the Chatkal river basin, the diagnostics of anomalies also helps to identify likely erroneous data records. In Figure @ref(anomalies_T) for example, when we zoom into the data of the series T38471 in the first decade of the 21st century, problems in relation to positive anomalies during the winter are visible in 4 instances. One explanation would be that in at least some instances, the data are erroneously recorded as positive values when in fact they were negative (see dates ‘2002-01-31,’ ‘2005-01-10’ and ‘2007-02-28,’ Chatkal Station 38471). 3.1.5 Putting it all together Finally, we are now in the position to assemble all data that we will use for empirical modeling. The data is stored in long and wide form and used accordingly where required. For example, in Section @ref{TimeSeriesReg}, we are working with the wide data format to investigate model features in linear regression. Note that we also add a column with a decade identifier. Its use will become apparent in the Section 5.5 below. # Final concatenation data_wide_tbl &lt;- right_join(q_dec_filled_wide_tbl,p_dec_filled_wide_tbl,by=&#39;date&#39;) data_wide_tbl &lt;- right_join(data_wide_tbl,t_dec_filled_wide_tbl,by=&#39;date&#39;) # Add period identifiers (decades in this case) s &lt;- data_wide_tbl$date %&gt;% first() e &lt;- data_wide_tbl$date %&gt;% last() decs &lt;- decadeMaker(s,e,&#39;end&#39;) decs &lt;- decs %&gt;% rename(per=dec) data_wide_tbl &lt;- data_wide_tbl %&gt;% left_join(decs,by=&#39;date&#39;) # Creating long form data_long_tbl &lt;- data_wide_tbl %&gt;% pivot_longer(-date) # Cross checking completeness of record data_long_tbl %&gt;% group_by(name) %&gt;% summarize(n.na = sum(is.na(value)), n.na.perc = n.na/n()*100) ## # A tibble: 9 x 3 ## name n.na n.na.perc ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 P38462 0 0 ## 2 P38464 0 0 ## 3 P38471 0 0 ## 4 per 0 0 ## 5 Q16279 0 0 ## 6 Q16290 0 0 ## 7 Q16924 0 0 ## 8 T38462 0 0 ## 9 T38471 0 0 A consistent data record from 1933 until and including November 2015 is now prepared^Please note that by using left_join above, we have cut off discharge data from the year 1932 since we do not have meteorological data there.^. Let us analyze these data now. Where other data are used, their source and access options are indicated.↩︎ Please note that if you do not have the required package installed locally, you should install it prior to its use with the following command install.packages('simputation')↩︎ "],["geospatial-data.html", "3.2 Geospatial Data", " 3.2 Geospatial Data This Section follows the steps shown in Figure ?? and explains in a hands-on manner how to arrive at and process the required geospatial data for later inclusion in the hydrological-hydraulic model RS MINVERVE. Links to detailed descriptions of the individual tasks allow even beginners of QGIS to follow the procedure. 3.2.1 Setting up QGIS Open and save a new QGIS project (How to for absolute beginners). Make sure that the maps projection is in UTM (How to change the projection of a project). The needed plugins are: qProf or profile tool (How to install and activate a plugin). The optional plugins are: SRTM plugin (requires a login to USGS Earth Explorer How to register). Note, SAGA routines must be installed and correctly configured in QGIS. 3.2.2 Load DEM We use the SRTM 1 arc-second global DEM (Data description). The DEM tiles are downloaded directly in QGIS3 (SRTM plugin, How to) or via the USGS Earth Explorer (How to) and merged to a single GIS layer (How to). In any case you will need to register for an Earth Explorer account How to register. 3.2.3 Derive the boundaries of your catchment If you already have a shapefile for the boundaries of your catchment. Load it into QGIS How to add a vector layer. Otherwise, follow this tutorial on youtube to derive the boundaries of your catchment. 3.2.4 Run through the process model A process model is available that generates the raw GIS layers for RS Minerve. Navigate to Processing and open the Graphical Modeller…. The Model Designer window opens. Open the model Link to Model (3.8). Figure 3.8: The process of deriving the GIS layers for RS Minerve is simplified through the process model. *TODO: Make file available for download. Prior to running the model, read through the following brief notes. 3.2.4.1 Input files A shape file of the basin outline in UTM 42 N (How to derive the basin outline). The extent of the DEM needs to be larger than the boundaries of your catchment. You can use your raw SRTM DEM projected to UTM42N. 3.2.4.2 Model output Besides the 3 layers required (Basins, rivers, junctions, ?? in the Introduction), the model produces a number of additional output files which can be used for verifying the parameterisation which is described in more detail in the next section. 3.2.4.3 Parameterization of the model BasinShapeBuffer_meters is a buffer around the basin outline to which the DEM is cut. The DEM needs to be slightly larger than the shape line. The default value of 1000 m is sufficient. Can this parameter be internalised? River Network Level is a parameters for the resolution of the river network. Values of 7 and 8 have shown good results for smaller and medium sized catchments (3.9). Figure 3.9: Comparison of River Network Level = 7 (left) and River Network Level = 8 (rigth) for the example of the Pskem river catchment. The Channel Network Cutoff Value is a parameter required for the partitioning of the catchment into sub-catchments. Values between 2e8 and 5e8 have been found to work well for smaller and medium sized catchments. The model is sensitive to this parameter and might throw an error if the cutoff value is not appropriate. The Elevation Bands Table holds the altitude boundaries and class IDs for the elevation bands in the catchment. After a first test run, edit according to your needs. The model is sensitive to these parameters. Play around with them until you are satisfied with the resulting GIS layers. If you get an error message running Fix geometries that POLYGONS.shp was not found, try reducing the Channel Network Cutoff Value by 50-70%. You can still inspect the other layers and change other parameters as well. - Make sure that all input layers are projected to UTM 42 N. - Run the model through once, inspect the output and play with the input parameters: - Min and Max elevations on the smoothed DEM and the shape, number and spacing of elevation bands (edit the table for the elevation bands accordingly), - Location and shape of river reaches and the location of junctions (vary the threshold). 3.2.5 Manually edit the GIS Layers for import to RS Minerve Review ?? Elevation bands per sub-basin need to be in one Multi-Polygon layer. We need junctions only at the confluences of sub-basins (and river reaches). We only need river reaches where we need to do routing. The sub-basins at the upper-most reaches do not need river reaches. 3.2.5.1 Edit Junctions layer Select the Junctions layer and toggle manual editing by clicking on the yellow pen (3.10). Figure 3.10: Manually edit the layer with the river junctions, step 1: Toggle layer editing. When in editing mode, the yellow pen will appear in the Layers window next to the name of the layer being edited. The edit mode will also activate a button for adding points (i.e. junctions, we don’t need that now) and the vertex tool. Click on the vertex tool icon. It is active when a a boundary appears around the icon and the Vertex Editor windows opens (3.11). Figure 3.11: Manually edit the layer with the river junctions, step 2: Activate the vertex tool. Right-click on a junction point you would like to delete to activate it (3.12). Figure 3.12: Manually edit the layer with the river junctions, step 3: Activate a junction node for editing. Select the activated point by drawing a rectangle over the point with your mouse. The point will appear blue (3.13). Figure 3.13: Manually edit the layer with the river junctions, step 3: Select the activated junction node. Delete the point with the delete key on your keyboard. You can save your edits by pressing the blue-white Save Layer Edits button that is decorated with an orange pen (3.14). This saves your changes without exiting the edit mode. Figure 3.14: Manually edit the layer with the river junctions, step 3: Save edits. If you have many points to remove, as in our case, it may be faster to identify the IDs of the features you want to keep, select these and delete all others. To start, you activate the Identify Features mode by clicking on the icon with the white i on the blue circle (3.15). Figure 3.15: Alternative method to manually edit junctions if many nodes need to be deleted. Step 1: Get ID of features to keep, part 1. A black i will appear next to your cursor. You then click on the first of your nodes that you want to keep. This will highlight it in red and a list with information on the selected feature appears on the right in the Identify Results window. You will see the attribute NODE_ID with value 1 for the outflow node (3.16). Note down the ID of the feature. Figure 3.16: Alternative method to manually edit junctions if many nodes need to be deleted. Step 1: Get ID of features to keep, part 2. You then press on the node at the confluence of the two tributaries in the center of the catchment. The Identify Results window shows 2 results, that means, that two junction nodes are close to each other (3.17). Figure 3.17: Alternative method to manually edit junctions if many nodes need to be deleted. Step 1: Get ID of features to keep, part 3. Zoom in in your map window with your mouse to see the two nodes (3.18). Figure 3.18: Alternative method to manually edit junctions if many nodes need to be deleted. Step 1: Get ID of features to keep, part 4. Select the node that should be kept and not the ID of the node (NODE_ID 11), (3.19). Figure 3.19: Alternative method to manually edit junctions if many nodes need to be deleted. Step 1: Get ID of features to keep, part 5. Zoom back to the entire Junctions layer (see ??). and go to Select Features by Values… in the toolbar (3.20). Figure 3.20: Alternative method to manually edit junctions if many nodes need to be deleted. Step 2: Select features to delete, part 1 Add NODE_ID 1 to your selection as demonstrated in (3.21). Figure 3.21: Alternative method to manually edit junctions if many nodes need to be deleted. Step 2: Select features to delete, part 2 The outflow node with ID 1 will change color in your map (3.22). Figure 3.22: Alternative method to manually edit junctions if many nodes need to be deleted. Step 2: Select features to delete, part 3 Add node 11 to your selection in the same way and close the Select Node by Value window. Invert the feature selection as shown in (3.23). Figure 3.23: Alternative method to manually edit junctions if many nodes need to be deleted. Step 2: Select features to delete, part 4 All other nodes will now be yellow and the ones to keep will appear in the layer color (3.24). Figure 3.24: Alternative method to manually edit junctions if many nodes need to be deleted. Step 2: Select features to delete, part 5 Delete the features (nodes) by pressing the Delete Selected button in the edit features toolbar (3.25). Figure 3.25: Alternative method to manually edit junctions if many nodes need to be deleted. Step 2: Select features to delete, part 6 Save your edits (3.14). To verify that, indeed, all superfluous nodes are deleted from the Junctions file, open the Attribute table (3.26). Figure 3.26: Edit Attribute table. Step 1: Open the attribute table with a click on the Attribute Table icon in the QGIS toolbar. Only 2 features should be listed under each attribute. We will now edit the attribute table to prepare it for the RS Minerve model (see ??). RS Minerve needs an identifyer to differentiate between the junctions. We can use the attribute TYPE to uniquely identify the two junctions needed. RS Minerve further needs the ID of the downstream river. Add a collumn to the attribute table by pressing the Add Field button (3.27). Figure 3.27: Edit Attribute table. Step 2: Add a column to the attribute table manually, part 1. Define a name for the attribute, a type and admissible length of each entry in the Add Field window. In our case, we choose a string (a word) as ID and allow it to be 20 characters long (3.28). Figure 3.28: Edit Attribute table. Step 2: Add a column to the attribute table manually, part 2. Close the window by pressing OK. By clicking in the newly created NULL fields, you can now type names for the downstream rivers and save your edits by pressing the save edits icon (3rd from the left in the toolbar of the attribute table window). As the outlet of the catchment goes directly into Charvak reservoir, we can type Charvak as the downstream river ID. The river stretch between junction and outlet is called Pskem (3.29). Figure 3.29: Edit Attribute table. Step 2: Add a column to the attribute table manually, part 3. We are done editing the Junctions layer. Deactivate the edit mode by clickig on the yellow pen in the attribute table window (3.30) and close the window. Figure 3.30: Save your edits. Now save the Junctions layer in an appropriate place on your drive and save your project. 3.2.5.2 Edit Channels layer Proceed in the same way as for the junctions: Delete channel sections that are not needed in the Minerve model. "],["climate-reanalysis-data.html", "3.3 Climate Reanalysis Data", " 3.3 Climate Reanalysis Data Information about the spatio-temporal distribution of precipitation (P) and temperature (T) is vital for water balance studies, including for modeling. Poorly gauged basins do not have dense enough ground-based monitoring network that would allow to obtain reliable meteorological fields that can be used to drive hydrological models. Station data are especially poor in complex and remote mountain catchments in developing and transition countries. The Central Asian river basins are examples of such basins. Global reanalysis data can help to cover these existing gaps. In this Chapter, we show how this can be achieved. We use ERA5 global reanalysis data to obtain temperature at 2 meters above ground and total precipitation fields on an hourly base from 1981-01-01 through 2020-12-31. ERA5 data comes with a 30km grid cell size resolution and is thus quite course. This is relevant for complex mountain terrains which feature highly variable climate over very short distances, sometimes from one valley to the next. To address this problem, a bias correct version of the monthly CHELSA high resolution climatology product is used to arrive at high resolution hourly climatology fields (the CHELSA dataset is described in (Karger et al. 2017) and the bias correction of it in (Beck et al. 2020a). Like this, high-resolution hourly data from 1981-01-01 through 2013-12-31, i.e. the period for which the CHELSA dataset is available, could be derived. These data are then used for model calibration and validation and for the computation of the reference hydro-climatological situation from 1981 through 2013. More details can be found in Chapter 4. The analysis present here is for the Gunt river catchment. The approach and methods we have developed are universally applicable for other catchments. 3.3.1 Original V1.2.1 Data and PBCORR CHELSA Data The CHELSA V1.2.1 data covers the period 1981-01-01 until 2013-12-31. Mean monthly temperature and total monthly precipitation over this time frame can be accessed and downloaded for the Central Asia domain from the Github Repository in the /data/CentralAsiaDomain/ folder. We define the Central Asia domain as aoi_CentralAsia_LatLon &lt;- extent(c(65,80.05,35.95,44.05)) # this is a raster::extent() object. For more information, type ?extent into the console. The CHELSA data are downscaled ERA-INTERIM model outputs for temperature and precipitation with a resolution of 30 arc seconds (Karger et al. 2017). Temperature fields are statistically downscaled ERA-INTERIM temperatures whereas for precipitation downscaling, several orographic predictors are taken into account, including for example wind fields, valley exposition, height of the atmospheric boundary layer, etc. Because of its highly resolved climatologies, the CHELSA data has been shown to be particularly useful for studies in regions with complex topography. However, it has also been shown that the original CHELSA precipitation data is underestimating high mountain precipitation. This can be explained by the phenomenon of snow undercatch which explains measured precipitation deficits by sublimation and blowing snow transport at high altitude meteorological stations. An example of this is shown in Figure 3.31 for high elevation gauges in Spain. In a recent intercomparison project carried out in Spain, it has been shown that undercatch poses significant problems in accurately measuring solid precipitation (Buisán et al. 2017) in mountainous regions. Both, ERA-INTERIM and CHELSA themselves assimilate station data in their models and hence are affected by these wrong measurements. Figure 3.31: Measured snow undercatch values in high-mountain stations in Spain. The values were determined within the World Meteorological Organization Solid Precipitation Intercomparison Experiment (WMO-SPICE). See text for more information and reference. Beck et al. (2020b) has recognized this and released monthly correction factors for the CHELSA data. (#fig:CHELSA_biasCorrectionFactors)Figure from (Beck et al. 2020b) We can easily compute and plot monthly norm climatologies for the Central Asia domain using these data. fDir &lt;- &#39;./data/CentralAsiaDomain/CHELSA_V1.2.1/&#39; # First, we process and display mean monthly 2 meters temperatures. varDir &lt;- &#39;t2m/&#39; t2m_files &lt;- list.files(paste0(fDir,varDir)) months &lt;- month.abb # Get monthly fields and average over all data for resulting climatology #for idx (1:12:) 3.3.2 ERA5 Data Resampling with PBCORR CHELSA ERA5 data will not be uploaded on GitHUB because they are simply too big to handle (30 GB t2m and 30 GB tp). Due to the LFS storage and data traffic restrictions, this is not a vidable model here. We leave them on Dropbox "],["climate-projections.html", "3.4 Climate Projections", " 3.4 Climate Projections "],["snow-cover-data-optional.html", "3.5 Snow Cover Data (optional)", " 3.5 Snow Cover Data (optional) "]]
