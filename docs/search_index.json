[["index.html", "Applied Modeling of Hydrological Systems in Central Asia Welcome", " Applied Modeling of Hydrological Systems in Central Asia Tobias Siegfried 2021-04-07 Welcome Welcome to the online course book on Central Asian hydrology and the mathematical modeling of the hydrological systems there. The book is dedicated to the young and aspiring water professionals of the region. The book adopts an open-source philosophy and promotes the use of open-source data and software. The online version is hosted at https://tobiassiegfried.github.io/HydrologicalModeling_CentralAsia/, maintained via GitHub and currently work in progress. "],["preface.html", "Preface", " Preface This handbook is geared towards young water professionals in Central Asia. As they now graduate, they inherit an extraordinarily complex system of man made irrigation and hydropower infrastructure that is gradually aging. At the same time, they enter a sector that has enabled the region to prosper and flourish over hundreds if not thousands of years. Finally, they face work where opportunities for modernization abound after decades of inadequate investments. It is the hope of the author that this text provides a source of inspiration for this target group and that the text and the methods presented will also be used by teachers and integrated in university curricula locally. The author is grateful for the support by the Global Water Programme at the Swiss Agency for Development and Cooperation and especially to Stephanié Piers de Raveschroot and André Wehrli there. The author owes a lot to Andrey Yakovlev who provided invaluable guidance throughout his professional career. Finally, this book is dedicated to our past, current and future colleagues at the Central Asian Hydrometeorological Agencies whose tireless work in collecting and analyzing hydro-meteorological data in Central Asia has helped to greatly improve our understanding of the complex runoff generation processes at work in the region. Tobias Siegfried, hydrosolutions GmbH Zurich, February 2021 "],["foreword.html", "Foreword", " Foreword Oasis between Samarkand and Buchara. Source: Tobias Siegfried, hydrosolutions GmbH. This is a online book about applied hydrological modeling. It is geared towards students and young professionals in Central Asia who are interested in learning modern modeling approaches. The book teaches by examples and uses two catchments from the Syr Darya and Amu Darya river basins as case studies. While the presented case studies are exclusively from Central Asia, the methods demonstrated can be applied elsewhere. First, key hydro-climatological characteristics of the region are presented. This Section draws heavily on Victor Shults’ “Rivers of Middle Asia” and presents relevant materials from this famous book in a modern way. Two hydrological basins are further highlighted in in-depth case studies, i.e. the Gunt River in the Amu Darya catchment and the Chirchik river basin as the biggest right tributary to Syr Darya. The analyses of these catchments draws on available data from the Central Asian Hydrometeorological Services. The analysis of the available hydro-meteorological data uses different types of methods implemented in R which is a programming language widely used in data analysis and mining (R Core Team 2013). Three different types of modeling approaches are demonstrated and discussed in greater depth. First, long-term water balance modeling using the Budyko framework is introduced to discuss and demonstrate the quantification of climate change impacts on hydrological systems in Central Asia. Using this type of model, the effects of climate change on the long-term water balance are discussed for the Gunt River. The Gunt river is one of the key right tributaries to the Pjandz, i.e. the upstream Amu Darya, and emerges at high altitudes in the eastern Pamirs. Second, a more detailed modeling approach using semi-distributed, lumped, conceptual hydrologic-hydraulic modeling will be used to demonstrate how intra-annual changes due to climate forcing and changes therein can be quantified. These approaches will be demonstrated again for the Gunt catchment and also for the Chirchik river basin and its tributaries. Furthermore, the use of these models to study different kinds of reservoir operations will be presented and impact on hydropower production as well as on downstream water availability discussed. Third and finally, empirical modeling will be introduced for forecasting. These types of models are currently utilized in the Hydrometeorological Agencies of the region for forecasting discharge at different lead times, ranging from one day ahead up to seasonal forecasts. These models use long term time series to learn relationships between past, observed quantities and future system responses. With everything that is presented, the focus is on the use of open source and free software. For data preparation and analysis as well as for water balance and empirical modeling, R and RStudio are utilized (R Core Team 2013; Team’ 2020). For the processing of geographic data, workflows in QGIS are demonstrated (QGIS Development Team 2021). For hydrological-hydraulic modeling, the free RS MINERVE is utilized which is a environment for the modeling of free surface runoff flow formation and propagation (Foehn et al. 2020; Garcia Hernandez et al. 2020). The reader is expected to have a basic understanding about R and QGIS and how to use these software for data analysis and processing. The R code to produce figures, conduct analyses as well carry out empirical modeling is deliberately provided throughout the text and embedded there. While this makes the text heavier than necessary, it gives the local target group to reuse code and directly reproduce results while working on their own problems. An attempt has been made to make everything as reproducible as possible. "],["ShortHistory.html", "Chapter 1 Short History of Water in Central Asia", " Chapter 1 Short History of Water in Central Asia Still from a propaganda film documenting the construction of the Big Fergana Canal. The canal was constructed in 1939 constructed over a period of 45 days and a length of approx. 280 kilometers. Source: Youtube This Chapter provides a short introduction to the region of how man has tamed and allocated the Central Asian water resources. It draws on texts from an emerging field of the study of colonial times in the region and how the Zarist and Russian conquests effectuated dramatic changes in the use and allocation of water resources. These changes, however, as is becoming more and more clear thanks to recent research, were built on top of a system of traditions, some of which were developed over the course of centuries, and neither able to complete replace them nor root them out. The clash of tradition with modernity continues to this day and is the cause of renewed attention of governments and the international community alike as environmental degradation directly linked to the ineffective and unustainable use of water grows in extent. "],["TamingHydrologicalSystems.html", "1.1 The Taming of the Central Asian Rivers", " 1.1 The Taming of the Central Asian Rivers The conquest of nature, as per the thinking of the tsarist Russian government in the late 19th century, was an essential precondition of successful colonial policy. It enabled the economic appropriation of new territories and facilitated the development of administration and the integration of local authorities into the colonial system (Pravilova 2009). Figure 1.1: Geography of the Central Asia Region. Source: Zoï Environment Network. Figure 1.2: Water resources formation and use. The infographics river width is proportional to average discharge at the particular locations. The red arrows indicate water diversion from rivers into irrigation canals. The gray bands around the rivers in the downstream indicate their natural unaltered runoff. Also note the reuse of drainage water in the Syr Darya. Source: Zoï Environment Network. "],["PostTransitionDevelopmentsChallenges.html", "1.2 Post-Transition Development and Challenges", " 1.2 Post-Transition Development and Challenges Political Changes: Waking up to a new Reality and Adverse Developments "],["HydrologicalSystems.html", "Chapter 2 Hydrological Systems in Central Asia", " Chapter 2 Hydrological Systems in Central Asia Upstream Naryn, Kyrgyzstan. Source: Tobias Siegfried, hydrosolutions GmbH. "],["regional-hydro-climatological-features.html", "2.1 Regional Hydro-Climatological Features", " 2.1 Regional Hydro-Climatological Features The inhomogeneity of the relief structure causes Central Asia to be the territory of immense contrasts. Here, extreme aridity in the hot deserts of the plains and, only 100 km away, abundant humidity and snowfields in the mountains where precipitation levels can range between 1’000 mm up to 2’000 mm. The uneven distribution of water bodies is striking. The mountains of Central Asia are riddled with an extremely branched river network consisting of more than ten thousand watercourses. In the flat foothill areas surrounding the mountain ranges, another branching river network is found which consists of irrigation channels, which do not contribute to the runoff of the core rivers, but rather divert the water from the river network and diffuse it in the irrigated oases where much of it gets evapotranspirated. Central Asia is spreading over approximately 4 million km2, including the territories of Kazakhstan, Kyrgyzstan, Tajikistan, Turkmenistan and Uzbekistan (Central Intelligence Agency, n.d.). In the vast plains that cover around 70% of the total territory, there are only very few rivers which have scarcely any tributaries from the point they leave the mountain areas all the way to their mouth. Abundant solar radiation, high temperatures, small amounts of precipitation, a lack of humidity, unstable snow cover, slight slopes, geological structures1, etc., hinder the formation of surface flows in the plains of Central Asia, despite their big importance for the local agricultural production there. Only the largest rivers, such as Syr Darya, Amu Darya, and Ili are able to survive hundreds of kilometers of deserts and reach the most important landlocked reservoirs – the Aral Sea and Lake Balkhash (Shults 1965). Figure 2.1 shows an overview of the region. Figure 2.1: Map highlighting the Central Asian rivers network (rivers are shown in blue color). The dense river network in the mountainous areas starkly contrasts with the sparse one in the plains. Source: Zoï Environment Network. All Central Asian river basins are endorheic with no water draining out of the region but only evaporating back to the atmosphere. This emphasizes the importance of moisture transfer as an important mechanism in the region region since the formation of substantial watercourses in the mountains is followed by their complete dissipation in the plains, including in the irrigated oases and the terminal lakes, i.e. the Aral Sea and Lake Balkash. In the boreal summer, tropical air masses form in the plains of Central Asia. At that time, even cold air masses coming from the north heat quickly. There is no possibility for temperature differences between the lower and the middle troposphere to occur which explains the horizontal uniformity of high temperatures during summer. Contrary to this, the temperature differences in the region are highest in January during the peak of the boreal winter. As the territory of Central Asia is unprotected from the north, it is under the influence of dry, exceptionally cold air masses originating from the Arctic region and Siberia. These air masses can cause sharp frosts. The further the Siberian or Arctic air masses penetrate towards the west and the south, the more their temperature increase. This explains the big difference in air temperatures during winter between the north and the south of Central Asia. Cold air intrusions are often accompanied by the influx of warm air from the tropics. The cold waves taking turns with hot air masses cause unsteady frost in the plains and, together with generally low precipitation values, do not allow for the formation of a significant snow pack there (Shults 1965). //TODO: properly reference Section here. As is discussed further below, the winter snow cover in the high mountain ranges plays an essential role in runoff formation in the spring and summer months and is thus of key relevant to irrigation agriculture in the downstream and for hydropower production (see Section ~ below). Understanding the atmospheric mechanisms resulting in favorable conditions for winter precipitation is thus essential. With a warming climate and the associated increase of evapotranspiration in the downstream plains and the loss of glacier storage in the mountainous areas, a solid understanding of snow pack formation becomes even more pertinent. (Gerlitz et al. 2018) discusses how the position of the westerly jet stream is connected with the frontal trajectories and the westerly disturbances which are the main moisture sources if the region. The relative position of these planetary wave tracks and their associated westerly flows to the orographic mountain barriers plays thus a key role. The main precipitation events migrate over the winter season from south to north. The southern parts of central Asia, particularly the windward slopes of the Karakorum and Hindu Kush mountain ranges, receive high amounts of winter precipitation (December-January-February), which reaches up to 60% of the total annual precipitation. During spring the zone of maximum precipitation migrates northward, reaches the Pamir plateau in March, and continues to Tien Shan in April/May. The interaction of tropical air masses from the Arabian Golf with westerly flow in Central Asia is another important moisture source for the region. Using data of the ERA-Interim reanalysis, (Gerlitz et al. 2018) classifies 8 weather types (WT) based on typical regional pressure field patterns over a domain covering 20- 60N and 50- 90E. Like this, large-scale features of winter circulation patterns in Central Asia can be captured. WT are analyzed with regard to the spatial anomalies of temperature and precipitation. Figures 2.2 and 2.3 show results. In these Figures, the individual plates WT 1 - 8 are labeled according to the main circulation feature over Central Asia, i.e. a Rossby ridge (R) or trough (T). Generally, it can be observed that configurations that are associated with a Rossby trough over Central Asia lead to an intensification of westerly moisture fluxes (WT 3, WT 7, and WT 8). If there is, however, a Rossby ridge type configuration as shown in WT 1, WT 2, and WT 4 panels of Figures 2.2 and 2.3, moisture fluxes are northward-shifted and precipitation suppressed (Gerlitz et al. 2018). Figure 2.2: Composite maps illustrating the averaged anomalies of ERA-Interim/Land 6-hourly temperature for each weather type (WT 1 - WT 8). Values are depicted in standard deviations for each grid cell, respectively. Arrows indicate anomalies of the 500-hPa ERA-Interim wind field (Gerlitz et al. 2018). Figure 2.3: Composite maps illustrating the averaged anomalies of ERA-Interim/Land 6-hourly precipitation sums for each weather type. Values are depicted relative to the seasonal mean 6-hourly precipitation sum ((100)-1) for each grid cell, respectively. Arrows indicate anomalies of vertically integrated moisture fluxes (Gerlitz et al. 2018). Precipitation is extremely unevenly distributed in the region. 20% of the plain area receives less than 100 mm, while 91% of the territory receives less than 300 mm of precipitation a year with an overall average of 173 mm. The mountains are thus an important climatological and hydrological factor, since they are the places where the water condensates and where the rivers and groundwater originate. Although the range of precipitation levels is wide (60 mm - 2’500 mm), the mountains receive on average more than three times more precipitation than the plains, and the low temperatures favor its accumulation in the solid state (Shults 1965). The influence of the relief is notable also when speaking of precipitation distribution during the year. The high ground areas in Central Asia are witnessing an almost even distribution of precipitation on monthly basis, whereas at the same time, in the inner parts of high mountain ridges there is more precipitation in summer. Such a distribution of precipitation in the inner parts of mountain ridges is a consequence of high condensation levels in summer due to intense evaporation taking place in snow melting areas or, less often, on water surfaces. A typical example showing the influence of the local water vapor emission on annual distribution of precipitation could be the Issyk Kul Lake Basin. There the percentage of precipitation received during summer and the second half of spring, so from May to August, is sometimes reaching even 80% of the total annual precipitation amount, all thanks to the evaporation of the water from the lake and the emergence of thermal convection and subsequent moisture recycling. The areas that are characterized by a predominant precipitation during summertime are the Central Tian Shan and Eastern Pamir, where the difference between the summer and the rest of the year is so big that during summer 60% of all annual precipitation is received. The predominance of the precipitation during summer in case of mountains with steep slopes (15° - 30°) causes fast and abundant snowmelt runoff which is directed to the lower areas and then turns into a river network. Thanks to a large amount of precipitation, relatively low evaporation levels and steep slopes, all rivers of Central Asia, including the largest ones such as Amu Daria, Syr Daria, Ili or Zeravshan, arise in the mountains. Arising in the high ground area, these rivers are mainly fed by snow, glaciers and snow patches melting, as well as by groundwater that, again, were all formed by the same sources. Due to the presence of the vertical thermal gradient, the start of the positive air temperature season, and consequently, the start of the ice and snow melting season does not take place at the same time uniformly. Rather, the snow melting process is of protracted nature and the higher the mountains are in a particular catchment, the later the snow-melt floods take place on rivers that are emerging there. The melting process starts last in the permanent snow and glaciers regions. Because of this, the rivers, which are fed by snowmelt in the upper parts of the catchment area, are of great importance for the irrigation of crop fields, since they are characterized by the most significant water runoff during July and August, at which point the irrigated plains experience severe drought and when irrigated crops have the highest water demand. The rivers having this kind of a runoff regime (nivo-glacial) are Pyandzh and Vakhsh Rivers, as well as the ones deriving from them, such as Amu Darya, Chu, Zeravshan, Talas and Ili Rivers with its numerous tributaries. These rivers all feature a small variability of annual runoff. This is partly a result of the regulating effect of the zone of eternal snows and ice and is very important from the perspective of agricultural production in the downstream. These rivers are thus particularly valuable, not only for irrigation but also as a source of hydroelectric power. Rivers originating from the low mountains but being fed mainly by the snowmelt (nival regime rivers), are characterized by the early floods (March-May) and a sharp variability of annual runoff, since the amount of water is almost entirely determined by the snow reserves in the mountains which were accumulated in the previous winter season. Finally, the watercourses originating from the lowest parts of mountains or from low mountains, (nivo-pluvial regime rivers), which in comparison to other regimes receive much more liquid precipitation, are characterized by large amounts of water, often saturated by sediment, passing during short periods of time. These are so-called mudflows. These watercourses often dry up during summer because of a decrease in supplies from groundwater. When entering the plains, the rivers of Central Asia form wide-spreading alluvial fans consisting of materials brought by them from the mountains. Here the rivers are usually divided into several channels, and a large part of water is filtered by these sediments. The large quantities of groundwater in these alluvial fans, which appear due to this process, mostly protrude from the earth surface at the edges these alluvial fans, causing the small rivers that are fed by groundwater, so-called Karasu rivers, to emerge, which are also used for irrigation. The relief thus has an extremely strong and many-sided impact on runoff formation processes. This influence is mediated through climatic factors, on which the recharge of the rivers, as well as the processes of thawing of snow and ice, etc., depend. In this regard, both the average water content, consistency of the annual runoff and its distribution over a year, as well as other characteristics of the river runoff cannot be considered independently from key relief factors, first and foremost altitude. All this demands a careful and comprehensive analysis of the impact of the relief on runoff processes. namely the distribution of sand and loess relief types, where the former are more permeable and the latter contain more moisture↩︎ "],["chap-regionalWaterBalance.html", "2.2 Regional Water Balance", " 2.2 Regional Water Balance It is instructive to show the opposite hydrological functions of mountain versus flat areas of Central Asia by means of simple water balance considerations. The water balance equation for mountain area, broadly defined, can be written as follows: \\[\\begin{equation} \\tag{2.1} p = e + q_{s} + q_{g} \\end{equation}\\] whereby \\(p\\) represents the average long-term amount of precipitation and condensation of water vapor from the atmosphere, \\(e\\) the average long-term evaporation, \\(q_{s}\\) the average long-term surface outflow and \\(q_{g}\\) the average outflow of groundwater. This equation shows that the mountain area receives moisture only from the atmosphere and rainfall which precipitates within its limits and evaporates only partially. The remainder part of it flows down in the form of surface and underground drainage. Sharp partition of a relief in the mountain area, and consequently, a deep natural drainage is the reason why groundwater is almost entirely connected to the river network already in the mountain area. The Meso-Cenozoic deposits, containing waterproof horizons, and the Paleozoic massifs on the border with the flat areas obstruct groundwater flows. Thus, the groundwater inflow to the flat areas makes no more than 10% - 15% of the surface one and therefore it can be neglected in the first equation. Then the water balance equation will have the following appearance: \\[\\begin{equation} \\tag{2.2} p = e + q_{s} \\end{equation}\\] Based on available data, the rate of surface outflow \\(q_{s}\\) can be calculated quite precisely: 155 billion m3 or 201 mm annually. River basin-specific surface runoff values are provided in Table. It is impossible to measure the amount of the accumulated water vapor in the mountains accurately just by observation, so we have to proceed from the rate of the runoff, for which we need to know the value of runoff coefficient. The last can be approximately considered as equal to 0.35 (see also next Section). Then, \\(p\\) equals 575 mm and \\(e\\), as follows, 575 mm - 201 mm = 374 mm. Table 2.1: Key water balance basin statistics of selected large basins in Central Asia. Basin Name Area (km2) Runoff (m3/s) Runoff entering flatlands (m3/s) Runoff Coeff. (l/ (s km2)) Caspian Sea 29’700 22 12 0.74 Endhoreic Basins of TUK and AFG 193’300 180 155 0.93 Amu Darya 227’300 2’500 2’500 11 Syr Darya 150’100 1’200 1’200 8 Chu and Talas River Basin 37’540 190 190 5.1 Lake Issyk Kul 12’600 115 - 9.1 Southern Balkash Lake 119’000 800 800 6.7 Total 769’600 5’007 4’857 6.5 It should be noted that the rate of surface water inflow to the flatlands is smaller than the runoff which is generated in the mountain areas as part of it is utilized in the mountain area for irrigation purposes (the rivers of Turkmenistan are in this regard an especially good example), or it evaporates from a surface like of the Lake Issyk Kul and other smaller lakes. If, from the mountain area, we exclude reservoirs of the river Atrek and the rivers of Turkmenistan and Afghanistan with no runoff which, occupying the big space (29% of all mountain area of Central Asia), excel in a minute quantity of rainfall and exclusively low water levels, separate elements of water balance will be expressed by the following sizes: \\(p\\) = 675 mm , \\(q_{s}\\) = 270 mm and \\(e\\) = 405 mm. In this case, the water balance of mountain area shows its hydrological essence even more clearly. The equation of water balance for the flat area can be written in the form of \\[ p + q_{i} = e \\] where \\(q_{i}\\) represents the surface inflow of water. We neglect underground outflow in the flat area as, even when it takes place, it is absolutely insignificant. The average amount of rainfall calculated by planimetering of the isohyetal map is equal to 173 mm. The rate of inflow of water is equal to the outflow of water from mountain area, i. e. \\(q_{s} = q_{i} = 155 \\cdot 10^{9} \\text{ m} ^{3}\\). After making its way down to the flat area, which includes the surface of the Aral Sea and Lake Balkhash, the surface inflow of the rivers reaches 124 mm and the evaporation is \\(e = p + q_{i} = 173 \\text{ mm} + 124 \\text{ mm} = 297 \\text{ mm}\\). It is noteworthy to mention that from the entire moisture appearing in the flat area, 58% nevertheless is from atmospheric precipitation, despite its rather insignificant absolute amount. Comparing the two water balance equations shows that the mountain areas receive 575 mm of moisture from the atmosphere of which 374 mm evaporates back to the atmosphere and 201 mm reach the downstream flat area in the form of a surface runoff. Conversely to this, the flat areas receive 297 mm of water from direct precipitation and from inflow of mountain runoff. All of this water evaporates back to the atmosphere. To summarize, it is clear that in the area of runoff formation, \\(p&gt;e\\), in the area of runoff losses \\(p&lt;e\\), and that in the area of runoff balance \\(p\\approx e\\). In each area where the runoff processes show the same orientation, its origin, distribution in time and space, and also the intensity of processes can however vary. In this sense, depending mainly on local topography (generally speaking, depending on the altitude, orientation and exposure of a reservoir to humid air masses), the specific runoff, the persistence of the annual runoff and its distribution over a year, as well as other characteristics of the river flow can sharply differ in different parts of the area of runoff formation, as it was already discussed above. On the other hand, the intensity of runoff losses, their distribution over a year, etc. within the area of runoff losses, considerably depend on the economic activities and features of climatic conditions. "],["runoff-formation.html", "2.3 Runoff Formation", " 2.3 Runoff Formation The process of runoff formation is critically determined by the generation of direct runoff from liquid precipitation and the melting of snow and ice over the course of a hydrological year. Considering carefully the individual components for basins under consideration is important in order to properly understand the runoff regimes of individual rivers and for conceptualizing key processes for mathematical modeling. Figure 2.4: Map of the upper Syr Darya and Amu Darya catchments. The catchments are outlined. Colors encode topographic height. Basin outlets (red dots) are chosen to correspond to the confluence of the two main rivers for each catchment. Topographic data is from (“Srtmgl1 n -ASA SRTM Version 3.0” 2020). First, we study the role of glacier melt. For this, it is instructive to start to look at the condition of discharge formation in the Amu Darya and Syr Darya basins (see Figure 2.4). More specifically, we compare the two main subcatchments of the Vakhsh and Pyandzh Rivers in the Amu Darya and the Naryn and Kara Darya catchments in the Syr Darya2. In the first two reservoirs, the mountains exceeding 5’000 m occupy 6.2% of their area, and the mountains higher than 4’000 m — 42%, in case of reservoirs of the second two rivers, the mountains exceeding 5’000 m occupy less than 1% of their area and the ones with altitudes over 4’000 m — only 4%. Figure 2.5 shows the basins’ hypsographic curves. Even if we take into account that in the case of the basins of Pyandzh and Vakhsh the snow line is on average located at the height of 4’600 m — 4’500 m and in case of the basins of Naryn and Kara Darya at 4’000 m - 3’900 m, 19% — 22% of the reservoirs of Pyandzh and Vakhsh is located above the snow line, and in case of reservoirs of Naryn and Kara Darya it is only 4% — 5%. The role of glaciers in the annual distribution of runoff is thus more substantial in the case of the Amu Darya as compared to the Syr Darya. And to come to an informed conclusion, one has to take into account all land ice features, including their altitude. Figure 2.5: Hypsographic curves of the upper Syr Darya and Amu Darya catchments. The SRTM topographic model was used for the calculation of the curves (“Srtmgl1 n -ASA SRTM Version 3.0” 2020). Data on land ice can be obtained from the Global Land Ice Measurements from Space glacier database GLIMS (GLIMS and NSIDC 2005, updated 2018). The Global Land Ice Measurements from Space (GLIMS) project at NSIDC has implemented a database of glacier outlines from around the world and other information about glaciers that includes the metadata on how those outlines were derived. At &lt;www.glims.org&gt;, one can download outlines and metadata for glaciers in a choice of different formats, including KML (for viewing in Google Earth), ESRI shapefiles, GMT (Generic Mapping Tools), MapInfo, or GML (Geography Markup Language). Figure 2.6 shows a sample visualization of the GLIMS data for the zone of runoff formation of the Amu Darya. The data will also be utilized in Chapter (ref?)(#HydrologicalSystems) below to obtain good ideas of the subcatchments’ glaciation levels. Figure 2.6: Visualization of the GLIMS data glaciation in the Vaksh-Pyandzh basin. The light blue shaded polygons show land ice on top of the underlying digital elevation model. Figure 2.7 shows the distribution of mean glacier elevations of the Randolph Glacier Inventory 6.0 dataset in the GLIMS database. The center of mass for the Amu Darya is at 4’827 masl whereas it is at 4’110 masl for the Syr Darya. Figure 2.7: Distribution of mean glacier elevations as extract from the GLIMS database. Only data from the Randolph Glacier Inventory 6.0 was used as subset of the complete GLIMS record. For the computation of the mean elevation, SRTM data was utilized. Since the GLIMS database contains contains the shapes of glaciers as geometric features, we can calculate easily calculate areas that the individual glaciers cover and then utilize scaling relationships between surface area and volume to estimate total ice storage in the individual Central Asian catchments. (Aizen, Aizen, and Kuzmichonok 2007) reports such scaling relationship for the Central Asian region. As a function of glacier area, they are \\[ V = 0.03782 S^{1.23} \\text{ for } S&lt; 0.1 \\text{ km}^2 \\\\ V = \\frac{0.03332 S ^ {1.08} e^{0.1219 L}}{L^{0.08846}} \\text{ for } 0.1 &lt; S &lt; 25 \\text{ km}^2 \\\\ V = 0.01848 S + 0.021875 S^{1.3521} \\text{ for } S &gt; 25 \\text{ km}^2 \\] where \\(S\\) is the area of a glacier, \\(L\\) the length of the glacier and \\(V\\) the computed volume from the scaling relationship. The length \\(L\\) of the individual glaciers can be approximated by subtracting the minimum glacier elevation from the corresponding maximum elevation. Like this, we can compute land ice volumes of the Amu Darya and Syr Darya basins in an approximate yet scientific way. Alternatively, we can use the volume area scaling relationship by Yerashov. (The emerging river after the confluence of the Vakhsh and Pyandzh rivers is called Amu Darya whereas the Syr Darya emerges after the confluence of the Naryn and Kara Darya rivers.)↩︎ "],["average-multi-year-runoff.html", "2.4 Average Multi-Year Runoff", " 2.4 Average Multi-Year Runoff "],["annual-runoff-fluctuations.html", "2.5 Annual Runoff Fluctuations", " 2.5 Annual Runoff Fluctuations Here, maybe also take a long-term view over the 20th century and even longer, by looking at e.g. Issy Kul Lake Level variations. "],["CaseStudies.html", "Chapter 3 Case Studies", " Chapter 3 Case Studies Charvak reservoir in the Chirchik river basin.. Source: Tobias Siegfried, hydrosolutions GmbH. In this Chapter, two Central Asian river basins are presented and extensively discussed. The purpose is to familiarize the reader with key hydrological processes but also to demonstrate ways of hydro-meteorological time series analyses with R (R Core Team 2013). First, the Gunt river basin in the upper Pandzh is presented. It is representative of a high mountain catchment in an arid to hyper-arid part of Central Asia, i.e. the Pamirs. Mostly because of its elevation, the catchment is largely untouched by human activity. In recent years, hydropower was developed in the of the basin in the lower reaches of the river, shortly before its confluence with the Pyandzh river. Second the Chirchik river basin is presented in detail. The Chirchik river is the largest right tributary to the Syr Darya and the vital source for freshwater supply of the Tashkent Oasis and its surrounding. The bulk of the water is utilized there in irrigation and some of it even diverted to neighboring catchments. After the confluence of its main tributaries, the river is heavily regulated for hydropower production and the supply of irrigation water in the downstream oases. "],["GuntRB.html", "3.1 Gunt River Basin", " 3.1 Gunt River Basin 3.1.1 Basin Characterization The Gunt river basin is located in the Pamir mountain in ## MeteoStation locations and elevation meteoStations &lt;- # just put everything in a regular dataframe tibble(StationName=c(&quot;Bulunkul&quot;,&quot;Khorog&quot;,&quot;Khorog&quot;,&quot;Javshangoz&quot;,&quot;Navobod&quot;), StationCode = c(38953, 38954, 17050, 38956, 38950), lat = c(37.70416667,37.50361111,37.50361111,37.39083333,37.59416667), lon = c(72.94583333,71.515,71.515,72.29583333,71.86555556), utm.x = NA, utm.y = NA, masl = c(3746, 2075, 2075,3438, 2566), type = c(&quot;Meteo&quot;, &quot;Meteo&quot;,&quot;Discharge Gauge&quot;,&quot;Meteo&quot;,&quot;Meteo&quot;)) ## convert lat / lon coordinates to UTM cord.dec = SpatialPoints(cbind(meteoStations$lon, meteoStations$lat), proj4string=CRS(&quot;+proj=longlat&quot;)) cord.UTM &lt;- spTransform(cord.dec, CRS(&quot;+init=epsg:32642&quot;)) cord.UTM.tbl &lt;- cord.UTM %&gt;% as_tibble() meteoStations$utm.x &lt;- cord.UTM.tbl$coords.x1 meteoStations$utm.y &lt;- cord.UTM.tbl$coords.x2 meteoStations ## # A tibble: 5 x 8 ## StationName StationCode lat lon utm.x utm.y masl type ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Bulunkul 38953 37.7 72.9 847891. 4180325. 3746 Meteo ## 2 Khorog 38954 37.5 71.5 722309. 4153714. 2075 Meteo ## 3 Khorog 17050 37.5 71.5 722309. 4153714. 2075 Discharge Gauge ## 4 Javshangoz 38956 37.4 72.3 791785. 4143330. 3438 Meteo ## 5 Navobod 38950 37.6 71.9 752995. 4164650. 2566 Meteo A map of the Gunt river is shown in Figure 3.1. # convert to sf object meteoStations_sf &lt;- st_as_sf(meteoStations %&gt;% dplyr::select(StationName,lat,lon), coords = c(&quot;lon&quot;,&quot;lat&quot;), remove = FALSE, crs = &quot;epsg:32642&quot;, agr = &quot;constant&quot;) # Load catchment shp gunt_Shapefile &lt;- st_read(&#39;./data/AmuDarya/Gunt/GeospatialData/Gunt_Basin_poly.shp&#39;,quiet = TRUE) gunt_Shapefile &lt;- gunt_Shapefile %&gt;% subset(fid==2) gunt_Shapefile_LatLon &lt;- st_transform(gunt_Shapefile,crs = st_crs(4326)) areaGunt &lt;- gunt_Shapefile %&gt;% st_area() %&gt;% as.numeric() # Load subbasins gunt_subbasins_shp &lt;- st_read(&#39;./data/AmuDarya/Gunt/GeospatialData/Gunt_Subbasins_RSMinerve.shp&#39;,quiet = TRUE) # Areas of Interest aoi_CentralAsia_LatLon &lt;- extent(c(65,80.05,35.95,44.05)) # in lat/lon aoi_Basin_LatLon &lt;- gunt_Shapefile_LatLon %&gt;% extent() # GUNT aoi_Basin_UTM &lt;- gunt_Shapefile %&gt;% extent() # GUNT, in UTM # Load DEM Gunt_DEM &lt;- raster(&#39;./data/AmuDarya/Gunt/GeospatialData/17050_Gund_Basin_DEM.tif&#39;, ) Gunt_DEM_lr &lt;- raster::aggregate(Gunt_DEM,fact=10) # this is in UTM 42N Gunt_DEM_lr_LatLon &lt;- raster::projectRaster(Gunt_DEM_lr, crs = &quot;+init=epsg:4326&quot;) # Load simplified river network with first order tributaries only gunt_river_shape &lt;- st_read(&#39;./data/AmuDarya/Gunt/GeospatialData/Gunt_Rivers_RSMinerve.shp&#39;,quiet = TRUE) # Create slope and hillshade slope = terrain(Gunt_DEM_lr_LatLon, opt=&#39;slope&#39;) aspect = terrain(Gunt_DEM_lr_LatLon, opt=&#39;aspect&#39;) hillshade_Gunt = hillShade(slope, aspect, 40, 270) # Subbasins naming subbasins &lt;- # just put everything in a regular dataframe tibble(basin=c(&quot;Shakhdara&quot;,&quot;Gunt&quot;,&quot;Tokusbulak&quot;,&quot;Alishur&quot;), lat = c(37.2,37.75,37.6,37.61), lon = c(72.0,71.8,72.6,73.25)) subbasins_coord_latlon = SpatialPoints(cbind(subbasins$lon, subbasins$lat), proj4string=CRS(&quot;+proj=longlat&quot;)) subbasins_coord_UTM &lt;- spTransform(subbasins_coord_latlon, CRS(&quot;+init=epsg:32642&quot;)) %&gt;% coordinates() %&gt;% as_tibble() %&gt;% rename(x=coords.x1,y=coords.x2) subbasins &lt;- bind_cols(subbasins,subbasins_coord_UTM) # Convert to dataframe for ggplotting Gunt_DEM_spdf &lt;- as(Gunt_DEM_lr_LatLon, &quot;SpatialPixelsDataFrame&quot;) Gunt_DEM_df &lt;- as.data.frame(Gunt_DEM_spdf) colnames(Gunt_DEM_df) &lt;- c(&quot;value&quot;, &quot;x&quot;, &quot;y&quot;) hillshade_Gunt_spdf &lt;- as(hillshade_Gunt, &quot;SpatialPixelsDataFrame&quot;) hillshade_Gunt_df &lt;- as.data.frame(hillshade_Gunt_spdf) colnames(hillshade_Gunt_df) &lt;- c(&quot;value&quot;, &quot;x&quot;, &quot;y&quot;) # plot ggplot() + geom_tile(data = hillshade_Gunt_df, aes(x = x, y = y, fill = value)) + scale_fill_gradient(low = &quot;black&quot;, high = &quot;white&quot;) + new_scale_fill() + geom_tile(data=Gunt_DEM_df, aes(x=x, y=y, fill=value), alpha=0.8)+ geom_sf(data=gunt_Shapefile_LatLon,color=&quot;black&quot;,fill=NA) + geom_sf(data=gunt_subbasins_shp,color=&quot;black&quot;,fill=NA,linetype=&quot;11&quot;,size=0.25) + geom_sf(data=gunt_river_shape,color=&quot;blue&quot;,fill=NA) + geom_point(data = meteoStations, aes(x = lon, y = lat), size = 2, shape = 23, fill = &quot;darkred&quot;) + geom_text(data = meteoStations, aes(x = lon, y = lat, label = StationName), vjust = -.3, hjust = -.2) + scale_fill_gradientn(colours = terrain.colors(100)) + xlab(&quot;Longitude&quot;) + ylab(&quot;Latitude&quot;) + guides(fill=guide_legend(title=&quot;Alt. [masl]&quot;)) + # ggtitle(&quot;Gunt Catchment with Main Subbasins&quot;) + coord_sf(xlim = c(71.4, 74.2), ylim = c(36.9, 38.2), expand = FALSE) + geom_label(data = subbasins,aes(x = lon, y = lat, label = basin),vjust = 0,hjust = 0) Figure 3.1: Map of the Gunt river basin. The subcatchments are named with white labels and the meteorological stations indicated with red diamonds. The discharge station 17050 at Gunt is located at the meteostation 38954. The naming convention we apply is not entirely corresponding to reality since the Alsihur changes its name to Gunt after the outflow of Yashilkul lake some kilometers upstream of the confluence with Tokusbulak. The reason for our simnplified naming is that it facilitates the rainfall-runoff modeling which will be demonstrated in Chapter 7. ## [1] 239.5385 ## [1] -31.51862 Key relevant basin statistics for Gunt river basin. All values are derived from data provided by the Tajik Hydrometeorological Agency. Mean basin precipitation is from the CHELSA v.1.2.1 dataset (Beck et al. 2019). Attribute Value Basin Area (calculated after basin GIS delineation) 13’693 km2 Norm hydrological year discharge 103.8 m3/s Norm cold season discharge (Oct. - Mar., Q4/Q1) 19.8 m3/s Norm warm season discharge (Apr. - Sept., Q2/Q3) 84.2 m3/s Annual discharge volume 3.28 km3 Annual specific discharge 239 mm Mean basin precipitation \\(P\\) (Beck et al. 2020a) 349 mm Potential Evaporation \\(E_{pot}\\) (Trabucco and Zomer 2019) 929 mm Aridity Index \\(\\phi = E_{pot} / P\\) 2.7 With the values provided in the table above, the discharge index \\(Q/P\\) is 68.5 % and the evaporative index \\(E/P\\) is 31.5 %. In other words, the long-term water balance shows that 3 precipitation units gets partitioned into 2 discharge units and 1 evaporation unit, approximately. At the same time, the aridity index \\(\\phi\\) as calculated in the Table above confirms the pronounced arid characteristics of the catchment. 3.1.2 Hydrology For the analysis of the key hydro-climatological characteristics, we first load the available decadal and monthly station data3. The data used in this Chapter can be accessed on the public GitHub repository Applied Modeling of Hydrological Systems in Central Asia and the ./data/AmuDarya/Gunt/ folder in there. First, we load the available data into R. # Load data records fPath = fPath &lt;- &#39;./data/AmuDarya/Gunt/StationData/&#39; fName = &#39;gunt_data_cleaned.Rds&#39; data &lt;- read_rds(paste0(fPath,fName)) data ## # A tibble: 23,352 x 10 ## date data norm units type code station river basin resolution ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; ## 1 1940-01-31 30.5 32.9 m3/s Q 17050 Gunt_Khorog Gunt Pyandz mon ## 2 1940-02-29 27.3 30.1 m3/s Q 17050 Gunt_Khorog Gunt Pyandz mon ## 3 1940-03-31 24.9 28.4 m3/s Q 17050 Gunt_Khorog Gunt Pyandz mon ## 4 1940-04-30 26.4 30.7 m3/s Q 17050 Gunt_Khorog Gunt Pyandz mon ## 5 1940-05-31 59 68.5 m3/s Q 17050 Gunt_Khorog Gunt Pyandz mon ## 6 1940-06-30 309 232. m3/s Q 17050 Gunt_Khorog Gunt Pyandz mon ## 7 1940-07-31 224 319. m3/s Q 17050 Gunt_Khorog Gunt Pyandz mon ## 8 1940-08-31 201 237. m3/s Q 17050 Gunt_Khorog Gunt Pyandz mon ## 9 1940-09-30 121 117. m3/s Q 17050 Gunt_Khorog Gunt Pyandz mon ## 10 1940-10-31 60.8 63.1 m3/s Q 17050 Gunt_Khorog Gunt Pyandz mon ## # … with 23,342 more rows This dataframe contains all available data hydro-meteorological data from the basin. All available hydro-meteorological stations in the basin are listed in the following dataframe. The Khorog meteorological station is at the same site as the gauging stations, hence their locations coincide (lat and lon are latitude and longitude, utm.x and utm.y are station locations in Universal Transverse Mercator (UTM) coordinate reference system for Zone 42N, masl denotes the approximate station location). ## MeteoStation locations and elevation meteoStations &lt;- # just put everything in a regular dataframe tibble(StationName=c(&quot;Bulunkul&quot;,&quot;Khorog&quot;,&quot;Khorog&quot;,&quot;Javshangoz&quot;,&quot;Navobod&quot;), StationCode = c(38953, 38954, 17050, 38956, 38950), lat = c(37.70416667,37.50361111,37.50361111,37.39083333,37.59416667), lon = c(72.94583333,71.515,71.515,72.29583333,71.86555556), utm.x = NA, utm.y = NA, masl = c(3746, 2075, 2075,3438, 2566), type = c(&quot;Meteo&quot;, &quot;Meteo&quot;,&quot;Discharge Gauge&quot;,&quot;Meteo&quot;,&quot;Meteo&quot;)) ## convert lat / lon coordinates to UTM cord.dec = SpatialPoints(cbind(meteoStations$lon, meteoStations$lat), proj4string=CRS(&quot;+proj=longlat&quot;)) cord.UTM &lt;- spTransform(cord.dec, CRS(&quot;+init=epsg:32642&quot;)) cord.UTM.tbl &lt;- cord.UTM %&gt;% as_tibble() meteoStations$utm.x &lt;- cord.UTM.tbl$coords.x1 meteoStations$utm.y &lt;- cord.UTM.tbl$coords.x2 meteoStations ## # A tibble: 5 x 8 ## StationName StationCode lat lon utm.x utm.y masl type ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Bulunkul 38953 37.7 72.9 847891. 4180325. 3746 Meteo ## 2 Khorog 38954 37.5 71.5 722309. 4153714. 2075 Meteo ## 3 Khorog 17050 37.5 71.5 722309. 4153714. 2075 Discharge Gauge ## 4 Javshangoz 38956 37.4 72.3 791785. 4143330. 3438 Meteo ## 5 Navobod 38950 37.6 71.9 752995. 4164650. 2566 Meteo Data are available at monthly time scales. The discharge data from Gauge 17050 can be accessed and extracted from the Gunt dataset in the following way. q_17050_mon &lt;- data %&gt;% filter(type == &quot;Q&quot; &amp; code == &#39;17050&#39; &amp; resolution == &#39;mon&#39;) q_17050_mon ## # A tibble: 972 x 10 ## date data norm units type code station river basin resolution ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; ## 1 1940-01-31 30.5 32.9 m3/s Q 17050 Gunt_Khorog Gunt Pyandz mon ## 2 1940-02-29 27.3 30.1 m3/s Q 17050 Gunt_Khorog Gunt Pyandz mon ## 3 1940-03-31 24.9 28.4 m3/s Q 17050 Gunt_Khorog Gunt Pyandz mon ## 4 1940-04-30 26.4 30.7 m3/s Q 17050 Gunt_Khorog Gunt Pyandz mon ## 5 1940-05-31 59 68.5 m3/s Q 17050 Gunt_Khorog Gunt Pyandz mon ## 6 1940-06-30 309 232. m3/s Q 17050 Gunt_Khorog Gunt Pyandz mon ## 7 1940-07-31 224 319. m3/s Q 17050 Gunt_Khorog Gunt Pyandz mon ## 8 1940-08-31 201 237. m3/s Q 17050 Gunt_Khorog Gunt Pyandz mon ## 9 1940-09-30 121 117. m3/s Q 17050 Gunt_Khorog Gunt Pyandz mon ## 10 1940-10-31 60.8 63.1 m3/s Q 17050 Gunt_Khorog Gunt Pyandz mon ## # … with 962 more rows When we plot the data, we see that we have a near complete monthly record from 1940 onward (Figure 3.2). The data gap in the 1990ies was during the Tajik civil war. q_17050_mon %&gt;% plot_time_series(date, data, .smooth = FALSE, .interactive = TRUE, .title = &quot;&quot;, .x_lab = &#39;Year&#39;, .y_lab = &#39;Mean monthly Q [m3/s]&#39;, .plotly_slider = TRUE) Figure 3.2: Monthly Discharge Data at Gunt Gauging Station (17050) Please also note the visible changes in the low flow regime from 2007 onward. Todo: Ask TAJ HM for clarification on this observation and its probably causes. The seasonal diagnostics of the monthly discharge time series is shown in Figure 3.3. The peak discharge of this very high elevation basin is in July unlike in the lower lying Chirchik tributaries as shown in Figure 3.14 above. q_17050_mon %&gt;% plot_seasonal_diagnostics(.date_var = date, .value = data, .title = &quot;&quot;, .feature_set = c(&quot;month.lbl&quot;), .interactive = FALSE, .x_lab = &quot;Year&quot;, .y_lab = &quot;Mean monthly Q [m3/s]&quot;) + scale_x_discrete(breaks=c(&quot;January&quot;, &quot;February&quot;, &quot;March&quot;, &quot;April&quot;, &quot;May&quot;, &quot;June&quot;, &quot;July&quot;, &quot;August&quot;, &quot;September&quot;, &quot;October&quot;, &quot;November&quot;, &quot;December&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;), labels=c(&quot;J&quot;, &quot;F&quot;, &quot;M&quot;, &quot;A&quot;, &quot;M&quot;, &quot;J&quot;, &quot;J&quot;, &quot;A&quot;, &quot;S&quot;, &quot;O&quot;, &quot;N&quot;, &quot;D&quot;,&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;)) Figure 3.3: Seasonal diagnostics of the monthly discharge time series at the Gunt-Khorog gauging station (17050) Below in Figure ??, we are plotting changes to monthly flows over time by binning all available data in the corresponding monthly slots. The red lines are linear regression lines that indicate trends for the individual months. Over the observational record of approx. 80 years, changes in monthly discharge regimes are clearly visible. On the one hand, summer discharge of Gunt river during the third quarter (Q3) is decreasing whereas the cold season discharge in Q1 and Q4 is increasing. This is a clear indication that the basin hydrology is already reacting to a changing climate. This observation motivates us to further investigate future changes with hydrological modeling (see Chapter ?? for more details). q_17050_mon %&gt;% summarise_by_time(.date_var = date, .by = &quot;month&quot;, value = mean(data)) %&gt;% tk_ts(frequency = 12) %&gt;% forecast::ggsubseriesplot(year.labels = FALSE) + geom_smooth(method = &quot;lm&quot;,color=&quot;red&quot;) + xlab(&#39;Month&#39;) + ylab(&#39;Mean monthly Q [m3/s]&#39;) Whenever we analyze annual data and changes therein, we should work with data as observed during the hydrological year. The hydrological year in Central Asia is defined as: monHY(Oct) = 1 monHY(Nov) = 2 … monHY(Sep) = 12 This also holds for meteorological data. Using this definition, we can further define cold and warm seasons easily where the cold season lasts from October through end of March (Q4 to Q1 the following year) and the warm season from April through end of September (Q2 and Q3). With this in mind, we can define the hydrological year discharge. The function convert2HYY() as part of the riversCentralAsia package provides a convenient way to compute hydrological year mean discharge, including for cold and warm seasons. For monthly mean temperatures mean(T), it computes hydrological year mean temperatures, including for cold and warm seasons. Finally, for precipitation, the function computes the hydrological year sum, including also for cold and warm season months. Figure 3.4 shows the discharge time series analysis for the Khorog gauging station. # Computation of hydrological year discharge and plotting qHYY &lt;- data %&gt;% convert2HYY(.,&#39;17050&#39;,&#39;Q&#39;) qHYY %&gt;% pivot_longer(-hyYear) %&gt;% plot_time_series(hyYear,value,name, .title = &#39;&#39;, .x_lab = &#39;Year&#39;, .y_lab = &#39;Mean monthly Q [m3/s]&#39;, .interactive = FALSE, .smooth=FALSE) Figure 3.4: Hydrological year discharge timeseries, incl. cold and warm season values. If data are not complete for all 12 months, the hydrological year statistics are not computed. data: entire year discharge, data_cs: cold season Q1/Q4 discharge and data_ws: warm season Q2/Q3 discharge. Figure 3.4 confirms the findings from the seasonal analysis. However, it also shows that the first two decades of the 21st century show a marked decline in total discharge as compared to the period between 1960 to 2000. A common way to plot changes over time in hydrometeorological time series is to plot annual deviations from corresponding long-term norms (long-term mean values). For this, we can use the plotNormDevHYY() function. Given the three hydrological year annual time series, it computes long-term norms over the entire data set and subtracts actual annual values from the norm value. Like this, temporal changes and trends become even better visible. Figure 3.5 shows the results for the hydrological year data shown in Figure 3.4. plotNormDevHYY(qHYY,&#39;Q&#39;,&#39;Khorog-Gunt 17050&#39;) Figure 3.5: Deviations from the corresponding long-term norms for the discharge time series at gauging station 17050. It should be noted that the values shown are deviations from the corresponding norms which are shown in the subtitles above the figure plates. Figure 3.5 shows that in absolute terms, the discharge in the high-flow season is undergoing a much greater reduction than an increase in the low-flow season. Hence, we cannot simply explain the decline of discharge in one season with the increase in the other. In other words, the early melting of the winter snow pack cannot alone explain the summer decline in water availability. Some other mechanism much be at work which we still need to better understand. One hypothesis could be that an increase in summer temperatures leads to higher evaporation over the basin thus leading to reduced discharge (see also Section ?? below). In order to gauge whether there is a robust trend in discharge over the observed time period, we compute decadal (10 year means) and plot the results. mean10yearQ &lt;- qHYY %&gt;% filter(hyYear &lt; &#39;2020-01-01&#39;) %&gt;% pivot_longer(-hyYear) %&gt;% group_by(name) %&gt;% summarise_by_time(hyYear,value,.by=&quot;10 year&quot;,mean10yearQ = mean(value,na.rm=TRUE)) %&gt;% dplyr::select(-value) %&gt;% distinct() %&gt;% ungroup() mean10yearQ %&gt;% pivot_wider(names_from = name,values_from = mean10yearQ) ## # A tibble: 8 x 4 ## hyYear data data_cs data_ws ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1940-01-01 111. 20.1 91.1 ## 2 1950-01-01 108. 19.4 88.6 ## 3 1960-01-01 96.2 17.8 78.4 ## 4 1970-01-01 105. 17.9 86.9 ## 5 1980-01-01 105. 18.7 85.8 ## 6 1990-01-01 114. 20.5 93.9 ## 7 2000-01-01 104. 21.9 82.6 ## 8 2010-01-01 94.2 22.3 71.9 mean10yearQ %&gt;% plot_time_series(hyYear,mean10yearQ,name, .smooth = FALSE, .x_lab = &quot;Year&quot;, .y_lab = &quot;Q [m^3/s]&quot;, .title = &quot;&quot;) Figure 3.6: 10-year mean hydrological year discharge of Gunt River, including the cold and warm season components. The decadal mean values are related in time to the beginning of the corresponding decade in the Figure. The strongly declining trend in warm season discharge causes the overall observed decline in hydrological year discharge. This is informative. From the 1990ies onwards, a strong reduction in mean hydrological year warm season discharge is observed of about - 16% relative to mean 1940 - 1989 values. At the same time, 10-year mean hydrological year cold season discharge remained almost stable. These findings are the a key motivation to study climate impacts in the Gunt River basin in greater details. 3.1.2.1 Climatology A significant amount of meteorological data available. This is an invitation to explore these data. We start with the Gunt-Khorog station 38954. Lets get some data ready to be analyzed later. Note, to remain consistent with the discharge data, we only use data from 1940 onward. While we mostly concentrate on mean monthly data for temperature, we should note that the available data record also contains data on absolute and mean minimum and maximum temperatures. # Extracting mean station data from the four stations. Tmean_38954 &lt;- data %&gt;% filter(code==&quot;38954&quot; &amp; type ==&#39;mean(T)&#39;) %&gt;% filter(date&gt;=&#39;1939-01-01&#39;) %&gt;% dplyr::select(date,data) %&gt;% rename(Tmean_38954=data) Tmean_38950 &lt;- data %&gt;% filter(code==&quot;38950&quot; &amp; type ==&#39;mean(T)&#39;) %&gt;% filter(date&gt;=&#39;1939-01-01&#39;) %&gt;% dplyr::select(date,data) %&gt;% rename(Tmean_38950=data) Tmean_38953 &lt;- data %&gt;% filter(code==&quot;38953&quot; &amp; type ==&#39;mean(T)&#39;) %&gt;% filter(date&gt;=&#39;1939-01-01&#39;) %&gt;% dplyr::select(date,data) %&gt;% rename(Tmean_38953=data) Tmean_38956 &lt;- data %&gt;% filter(code==&quot;38956&quot; &amp; type ==&#39;mean(T)&#39;) %&gt;% filter(date&gt;=&#39;1939-01-01&#39;) %&gt;% dplyr::select(date,data) %&gt;% rename(Tmean_38956=data) # Assembling the data. T &lt;- full_join(Tmean_38950,Tmean_38953,by=&quot;date&quot;) T &lt;- full_join(T,Tmean_38954,by=&quot;date&quot;) T &lt;- full_join(T,Tmean_38956,by=&quot;date&quot;) # Plotting the dataframe T %&gt;% pivot_longer(-date) %&gt;% filter(date&gt;=&#39;1940-01-01&#39;) %&gt;% plot_time_series(date, value, name, .smooth = FALSE, .x_lab = &#39;Year&#39;, .y_lab = &#39;Mean monthly T [deg. C]&#39;, .title = &quot;&quot;, .interactive = TRUE) Figure 3.7: Mean Monthly Temperature Climatology in the Gunt River Basin from 1940 - 2020. While first observations are available from the very beginning of the 20th century, data are only shown from 1940 onwards wich marks the start of a coherent record. # add a month identifier T &lt;- T %&gt;% mutate(mon = month(date)) Because of the high quality and the consistency of the long-term record of the data at Khorog station 39854, we focus the further climatological analysis there. Figure 3.8 shows deviations from norm mean temperatures over the last 120 years. The recent two decades stand out because of the pronounced warming observed at the station, especially during the cold season where norm deviations on average range between 1 - 2 degrees Celsius. # Station Khorog 38954 meanTHYY_38954 &lt;- data %&gt;% convert2HYY(38954,&#39;mean(T)&#39;) %&gt;% filter(hyYear &gt;= &quot;1900-10-01&quot;) meanTHYY_38954 %&gt;% plotNormDevHYY(.,&#39;mean(T)&#39;,&#39;Khorog 38954&#39;) Figure 3.8: Annual devations from the norm of the mean temperature for the Khorog station 38954 record are shown for the entire hydrological year and for the corresponding cold and warm seasons. Note that the entire data record is taken into account here from the start of the 20th century. The data was kindly provided by the Tajik Hydrometeorological Agency.↩︎ "],["CRB.html", "3.2 Chirchik River Basin", " 3.2 Chirchik River Basin 3.2.1 Basin Characterization The Chirchik is a river in the Tashkent region of Uzbekistan. Its natural basin covers 13’112 km2, not accounting for the modern-time interbasin water transfers to the neighboring Akhangaran basin in the south (the outline of the basin is shown in Figure 3.9) and to the north. In terms of total runoff contribution, it is the biggest right tributary of the Syr Darya (see also further below in Section ??). The river is formed by the confluence of the Chatkal and the Pskem rivers. They emerge at the south-western end of the Tien Shan mountains, i.e. the Talas Alatau, in the border region of Kyrgyzstan, Kazakhstan and Uzbekistan. The main tributaries are in clock-wise direction starting from north: Ugam, Pskem, Kosku and Chatkal. The Charvak reservoir receives water from these rivers. Ugam is the largest right tributary downstream of the reservoir and Aksak Ata the largest left-side tributary. Below the Charvak hydroelectric power station, the river water gets diverted in numerous canals for irrigation in and around the Tashkent oasis and for interbasin water transfer to the Akhangaran basin in the south. As part of the Chirchik-Bozsuu cascade, several smaller dams along the river serve hydropower production and irrigation purposes. Figure 3.9: Overview over the Chirchik river basin with tributaries and the location of the main gauging stations in the zone of runoff formation and near the confluence with the Syr Darya. Figure 3.9 shows a comprehensive overview of the Chirchik river basin and its tributaries as well as relevant modern gauging stations. Gauges are indicated with the semi-round shapes and the corresponding five digit official code as utilized by the Uzbek Hydrometeorological Service (HMS) indicated. The virtual gauge is not a real gauge in the sense that reservoir inflow is calculated from all contributing tributary flow components, i.e. Chatkal river, Pskem river, Nauvalisoy and Koksu Rivers. Koksu however, with a basin area of 392 km\\(^2\\), is ungauged. Its discharge contribution is calculated using an established empirical relationship between discharge in Chatkal River and discharge in Koksu. The empirical relationship is derived further below in Section ??. First, we now turn our attention to the description of key hydrological basin features. 3.2.2 Hydrology This Section uses a number of available data that are available to characterize the Chirchik River Basin from the hydro-climatological perspective. Data access and modeling is further described in Chapter @ref{HydroModelsEmpiricalModels} in Part II of this Book. The available discharge data is shown in Figure 3.10. These are near complete historic records. See above Figure 3.9 for the station locations. Figure 3.10: Available discharge data of Chirchik River Basin The discharge measurements at Gazalkent gauge started already in 1900 and is one of the longest records available in Central Asia. The monthly record of the station is shown in Figure 3.11. You can zoom into the time series and investigate it in detail. Figure 3.11: Monthly discharge at Gauge 16262, Gazalkent. As is easily visible, the June 1969 discharge was the historic monthly mean maximum with 1’220 m3/s. The characteristics of the timeseries feature the typical snowmelt-driven runoff pattern with pronounced seasonality and interannual variability. At Chinaz near the confluence of the Chirchik River with the Syr Darya (Gauge 16275), however, a changing discharge regime can be identified over time (Figure 3.12). The drastic decrease in discharge there is due to two effects. First, water diversions and interbasin water transfers for irrigation purposes have greatly increased over the course of the 20th century. Second, the closure of the Charvak dam in 1974 and the subsequent filling of the dam decreased discharge during the filling period. Furthermore, the interannual variability of flows decreased from there onwards due to the now regulated flow regime. This latter effect is also visible at the Gazalkent gauge. The non-stationarity in the discharge timeseries at these stations is thus explained by anthropogenic effects. Figure 3.12: Monthly discharge at Gauge 16275, Chinaz The effect of water diversion becomes even more apparent when the annual discharge at Gazalkent gauging station upstream of any major water diversion and at Chinaz gauge, which is in the very downstream of Chirchik River right before its confluence with the Syr Darya, are compared. The corresponding annual timeseries are shown in Figure 3.13 together with the difference of the two time series. Figure 3.13: Annual discharge at Gauge 16262, Gazalkent and Gauge 16275, Chinaz and the difference of the two timeseries. The difference of the two time series is from the allocation of water for human purposes, mostly for irrigation. Figure 3.13 shows the growing water allocation in the catchment from the 1930ies up to the end of the 20th century. Allocation grew almost 3-fold over this period. Interestingly, in the first decade of the 21st century, trends in allocation completely revered and in 2009, roughly one third of the total flow at Gazalkent was allocated consumptively. The trend reversal might be due to a change in irrigation policy, problems with intake infrastructure or both. ## # A tibble: 7 x 5 ## # Groups: code [7] ## code mean min max sd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 16262 229. 48.7 1220 186. ## 2 16275 105. 1.2 912 121 ## 3 16279 116. 21.1 729 110. ## 4 16290 79.4 12.7 438 69.3 ## 5 16298 3.8 0.9 21.1 2.8 ## 6 16300 22.4 3.9 114 19.3 ## 7 16924 205. 40.7 1231 183. Table 3.1: Key statistics of Chirchik basin rivers. code mean min max sd 16262 228.6 48.7 1220.0 186.4 16275 104.9 1.2 912.0 121.0 16279 115.7 21.1 729.0 109.9 16290 79.4 12.7 438.0 69.3 16298 3.8 0.9 21.1 2.8 16300 22.4 3.9 114.0 19.3 16924 205.3 40.7 1231.0 183.2 The largest left tributary to Chirchik below the Charvak reservoir Aksak Ata. The gauging station on the river got dismantled a long time ago. An average long-term mean discharge of 2.35 m\\(^{3}\\)/s is a solid estimated of its contribution to the overall discharge of Chirchik. Thus, if we add up long-term average discharge at Gazalkent and the one from Aksak Ata we obtain an annual norm discharge of 231 m\\(^{3}\\)/s. Chirchik river is thus the biggest right-tributary to the Syr Darya. Chatkal river contributes exactly half to it (115.7 m\\(^{3}\\)/s) and Pskem river approximately one third (34.4% or 79.4 m\\(^{3}\\)/s). Nauvalisoy is only a very small river with 1.6 % runoff contribution (3.8 m\\(^{3}\\)/s). From the available data, the long-term average runoff contribution by the ungauged Koksu river can be estimated to be 6.4 m\\(^{3}\\)/s or 2.8 %. Downstream of the reservoir, Ugam river contributes an additional 9.7 % (22.4 m\\(^{3}\\)/s) to the total flow. Let us now turn our attention to the seasonality of the tributaries. We exclude both, the Chinaz Gauge and Gazalkent Gauge data in our analysis for the above-mentioned reason that flow there is no longer representing a natural runoff regimes there. We thus plot seasonalities of the key gauged and unregulated tributaries , i.e. Chatkal, Pskem, Nauvalisoy and Ugam rivers in Figures ?? and ?? below. Figure 3.14: Seasonality diagnostics of the two large tributaries, i.e. the Chatkal and Pskem rivers. Discharge seasonality of the gauging stations downstream of Charvak reservoir is shown below. Note that we only have monthly values for Ugam station which explains the appearance of the weekly plot in the upper right panel of Figure 3.15. Figure 3.15: Seasonality diagnostics of the two minor tributaries taht are gauged. The seasonality with the spring and summer runoff peaks is striking in all the rivers. Nauvalisoy discharge peaks, on average, during or around week 20. Chatkal river discharge peaks around week 23 and Pskem river around week 26. These differences can be explained with the difference in mean catchment elevations which are as follows: Nauvalisoy catchment: 2’160 masl, Ugam catchment: 803 masl, Chatkal catchment: 2’692 masl, and Pskem Catchment: 2’795 masl where Ugam is the lowest lying and Pskem catchment the highest catchment (see also Chapter 2 for more information). Figure 3.16 shows the hypsometric curves of the main tributaries to the Chirchik River. Figure 3.16: Hypsometric Curves of the tributaries to the Chirchik River Basin. Using a LOESS smoother, we can remove discharge time series seasonality and catch a glimpse of the underlying longterm trends. This is shown for gauging station 16294, i.e. the inflow to the Charvak Reservoir, in Figure 3.17. If anything, a slightly increasing trend in mean discharge can be observed over the last 40 years. We will further discuss this finding also in the context of the analysis of the meteorological data record in the next Section. Figure 3.17: Changes in mean monthly discharges are plotted with black lines over the entire observational record for Charvak Reservoir gauge (16924).The blue line shows a smoothed trend using a LOESS-smoother. But what about changes for particular seasons and months? To understand these changes, we plot monthly average data grouped together individually for all months. Figure 3.18 shows the resulting graphs together with their best fit regression lines for each month. Several interesting observations can be done. Figure 3.18: Changes in mean monthly discharges are plotted with black lines over the entire observational record for Charvak Reservoir gauge (16924).The red lines are the per month best fit regression lines. First, cold season discharge in quarter 1 (Q1) and Q4 have a slightly increasing trend. Converse to this, the warm season quarterly trends are not uniform where Q2 trends are strongly increasing and Q3 trends are markedly decreasing. This is in line with what one would expect from a warming climate, i.e. that the snow-melt driven hydrograph peak flows shift in their timing towards earlier towards spring. At the same time, Q3 warm season discharge diminishes because of the earlier snowmelt, assuming no changes in the precipitated water. We will investigate the available climate and precipitation record in the following section. Figure 3.19: The plates show mean, minimum and maximum quarterly discharges for Q1 (upper left plate), Q2 (upper right plate), Q3 (lower left plate) and Q4 (lower right plate). All values are in mean quarterly discharge per second The development of the quarterly minimum, maximum and mean discharge Q over the years for Gauge 16924 (Charvak reservoir inflow) is shown in Figure 3.19. The increasing trends in cold season discharge (Q1 and Q4) is confirmed. In these quarters, minimum, mean and maximum discharges appear to increase with a probably link to temperature increases during these quarters (see Section ?? for a discussion). In Q2, minimum and mean discharges have an increasing trend. In Q3, maximum discharge appears to decrease over time. 3.2.2.1 Climatology Long-term climate data from three different stations located in the vicinity and upstream of Charvak Reservoir is available. The stations are meteostation 38642 and 38339, both in Pskem River Basin, Meteostation 38471, Chatkal River Basin and Meteostation 38464 in the vicinity of the Charvak Reservoir (see also Figure 3.9 above for their locations. The raw temperature data is shown in Figure 3.20 whereas the per month temperature trends are shown in Figures 3.21 and 3.22. At both stations, a significant cold season warning trend is visible. Figure 3.20: Available decadal temperature records at Pskem and Chatkal meteorological stations. The record at the Kyrgyz Chatkal Meteo Station shows a large data gap in the post-transition years. The blue trend lines (LOESS smoother) indicate an increasing temperature trend at both mountain stations. Figure 3.21: Changes in mean monthly temperatures are plotted with black lines over the entire observational record for Pskem meteorological station.The red lines are the per month best fit regression lines. Figure 3.22: Changes in mean monthly temperatures are plotted with black lines over the entire observational record for Pskem meteorological station. The red lines are the per month best fit regression lines. The peak in the month of September is an outlier. Similarily to the analysis carried out above for the development of quarterly flows, we can analyze the development of quarterly temperature statistics. Figure (fig:quarterlyMeanMinMaxT_38462) shows the result. (#fig:quarterlyMeanMinMaxT_38462)Development of mean, minimum and maximum quarterly temperatures for Q1 at Station 38462 Figure 3.23: Available decadal and monthly data records from different meteorological stations that are located in the zone of runoff formation. As in the case of temperature, the precipitation record at the Kyrgyz Chatkal Meteo Station shows a large data gap in the post-transition years. Figure 3.24: Development of mean, minimum and maximum quarterly temperatures for Q1 at Station 38462 Investigate temperature precipitation link q1TP &lt;- left_join(q1T ,q1P ,by=c(&#39;date&#39;,&#39;name&#39;)) %&gt;% rename(T=value.x,P=value.y) p1 &lt;- q1TP %&gt;% na.omit() %&gt;% ggplot(aes(x=T,y=P)) + geom_point() + geom_smooth(formula = y ~ x,method = &quot;lm&quot;) q2TP &lt;- left_join(q2T ,q2P ,by=c(&#39;date&#39;,&#39;name&#39;)) %&gt;% rename(T=value.x,P=value.y) p2 &lt;- q2TP %&gt;% na.omit() %&gt;% ggplot(aes(x=T,y=P)) + geom_point() + geom_smooth(formula = y ~ x,method = &quot;lm&quot;) q3TP &lt;- left_join(q3T ,q3P ,by=c(&#39;date&#39;,&#39;name&#39;)) %&gt;% rename(T=value.x,P=value.y) p3 &lt;- q3TP %&gt;% na.omit() %&gt;% ggplot(aes(x=T,y=P)) + geom_point() + geom_smooth(formula = y ~ x,method = &quot;lm&quot;) q4TP &lt;- left_join(q4T ,q4P ,by=c(&#39;date&#39;,&#39;name&#39;)) %&gt;% rename(T=value.x,P=value.y) p4 &lt;- q4TP %&gt;% na.omit() %&gt;% ggplot(aes(x=T,y=P)) + geom_point() + geom_smooth(formula = y ~ x,method = &quot;lm&quot;) p &lt;- list(p1,p4) cowplot::plot_grid(plotlist = p,nrow = 1,ncol = 2) Investigate temperature discharge link q1TQ &lt;- left_join(q1T ,q1Q ,by=c(&#39;date&#39;,&#39;name&#39;)) %&gt;% rename(T=value.x,Q=value.y) p1 &lt;- q1TQ %&gt;% na.omit() %&gt;% ggplot(aes(x=T,y=Q)) + geom_point() + geom_smooth(formula = y ~ x,method = &quot;lm&quot;) q2TQ &lt;- left_join(q2T ,q2Q ,by=c(&#39;date&#39;,&#39;name&#39;)) %&gt;% rename(T=value.x,Q=value.y) p2 &lt;- q2TQ %&gt;% na.omit() %&gt;% ggplot(aes(x=T,y=Q)) + geom_point() + geom_smooth(formula = y ~ x,method = &quot;lm&quot;) q3TQ &lt;- left_join(q3T ,q3Q ,by=c(&#39;date&#39;,&#39;name&#39;)) %&gt;% rename(T=value.x,Q=value.y) p3 &lt;- q3TQ %&gt;% na.omit() %&gt;% ggplot(aes(x=T,y=Q)) + geom_point() + geom_smooth(formula = y ~ x,method = &quot;lm&quot;) q4TQ &lt;- left_join(q4T ,q4Q ,by=c(&#39;date&#39;,&#39;name&#39;)) %&gt;% rename(T=value.x,Q=value.y) p4 &lt;- q4TQ %&gt;% na.omit() %&gt;% ggplot(aes(x=T,y=Q)) + geom_point() + geom_smooth(formula = y ~ x,method = &quot;lm&quot;) p &lt;- list(p1,p4) cowplot::plot_grid(plotlist = p,nrow = 1,ncol = 2) Precipitation - Discharge Link q1PQ &lt;- left_join(q1P,q1Q ,by=c(&#39;date&#39;,&#39;name&#39;)) %&gt;% rename(P=value.x,Q=value.y) p1 &lt;- q1PQ %&gt;% na.omit() %&gt;% ggplot(aes(x=P,y=Q)) + geom_point() + geom_smooth(formula = y ~ x,method = &quot;lm&quot;) q2PQ &lt;- left_join(q2P,q2Q ,by=c(&#39;date&#39;,&#39;name&#39;)) %&gt;% rename(P=value.x,Q=value.y) p2 &lt;- q2PQ %&gt;% na.omit() %&gt;% ggplot(aes(x=P,y=Q)) + geom_point() + geom_smooth(formula = y ~ x,method = &quot;lm&quot;) q3PQ &lt;- left_join(q3P,q3Q ,by=c(&#39;date&#39;,&#39;name&#39;)) %&gt;% rename(P=value.x,Q=value.y) p3 &lt;- q3PQ %&gt;% na.omit() %&gt;% ggplot(aes(x=P,y=Q)) + geom_point() + geom_smooth(formula = y ~ x,method = &quot;lm&quot;) q4PQ &lt;- left_join(q4P,q4Q ,by=c(&#39;date&#39;,&#39;name&#39;)) %&gt;% rename(P=value.x,Q=value.y) p4 &lt;- q4PQ %&gt;% na.omit() %&gt;% ggplot(aes(x=P,y=Q)) + geom_point() + geom_smooth(formula = y ~ x,method = &quot;lm&quot;) p &lt;- list(p1,p2,p3,p4) cowplot::plot_grid(plotlist = p,nrow = 1,ncol = 4) Q1 versus Q2 q1QQ &lt;- q1Q %&gt;% add_column(Q2=q2Q$value) %&gt;% rename(Q1=value) p1 &lt;- q1QQ %&gt;% na.omit() %&gt;% ggplot(aes(x=Q1,y=Q2)) + geom_point() + geom_smooth(formula = y ~ x,method = &quot;loess&quot;) q2QQ &lt;- q2Q %&gt;% add_column(Q3=q3Q$value) %&gt;% rename(Q2=value) p2 &lt;- q2QQ %&gt;% na.omit() %&gt;% ggplot(aes(x=Q2,y=Q3)) + geom_point() + geom_smooth(formula = y ~ x,method = &quot;loess&quot;) # q3QQ &lt;- q3Q %&gt;% add_column(Q4=q4Q$value) %&gt;% rename(Q3=value) # p3 &lt;- q3QQ %&gt;% na.omit() %&gt;% ggplot(aes(x=Q3,y=Q4)) + geom_point() + geom_smooth(formula = y ~ x,method = &quot;loess&quot;) p &lt;- list(p1,p2) cowplot::plot_grid(plotlist = p,nrow = 1,ncol = 2) 3.2.3 Discharge Estimation from the Ungauged Kosku Tributary Before the closure of the Charvak dam and the subsequent filling of the reservoir in and after 1974, the HydroMet experts started a detailed 3-years measurement comparison campaign at Charvak gauge and at gauge 16279 in Khudaydod (see Figure 3.9). Both are located on Chatkal river. Charvak gauge had to be decommissioned after the closure of the dam because it got flooded. The confluence of Koksu river with Charvak river is just upstream of the former Charvak gauge. Using daily data from the measurement comparisons campaign, the HydroMet experts could then relate Charvak gauge discharge to Khudaydod discharge using a linear relationship. At the same time, they were now able to relate Koksu discharge to the discharge at Chatkal River in Khudaydod in the following manner \\[ Q_{Koksu} \\propto Q_{Charvak} - Q_{16279} \\] We show the procedure here. After loading the riversCentralAsia Package as shown above, the relevant daily data from 01/01/1965 - 31/12/1967 can be loaded. KoksuDischargeDerivation &lt;- riversCentralAsia:::KoksuDischargeDerivation # load Data Please not that, unless otherwise mentioned, all data from discharge meteorological stations utilized in this book are from the Uzbek HMS. The data is stored in long format, meaning that measurements in time and for the two gauging stations are just stacked on top of each other in one long table. For the purpose here, we prefer the wide format where we have one date column with unique dates and then the data listed for each station in corresponding columns. KoksuDischarge_wide &lt;- KoksuDischargeDerivation %&gt;% pivot_wider(id_cols = &#39;date&#39;,values_from = &#39;data&#39;,names_from = &quot;code&quot;) KoksuDischarge_wide ## # A tibble: 988 x 3 ## date Charvak `16279` ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1965-01-01 35.6 33.4 ## 2 1965-01-02 35.6 32.4 ## 3 1965-01-03 33 30.4 ## 4 1965-01-04 33 29.4 ## 5 1965-01-05 33 30.4 ## 6 1965-01-06 33 30.4 ## 7 1965-01-07 34.3 31.4 ## 8 1965-01-08 35.6 32.4 ## 9 1965-01-09 38.4 34.4 ## 10 1965-01-10 35.6 32.4 ## # … with 978 more rows The runoff contribution of Koksu can be calculated in a simple manner. # Adding Koksu discharge to the dataframe KoksuDischarge_wide &lt;- KoksuDischarge_wide %&gt;% mutate(Koksu = Charvak - `16279`) KoksuDischarge_wide ## # A tibble: 988 x 4 ## date Charvak `16279` Koksu ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1965-01-01 35.6 33.4 2.2 ## 2 1965-01-02 35.6 32.4 3.2 ## 3 1965-01-03 33 30.4 2.6 ## 4 1965-01-04 33 29.4 3.6 ## 5 1965-01-05 33 30.4 2.6 ## 6 1965-01-06 33 30.4 2.6 ## 7 1965-01-07 34.3 31.4 2.90 ## 8 1965-01-08 35.6 32.4 3.2 ## 9 1965-01-09 38.4 34.4 4 ## 10 1965-01-10 35.6 32.4 3.2 ## # … with 978 more rows The relationship can now be visualized. # and visualize ggplot(KoksuDischarge_wide, aes(`16279`, Koksu)) + geom_point() + xlab(bquote(&#39;Discharge at Gauge 16279 Khudaydod in&#39;~m^3/s)) + ylab (bquote(&#39;Koksu river discharge in&#39;~m^3/s)) We can perform a linear regression to related discharge at Khudaydod to the one at Koksu. The coefficients of the linear regression can be obtained in the following way: lm &lt;- lm(Koksu ~ 0 + `16279`,KoksuDischarge_wide) summary(lm) ## ## Call: ## lm(formula = Koksu ~ 0 + `16279`, data = KoksuDischarge_wide) ## ## Residuals: ## Min 1Q Median 3Q Max ## -38.295 -5.867 -1.918 1.416 120.419 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## `16279` 0.14469 0.00298 48.55 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 12.4 on 987 degrees of freedom ## Multiple R-squared: 0.7049, Adjusted R-squared: 0.7046 ## F-statistic: 2357 on 1 and 987 DF, p-value: &lt; 2.2e-16 Please note, in the specification of the linear model we add the 0 term to force the regression through the origin. Hence, the discharge of Koksu River can be estimated to be \\[ Q_{Koksu} = 0.145 * Q_{Khudaydod} \\] "],["Part2-Applied-Modeling.html", "(PART) Part II Applied Modeling ", " (PART) Part II Applied Modeling "],["LongTermWaterBalance.html", "3.3 Budyko-Type Long-term Water Balance Modeling", " 3.3 Budyko-Type Long-term Water Balance Modeling In this Chapter, we are looking at watersheds from a long-term perspective and want to understand the key processes that lead to a partitioning of precipitation into evaporation and runoff. Figure 3.25: The generic river basin. Not yet final Figure, needs to be improved since E is missing. 3.3.1 Derivation of the Budyko Relationship The general water balance for a catchment can be written as \\[ Q = P - E + \\Delta S \\tag{3.1} \\] where \\(P\\) is precipitation in mm, \\(E\\) is evapotranspiration in mm, \\(\\Delta S\\) is net storage and \\(Q\\) is specific discharge in mm. Over hydrological years and longer time scales, we expect \\(\\Delta S\\) to be 0. Hence, the above Equation (3.1) can be rewritten as \\[ Q = P - E \\tag{3.2} \\] Dividing by \\(P\\), we get \\[ \\frac{Q}{P} = 1 - \\frac{E}{P} \\tag{3.3} \\] where \\(Q/P\\) can be called the runoff index and \\(E/P\\) is the evaporation index or evaporative fraction. Note that this equation does not distinguish between solid (smow) and liquid (rain) precipitation. How to adapt this equation to high mountain catchments where a substantial fraction of precipitation falls as snow is discussed in the next Section ?? below. In the following derivation of the relationship between a catchment’s aridity index and the evaporation fraction, we follow (Arora 2002). The surface energy balance can be written as \\[ R_{N} = H_{S} + H_{L} + \\Delta H_{G} \\tag{3.4} \\] where \\(R_{N}\\) is the net radiation [in W/m2 = kg/s3], \\(H_{S}\\) is the upward sensible heat flux, \\(H_{L}\\) is the latent heat flux and \\(\\Delta H_{G}\\) the net ground heat flux. It should be noted that \\(H_{L} = L \\cdot E\\) where \\(L = 2.5 \\cdot 10^{6}\\) J/kg [= m2/s2] is the latent heat of vaporization and \\(E\\) is the actual evaporation in [m/s]. As in the case of the water balance, at the annual or longer time scales, we can neglect the storage effect and get \\[ R_{N} = H_{S} + L \\cdot E \\tag{3.5} \\] With the Bowen ratio defined as the fraction of the sensible heat flux divided by the latent heat flux, i.e. \\[ \\gamma = \\frac{H_{S}}{H_{L}} = \\frac{H_{S}}{L \\cdot E } \\tag{3.6} \\] and by rearranging the terms, the long-term energy balance in Equation (3.5) can simply be rewritten as \\[ R_{n} = (1 + \\gamma)L E \\tag{3.7} \\] Using the fact that \\(R_{n} = L E_{pot}\\), where \\(E_{pot}\\) is the potential evaporation, and dividing by precipitation, we can rewrite the above Equation (3.7) as \\[ \\frac{E_{pot}}{P} = (1 + \\gamma) \\frac{E}{P} \\tag{3.8} \\] where the left-hand side is called the aridity index, i.e. \\(\\phi = E_{pot}/P\\) and \\(E/P\\) is called the evaporative fraction or evaporation index. With this, Equation (3.3) from above can be written as a function of the Bowen ratio and the aridity index, i.e. \\[ \\frac{E}{P} = 1 - \\frac{Q}{P} = \\frac{\\phi}{(1 + \\gamma)} \\tag{3.9} \\] \\(Q/P\\) is again the runoff index. Since the Bowen ratio is also water and energy limited, it too is a function of the aridity index and we can thus rewrite Equation (3.9) to \\[ \\frac{E}{P} = 1 - \\frac{Q}{P} = F[\\phi] \\tag{3.10} \\] Following the groundbreaking work by Budyko and others, many expressions have been developed for \\(F[\\phi]\\) to describe the long-term catchment water balance (citations…). Here, we use the Choudhury equation which relates the aridity index \\(\\phi\\) to the evaporative fraction \\(E/P\\) in the following way \\[\\begin{equation} \\frac{E}{P} = \\left[ 1 + \\left( \\frac{E_{pot}}{P} \\right) ^{-n} \\right]^{1/n} \\tag{3.11} \\end{equation}\\] where \\(n\\) is a catchment-specific parameter which accounts for factors such as vegetation type and coverage, soil type and topography, etc. (see e.g. (Zhang et al. 2015) for more information). "],["effects-of-snow-ratio-on-annual-runoff-within-the-budyko-framework.html", "3.4 Effects of Snow Ratio on Annual Runoff within the Budyko Framework", " 3.4 Effects of Snow Ratio on Annual Runoff within the Budyko Framework Source article: (Zhang et al. 2015) "],["hydrological-response-to-a-changing-climate.html", "3.5 Hydrological Response to a Changing Climate", " 3.5 Hydrological Response to a Changing Climate Source articles: Ning et al. (2018), (Zhou et al. 2016). "],["application-to-central-asian-river-basins.html", "3.6 Application to Central Asian River Basins", " 3.6 Application to Central Asian River Basins 3.6.1 Data 3.6.2 Model 3.6.3 Conclusions "],["Chapter-DATA.html", "Chapter 4 Data Retrieval, Preparation &amp; Analysis", " Chapter 4 Data Retrieval, Preparation &amp; Analysis In this Chapter, we will discuss how to retrieve, prepare and process the data that is required for modeling. This includes in-situ station data, geospatial data, climate reanalysis data, and climate projections data. As will become clear, the preparation of these data requires a substantial amount of work, local storage space and, in some instances, computational power. With a focus on the generation of input files for hydrological-hydraulic modeling with RS Minerve, the flow diagram Figure ?? shows the required steps for the individual data types. These steps will be discussed in the individual Sections below in detail. "],["Data-StationData.html", "4.1 Station Data", " 4.1 Station Data Much of key data visualization techniques have already been presented in the Chapter 3. Here, we are demonstrating important data preparation steps that should always precede modeling. These preparatory steps focus on data cleaning and gap filling. 4.1.1 Available Data The riversCentralAsia Package provides available data of the gauging and meteorological stations in the Chirchik River Basin4. Before starting any type of modeling, it is important to get a good understanding of the data that we are dealing with and whether there exist problems with the raw data that need to be addressed prior to modeling. Problems usually include data gaps and outliers as data records that one obtains are usually ever complete nor clean of errors. The steps performed here are thus required steps for any type of successful modeling and should be performed with great care prior to starting hydrological modeling. We concentrate our efforts here on discharge records and data from meteorological stations in the Chirchik River Basin. The techniques shown here for decadal (10-days) data naturally extend to monthly data and also, to data from other basins. 4.1.2 Gap Filling Discharge Data In the following, we will work with decadal discharge data from the two main tributaries, i.e. the Chatkal and (Gauge 16279) Pskem rivers (Gauge 16290) and the data of the inflow to the Charvak reservoir (Gauge 16924). The goal is to analyze the data and prepare for modeling. First, let us load the relevant discharge data. data &lt;- ChirchikRiverBasin # load data q_dec_tbl &lt;- data %&gt;% filter(code == &#39;16279&#39; | code == &#39;16290&#39; | code == &#39;16924&#39;) # Note for the new name of the object, we choose to add periodicity (_dec_) and data type (_tbl for tibble/dataframe) to the data name. This just helps to stay organized and is good practice in R programming. q_dec_tbl ## # A tibble: 9,072 x 14 ## date data norm units type code station river basin resolution ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; ## 1 1932-01-10 48.8 38.8 m3s Q 16279 Khudaydod Chatkal Chirch… dec ## 2 1932-01-20 48.4 37.5 m3s Q 16279 Khudaydod Chatkal Chirch… dec ## 3 1932-01-31 42.4 36.6 m3s Q 16279 Khudaydod Chatkal Chirch… dec ## 4 1932-02-10 43.7 36.4 m3s Q 16279 Khudaydod Chatkal Chirch… dec ## 5 1932-02-20 44.2 36.3 m3s Q 16279 Khudaydod Chatkal Chirch… dec ## 6 1932-02-29 47.7 36.9 m3s Q 16279 Khudaydod Chatkal Chirch… dec ## 7 1932-03-10 54.1 39.4 m3s Q 16279 Khudaydod Chatkal Chirch… dec ## 8 1932-03-20 63.2 47.6 m3s Q 16279 Khudaydod Chatkal Chirch… dec ## 9 1932-03-31 103 60.5 m3s Q 16279 Khudaydod Chatkal Chirch… dec ## 10 1932-04-10 103 86.4 m3s Q 16279 Khudaydod Chatkal Chirch… dec ## # … with 9,062 more rows, and 4 more variables: lon_UTM42 &lt;dbl&gt;, ## # lat_UTM42 &lt;dbl&gt;, altitude_masl &lt;dbl&gt;, basinSize_sqkm &lt;dbl&gt; You can get more information about the available data by typing ?ChirchikRiverBasin. Note that the original time series data has been packaged in this format by the riversCentralAsia::loadTabularData() function which takes a simple .csv file as input. It is advisable to check at this stage for missing data in time series and to fill gaps where present. Are there missing data? How can these be filled so as to arrive at complete time series that are required for hydrological modeling? As can be seen in Figure 4.1 , close inspection of the time series indeed reveals some missing data in the 1940ies. q_dec_tbl %&gt;% plot_time_series(date,data, .facet_vars = code, .smooth = FALSE, .interactive = TRUE, .x_lab = &quot;year&quot;, .y_lab = &quot;m^3/s&quot;, .title = &quot;&quot; ) Figure 4.1: Discharge data of selected gauges in the upstream zone of runoff formation in the Chirchik River Basin. Data Source: Uzbek Hydrometeorological Service. Note, Figure 4.1 and the following Figures are interactive, so you can zoom in to regions of interest. Missing data are also confirmed by the warning that the function timetk::plot_time_series() throws (suppressed here). Statistics of the missing data can be easily obtained. As the Table below shows, we can do this analysis for each discharge station separately. q_dec_tbl %&gt;% group_by(code) %&gt;% summarize(n.na = sum(is.na(data)), na.perc = n.na/n()*100) ## # A tibble: 3 x 3 ## code n.na na.perc ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 16279 15 0.496 ## 2 16290 39 1.29 ## 3 16924 42 1.39 Summarizing the number of observation with missing data reveals 15 data points for station 16279 (0.5 % of total record length) and 39 for station 16290 (1.3 % of total record length). As there are only very few gaps in the existing time series, we use a simple method to fill these. Wherever there is a gap, we fill in the corresponding decadal norm as stored in the norm column in the object q_dec_tbl. The visualization of the results confirms that our simple gap filling approach is indeed satisfactory (Figure 4.2). q_dec_filled_tbl &lt;- q_dec_tbl q_dec_filled_tbl$data[is.na(q_dec_filled_tbl$data)] = q_dec_filled_tbl$norm[is.na(q_dec_filled_tbl$data)] # Gap filling step q_dec_filled_tbl %&gt;% plot_time_series(date, data, .facet_vars = code, .smooth = FALSE, .interactive = TRUE, .x_lab = &quot;year&quot;, .y_lab = &quot;m^3/s&quot;, .title = &quot;&quot; ) Figure 4.2: Gap filled Pskem and Chatkal river discharges. All missing data are gone now. q_dec_filled_tbl %&gt;% group_by(code) %&gt;% summarize(n.na = sum(is.na(data)), na.perc = n.na/n()*100) ## # A tibble: 3 x 3 ## code n.na na.perc ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 16279 0 0 ## 2 16290 0 0 ## 3 16924 0 0 A note of caution here. This simple gap filling technique reduces variance in the time series. It should only be used when the percentage of missing data is low. As will be discussed in the next Section 4.1.3 below, better techniques have to be utilized when there exist substantial gaps and in the case of less regular data. Finally, we discard the norm data which we used for gap filling of the missing discharge data and convert the data to wide format (see the Table below) to add to it meteorological data in the next Section. q_dec_filled_wide_tbl &lt;- q_dec_filled_tbl %&gt;% # again we use the name convention of objects as introduced above mutate(code = paste0(&#39;Q&#39;,code %&gt;% as.character())) %&gt;% # Since we convert everything to long form, we want to keep information as compact as possible. Hence, we paste the type identifier (Q for discharge here) in from of the 5 digit station code. dplyr::select(date,data,code) %&gt;% # ... and then ditch all the remainder information pivot_wider(names_from = &quot;code&quot;,values_from = &quot;data&quot;) # in order to pivot to the long format, we need to make a small detour via the wide format. q_dec_filled_long_tbl &lt;- q_dec_filled_wide_tbl %&gt;% pivot_longer(-date) # and then pivot back q_dec_filled_wide_tbl ## # A tibble: 3,024 x 4 ## date Q16279 Q16290 Q16924 ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1932-01-10 48.8 38.3 87.1 ## 2 1932-01-20 48.4 37.7 86.1 ## 3 1932-01-31 42.4 36.2 78.6 ## 4 1932-02-10 43.7 35.6 79.3 ## 5 1932-02-20 44.2 35 79.2 ## 6 1932-02-29 47.7 37.1 84.8 ## 7 1932-03-10 54.1 43.1 97.2 ## 8 1932-03-20 63.2 47 110 ## 9 1932-03-31 103 72.1 175 ## 10 1932-04-10 103 73.2 176 ## # … with 3,014 more rows As a result, we now have a complete record of decadal discharge data for the two main tributaries of the Chirchik river and the inflow time series to Charvak Reservoir from the beginning of 1932 until and including 2015, i.e. 84 years. The same type of preparatory analysis will now be carried out for the meteorological data. 4.1.3 Gap Filling Meteorological Data Here, we use precipitation and temperature data from Pskem (38462), Chatkal (38471) and Charvak Reservoir (38464) Meteorological Stations (see Chapter ?? for more information on these stations). We also have data from Oygaing station (Station Code 38339) but the record only starts in 1962 and the time resolution is monthly. Therefore, we do not take this station into account here for the time being. We start with precipitation and plot the available data. p_dec_tbl &lt;- data %&gt;% filter(type==&quot;P&quot; &amp; code!=&quot;38339&quot;) p_dec_tbl %&gt;% plot_time_series(date,data, .facet_vars = code, .interactive = TRUE, .smooth = FALSE, .title = &quot;&quot;, .y_lab = &quot;mm/decade&quot;, .x_lab = &quot;year&quot; ) Figure 4.3: Raw decadal precipitation data from Pskem (38462), Charvak Reservoir (38471) and Chatkal Meteo Station (38471). The precipitation data from these 3 stations shows some significant data gaps. The Chatkal Meteorological Station that is located in Kyrgyzstan apparently did not work in the post-transition years and continuous measurements were only resumed there in 1998. Let us see what happens if we were to use the same simple gap filling technique that we introduced above for discharge. p_dec_filled_tbl &lt;- p_dec_tbl p_dec_filled_tbl$data[is.na(p_dec_filled_tbl$data)] = p_dec_filled_tbl$norm[is.na(p_dec_filled_tbl$data)] p_dec_filled_tbl %&gt;% plot_time_series(date,data, .facet_vars = code, .interactive = TRUE, .smooth = FALSE, .title = &quot;&quot;, .y_lab = &quot;mm/decade&quot;, .x_lab = &quot;year&quot; ) Figure 4.4: Precipitation Data gap-filled with norms. The filled values from 1990 - 2000 in the case of the Station 38471 indicate that the norm-filling technique is not good. Closely inspect the significant data gap in the 1990ies at Station 38741 (tip: play around and zoom into the time series in the 1990ies in Figure 4.3 and comparing it with the resulting gap-filled timeseries in Figure ??. We see that our technique of gap filling with long-term norms is not suitable for this type of data and the significant gap size. The effect of variance reduction is also clearly visible. Hence, we resort to a more powerful gap filling technique that uses a (regression) model to impute the missing values from existing ones at the neighboring stations, i.e. Stations 38462 and 38464. To do so, we utilize the simputation R package that is tightly integrated in the tidyverse5. library(simputation) # First, we bring the data into the suitable format. p_dec_wide_tbl &lt;- p_dec_tbl %&gt;% mutate(code = paste0(&#39;P&#39;,code %&gt;% as.character())) %&gt;% dplyr::select(date,data,code) %&gt;% pivot_wider(names_from = &quot;code&quot;,values_from = &quot;data&quot;) # Second, we impute missing values. p_dec_filled_wide_tbl &lt;- p_dec_wide_tbl %&gt;% impute_rlm(P38471 ~ P38462 + P38464) %&gt;% # Imputing precipitation at station 38471 using a robust linear regression model impute_rlm(P38462 ~ P38471 + P38464) %&gt;% # Imputing precipitation at station 38462 using a robust linear regression model impute_rlm(P38464 ~ P38462 + P38471) # Imputing precipitation at station 38464 using a robust linear regression model p_dec_filled_long_tbl &lt;- p_dec_filled_wide_tbl %&gt;% pivot_longer(c(&#39;P38462&#39;,&#39;P38464&#39;,&#39;P38471&#39;)) p_dec_filled_long_tbl%&gt;% plot_time_series(date,value, .facet_vars = name, .interactive = TRUE, .smooth = FALSE, .title = &#39;&#39;, .y_lab = &quot;mm/decade&quot;, .x_lab = &quot;year&quot; ) Figure 4.5: Precipitation Data gap filled with a robust linear regression modeling approach As you can see, we use simple linear regression models to impute missing value in the target time series using observations from the neighboring stations. Through simple visual inspection, it becomes clear that this type of regression model for gap filling is better suited than the previous approach chosen. Let us check whether we could successfully fill all gaps with this robust linear regression approach. p_dec_filled_long_tbl %&gt;% group_by(name) %&gt;% summarize(n.na = sum(is.na(value)), n.na.perc = n.na/n()*100) ## # A tibble: 3 x 3 ## name n.na n.na.perc ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 P38462 12 0.402 ## 2 P38464 12 0.402 ## 3 P38471 3 0.100 It turns out that we still have very few gaps to deal with. We can see them by simply visualizing the wide tibble. The problem persisted at times when two or more values were missing across the available stations at the same time and where thus the linear regression could not be carried out. p_dec_filled_wide_tbl %&gt;% head(10) ## # A tibble: 10 x 4 ## date P38462 P38464 P38471 ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1933-01-10 NA NA 2 ## 2 1933-01-20 NA NA 10 ## 3 1933-01-31 NA NA 5 ## 4 1933-02-10 NA NA 33 ## 5 1933-02-20 NA NA 8 ## 6 1933-02-28 NA NA 10 ## 7 1933-03-10 NA NA 31 ## 8 1933-03-20 NA NA 50 ## 9 1933-03-31 NA NA 6 ## 10 1933-04-10 23 21.3 13 p_dec_filled_wide_tbl %&gt;% tail() ## # A tibble: 6 x 4 ## date P38462 P38464 P38471 ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2015-11-10 72 81 19 ## 2 2015-11-20 122 76 43 ## 3 2015-11-30 7 2 3 ## 4 2015-12-10 NA NA NA ## 5 2015-12-20 NA NA NA ## 6 2015-12-31 NA NA NA We can solve the issues related to the missing values at the start of the observation record by using the same technique as above and by only regressing P38462 and P38464 on P38471. p_dec_filled_wide_tbl &lt;- p_dec_filled_wide_tbl %&gt;% impute_rlm(P38462 ~ P38471) %&gt;% # Imputing precipitation at station 38462 using a robust linear regression model impute_rlm(P38464 ~ P38471) # Imputing precipitation at station 38464 using a robust linear regression model p_dec_filled_wide_tbl %&gt;% head(10) ## # A tibble: 10 x 4 ## date P38462 P38464 P38471 ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1933-01-10 5.60 5.08 2 ## 2 1933-01-20 18.3 16.7 10 ## 3 1933-01-31 10.4 9.46 5 ## 4 1933-02-10 54.9 50.3 33 ## 5 1933-02-20 15.2 13.8 8 ## 6 1933-02-28 18.3 16.7 10 ## 7 1933-03-10 51.8 47.3 31 ## 8 1933-03-20 82.0 75.0 50 ## 9 1933-03-31 12.0 10.9 6 ## 10 1933-04-10 23 21.3 13 Converse to this, the complete set of observations is missing for December 2015. We will thus remove these non-observations from our tibble. p_dec_filled_wide_tbl &lt;- p_dec_filled_wide_tbl %&gt;% na.omit() p_dec_filled_wide_tbl %&gt;% tail() ## # A tibble: 6 x 4 ## date P38462 P38464 P38471 ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2015-10-10 5 1 0 ## 2 2015-10-20 89 108 58 ## 3 2015-10-31 34 40 12 ## 4 2015-11-10 72 81 19 ## 5 2015-11-20 122 76 43 ## 6 2015-11-30 7 2 3 p_dec_filled_long_tbl &lt;- p_dec_filled_wide_tbl %&gt;% pivot_longer(-date) Inspecting the temperature data, we see similar data issues as in the precipitation data set. t_dec_tbl &lt;- data %&gt;% filter(type==&quot;T&quot;) t_dec_tbl %&gt;% plot_time_series(date,data, .facet_vars = code, .interactive = TRUE, .smooth = FALSE, .title = &#39;&#39;, .y_lab = &quot;deg. Celsius&quot;, .x_lab = &quot;year&quot; ) (#fig:rawData_T)Raw temperature data from the meteorological stations Pskem (38462) and Chatkal (38471) # First, we bring the data into the suitable format. t_dec_wide_tbl &lt;- t_dec_tbl %&gt;% mutate(code = paste0(&#39;T&#39;,code %&gt;% as.character())) %&gt;% dplyr::select(date,data,code) %&gt;% pivot_wider(names_from = &quot;code&quot;,values_from = &quot;data&quot;) # Second, we impute missing values. t_dec_filled_wide_tbl &lt;- t_dec_wide_tbl %&gt;% impute_rlm(T38471 ~ T38462) %&gt;% # Imputing precipitation at station 38471 using a robust linear regression model impute_rlm(T38462 ~ T38471) # Imputing precipitation at station 38462 using a robust linear regression model t_dec_filled_long_tbl &lt;- t_dec_filled_wide_tbl %&gt;% pivot_longer(c(&#39;T38462&#39;,&#39;T38471&#39;)) t_dec_filled_long_tbl%&gt;% plot_time_series(date,value, .facet_vars = name, .interactive = TRUE, .smooth = FALSE, .title = &#39;&#39;, .y_lab = &quot;deg. Celsius&quot;, .x_lab = &quot;year&quot; ) (#fig:rawData_T_rlm)Temperature data gap filled with robust linear regression modeling. There are some irregularities in the temperature time series of Chatkal Meteorological Station in the first decade of the 20th century (tip: zoom in to see these more clearly). Note that these were not introduced by the gap filling technique that we used but are most likely wrong temperature readings. We will return to these in the outlier analysis below in Section 4.1.4. t_dec_filled_long_tbl %&gt;% group_by(name) %&gt;% summarize(n.na = sum(is.na(value)), n.na.perc = n.na/n()*100) ## # A tibble: 2 x 3 ## name n.na n.na.perc ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 T38462 3 0.100 ## 2 T38471 3 0.100 To see where the missing value are, we find them easily again by looking at the head and tail of the tibble. t_dec_filled_wide_tbl %&gt;% head() ## # A tibble: 6 x 3 ## date T38462 T38471 ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1933-01-10 -6.9 -16.6 ## 2 1933-01-20 -6.1 -15.5 ## 3 1933-01-31 -6.3 -15.6 ## 4 1933-02-10 -2 -8.6 ## 5 1933-02-20 -3.3 -12.5 ## 6 1933-02-28 -0.1 -8.5 t_dec_filled_wide_tbl %&gt;% tail() ## # A tibble: 6 x 3 ## date T38462 T38471 ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2015-11-10 2.4 -2.5 ## 2 2015-11-20 2 -2.2 ## 3 2015-11-30 4.6 -3.7 ## 4 2015-12-10 NA NA ## 5 2015-12-20 NA NA ## 6 2015-12-31 NA NA Finally, we remove the non observations again as above with the function na.omit. t_dec_filled_wide_tbl &lt;- t_dec_filled_wide_tbl %&gt;% na.omit() t_dec_filled_long_tbl &lt;- t_dec_filled_wide_tbl %&gt;% pivot_longer(-date) To deal with the missing values at the end of the observational record, we could also have used any other technique. Using the norm values however would have artificially reduced the variance in both cases as explained above. Furthermore and at least in the case of temperature, it is also questionable to what extent a norm calculated over the last 84 years is still representative given global warming. We will look in this important and interesting topic in the next section. 4.1.4 Anomalies and Outliers We use the function timetk::plot_anomaly_diagnostics to investigate anomalies in the time series. For discharge, we first log-transform the raw data with the following transformation to reduce the variance of the original data. \\[ \\hat{q}(t) = log(q(t) + 1) \\] where \\(\\hat{q}(t)\\) denotes the transformed discharge. Prior to the log transformation, 1 is added so as to avoid cases where discharge would be 0 and the logarithmic transform thus undefined. The transformation can easily be done with the log1p() function in R. Backtransformation via the function expm1() simply involves taking the exponent and subtracting 1 from the result. Figure ?? shows the result. The exceptionally wet year 19169 shows up as anomalous in the Chatkal River Basin and at the downstream Charvak Reservoir inflow gauge. , ?? and ?? show anomalies diagnostics of the available data. q_dec_filled_long_tbl %&gt;% plot_anomaly_diagnostics(date, value %&gt;% log1p(), .facet_vars = name, .frequency = 36, .interactive = TRUE, .title = &quot;&quot;) Figure 4.6: Anomaly diagnostics of discharge data. The transparent grey band shows the width of the normal range. The highly anomalous wet year of 1969 is clearly visible in the discharge record of the Chatkal river basin (Station 16279). The investigation of precipitation anomalies shows a succession of regular anomalous wet events over time. It is interesting to see that the winter 1968/69 regularly anomalous at all three stations (Figure 4.7, zoom in to investigate). p_dec_filled_long_tbl %&gt;% plot_anomaly_diagnostics(date, value, .facet_vars = name, .interactive = TRUE, .title = &quot;&quot;) Figure 4.7: Anomaly diagnostics of precipitation data. While intuitively, we would have expected an eceptionally mild winter in 1968/69 due to the precipitation excess, the corresponding anomaly does not show up in the temperature record (Figure 4.8). t_dec_filled_long_tbl %&gt;% plot_anomaly_diagnostics(date,value, .facet_vars = name, .interactive = TRUE, .title = &quot;&quot;) Figure 4.8: Anomaly diagnostics of temperature data. Apart from the identification of extremal periods since as the 1969 discharge year in the Chatkal river basin, the diagnostics of anomalies also helps to identify likely erroneous data records. In Figure @ref(anomalies_T) for example, when we zoom into the data of the series T38471 in the first decade of the 21st century, problems in relation to positive anomalies during the winter are visible in 4 instances. One explanation would be that in at least some instances, the data are erroneously recorded as positive values when in fact they were negative (see dates ‘2002-01-31,’ ‘2005-01-10’ and ‘2007-02-28,’ Chatkal Station 38471). 4.1.5 Putting it all together Finally, we are now in the position to assemble all data that we will use for empirical modeling. The data is stored in long and wide form and used accordingly where required. For example, in Section @ref{TimeSeriesReg}, we are working with the wide data format to investigate model features in linear regression. Note that we also add a column with a decade identifier. Its use will become apparent in the Section 6.5 below. # Final concatenation data_wide_tbl &lt;- right_join(q_dec_filled_wide_tbl,p_dec_filled_wide_tbl,by=&#39;date&#39;) data_wide_tbl &lt;- right_join(data_wide_tbl,t_dec_filled_wide_tbl,by=&#39;date&#39;) # Add period identifiers (decades in this case) s &lt;- data_wide_tbl$date %&gt;% first() e &lt;- data_wide_tbl$date %&gt;% last() decs &lt;- decadeMaker(s,e,&#39;end&#39;) decs &lt;- decs %&gt;% rename(per=dec) data_wide_tbl &lt;- data_wide_tbl %&gt;% left_join(decs,by=&#39;date&#39;) # Creating long form data_long_tbl &lt;- data_wide_tbl %&gt;% pivot_longer(-date) # Cross checking completeness of record data_long_tbl %&gt;% group_by(name) %&gt;% summarize(n.na = sum(is.na(value)), n.na.perc = n.na/n()*100) ## # A tibble: 9 x 3 ## name n.na n.na.perc ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 P38462 0 0 ## 2 P38464 0 0 ## 3 P38471 0 0 ## 4 per 0 0 ## 5 Q16279 0 0 ## 6 Q16290 0 0 ## 7 Q16924 0 0 ## 8 T38462 0 0 ## 9 T38471 0 0 A consistent data record from 1933 until and including November 2015 is now prepared^Please note that by using left_join above, we have cut off discharge data from the year 1932 since we do not have meteorological data there.^. Let us analyze these data now. Where other data are used, their source and access options are indicated.↩︎ Please note that if you do not have the required package installed locally, you should install it prior to its use with the following command install.packages('simputation')↩︎ "],["Data-GeospatialData.html", "4.2 Geospatial Data", " 4.2 Geospatial Data This Section follows the steps shown in Figure ?? and explains in a hands-on manner how to arrive at and process the required geospatial data for later inclusion in the hydrological-hydraulic model RS MINVERVE. Links to detailed descriptions of the individual tasks allow even beginners of QGIS to follow the procedure. 4.2.1 Setting up QGIS Open and save a new QGIS project (How to for absolute beginners). Make sure that the maps projection is in UTM (How to change the projection of a project). The needed plugins are: qProf or profile tool (How to install and activate a plugin). The optional plugins are: SRTM plugin (requires a login to USGS Earth Explorer How to register). Note, SAGA routines must be installed and correctly configured in QGIS. 4.2.2 Load DEM We use the SRTM 1 arc-second global DEM (Data description). The DEM tiles are downloaded directly in QGIS3 (SRTM plugin, How to) or via the USGS Earth Explorer (How to) and merged to a single GIS layer (How to). In any case you will need to register for an Earth Explorer account How to register. 4.2.3 Derive the boundaries of your catchment If you already have a shapefile for the boundaries of your catchment. Load it into QGIS How to add a vector layer. Otherwise, follow this tutorial on youtube to derive the boundaries of your catchment. 4.2.4 Run through the process model A process model is available that generates the raw GIS layers for RS Minerve. Navigate to Processing and open the Graphical Modeller…. The Model Designer window opens. Open the model Link to Model (4.9). Figure 4.9: The process of deriving the GIS layers for RS Minerve is simplified through the process model. *TODO: Make file available for download. Prior to running the model, read through the following brief notes. 4.2.4.1 Input files A shape file of the basin outline in UTM 42 N (How to derive the basin outline). The extent of the DEM needs to be larger than the boundaries of your catchment. You can use your raw SRTM DEM projected to UTM42N. 4.2.4.2 Model output Besides the 3 layers required (Basins, rivers, junctions, ?? in the Introduction), the model produces a number of additional output files which can be used for verifying the parameterisation which is described in more detail in the next section. 4.2.4.3 Parameterization of the model BasinShapeBuffer_meters is a buffer around the basin outline to which the DEM is cut. The DEM needs to be slightly larger than the shape line. The default value of 1000 m is sufficient. Can this parameter be internalised? River Network Level is a parameters for the resolution of the river network. Values of 7 and 8 have shown good results for smaller and medium sized catchments (4.10). Figure 4.10: Comparison of River Network Level = 7 (left) and River Network Level = 8 (rigth) for the example of the Pskem river catchment. The Channel Network Cutoff Value is a parameter required for the partitioning of the catchment into sub-catchments. Values between 2e8 and 5e8 have been found to work well for smaller and medium sized catchments. The model is sensitive to this parameter and might throw an error if the cutoff value is not appropriate. The Elevation Bands Table holds the altitude boundaries and class IDs for the elevation bands in the catchment. After a first test run, edit according to your needs. The model is sensitive to these parameters. Play around with them until you are satisfied with the resulting GIS layers. If you get an error message running Fix geometries that POLYGONS.shp was not found, try reducing the Channel Network Cutoff Value by 50-70%. You can still inspect the other layers and change other parameters as well. - Make sure that all input layers are projected to UTM 42 N. - Run the model through once, inspect the output and play with the input parameters: - Min and Max elevations on the smoothed DEM and the shape, number and spacing of elevation bands (edit the table for the elevation bands accordingly), - Location and shape of river reaches and the location of junctions (vary the threshold). 4.2.5 Manually edit the GIS Layers for import to RS Minerve Review ?? Elevation bands per sub-basin need to be in one Multi-Polygon layer. We need junctions only at the confluences of sub-basins (and river reaches). We only need river reaches where we need to do routing. The sub-basins at the upper-most reaches do not need river reaches. 4.2.5.1 Edit Junctions layer Select the Junctions layer and toggle manual editing by clicking on the yellow pen (4.11). Figure 4.11: Manually edit the layer with the river junctions, step 1: Toggle layer editing. When in editing mode, the yellow pen will appear in the Layers window next to the name of the layer being edited. The edit mode will also activate a button for adding points (i.e. junctions, we don’t need that now) and the vertex tool. Click on the vertex tool icon. It is active when a a boundary appears around the icon and the Vertex Editor windows opens (4.12). Figure 4.12: Manually edit the layer with the river junctions, step 2: Activate the vertex tool. Right-click on a junction point you would like to delete to activate it (4.13). Figure 4.13: Manually edit the layer with the river junctions, step 3: Activate a junction node for editing. Select the activated point by drawing a rectangle over the point with your mouse. The point will appear blue (4.14). Figure 4.14: Manually edit the layer with the river junctions, step 3: Select the activated junction node. Delete the point with the delete key on your keyboard. You can save your edits by pressing the blue-white Save Layer Edits button that is decorated with an orange pen (4.15). This saves your changes without exiting the edit mode. Figure 4.15: Manually edit the layer with the river junctions, step 3: Save edits. If you have many points to remove, as in our case, it may be faster to identify the IDs of the features you want to keep, select these and delete all others. To start, you activate the Identify Features mode by clicking on the icon with the white i on the blue circle (4.16). Figure 4.16: Alternative method to manually edit junctions if many nodes need to be deleted. Step 1: Get ID of features to keep, part 1. A black i will appear next to your cursor. You then click on the first of your nodes that you want to keep. This will highlight it in red and a list with information on the selected feature appears on the right in the Identify Results window. You will see the attribute NODE_ID with value 1 for the outflow node (4.17). Note down the ID of the feature. Figure 4.17: Alternative method to manually edit junctions if many nodes need to be deleted. Step 1: Get ID of features to keep, part 2. You then press on the node at the confluence of the two tributaries in the center of the catchment. The Identify Results window shows 2 results, that means, that two junction nodes are close to each other (4.18). Figure 4.18: Alternative method to manually edit junctions if many nodes need to be deleted. Step 1: Get ID of features to keep, part 3. Zoom in in your map window with your mouse to see the two nodes (4.19). Figure 4.19: Alternative method to manually edit junctions if many nodes need to be deleted. Step 1: Get ID of features to keep, part 4. Select the node that should be kept and not the ID of the node (NODE_ID 11), (4.20). Figure 4.20: Alternative method to manually edit junctions if many nodes need to be deleted. Step 1: Get ID of features to keep, part 5. Zoom back to the entire Junctions layer (see ??). and go to Select Features by Values… in the toolbar (4.21). Figure 4.21: Alternative method to manually edit junctions if many nodes need to be deleted. Step 2: Select features to delete, part 1 Add NODE_ID 1 to your selection as demonstrated in (4.22). Figure 4.22: Alternative method to manually edit junctions if many nodes need to be deleted. Step 2: Select features to delete, part 2 The outflow node with ID 1 will change color in your map (4.23). Figure 4.23: Alternative method to manually edit junctions if many nodes need to be deleted. Step 2: Select features to delete, part 3 Add node 11 to your selection in the same way and close the Select Node by Value window. Invert the feature selection as shown in (4.24). Figure 4.24: Alternative method to manually edit junctions if many nodes need to be deleted. Step 2: Select features to delete, part 4 All other nodes will now be yellow and the ones to keep will appear in the layer color (4.25). Figure 4.25: Alternative method to manually edit junctions if many nodes need to be deleted. Step 2: Select features to delete, part 5 Delete the features (nodes) by pressing the Delete Selected button in the edit features toolbar (4.26). Figure 4.26: Alternative method to manually edit junctions if many nodes need to be deleted. Step 2: Select features to delete, part 6 Save your edits (4.15). To verify that, indeed, all superfluous nodes are deleted from the Junctions file, open the Attribute table (4.27). Figure 4.27: Edit Attribute table. Step 1: Open the attribute table with a click on the Attribute Table icon in the QGIS toolbar. Only 2 features should be listed under each attribute. We will now edit the attribute table to prepare it for the RS Minerve model (see ??). RS Minerve needs an identifyer to differentiate between the junctions. We can use the attribute TYPE to uniquely identify the two junctions needed. RS Minerve further needs the ID of the downstream river. Add a collumn to the attribute table by pressing the Add Field button (4.28). Figure 4.28: Edit Attribute table. Step 2: Add a column to the attribute table manually, part 1. Define a name for the attribute, a type and admissible length of each entry in the Add Field window. In our case, we choose a string (a word) as ID and allow it to be 20 characters long (4.29). Figure 4.29: Edit Attribute table. Step 2: Add a column to the attribute table manually, part 2. Close the window by pressing OK. By clicking in the newly created NULL fields, you can now type names for the downstream rivers and save your edits by pressing the save edits icon (3rd from the left in the toolbar of the attribute table window). As the outlet of the catchment goes directly into Charvak reservoir, we can type Charvak as the downstream river ID. The river stretch between junction and outlet is called Pskem (4.30). Figure 4.30: Edit Attribute table. Step 2: Add a column to the attribute table manually, part 3. We are done editing the Junctions layer. Deactivate the edit mode by clickig on the yellow pen in the attribute table window (4.31) and close the window. Figure 4.31: Save your edits. Now save the Junctions layer in an appropriate place on your drive and save your project. 4.2.5.2 Edit Channels layer Proceed in the same way as for the junctions: Delete channel sections that are not needed in the Minerve model. "],["Data-ClimateReanalysisData.html", "4.3 Climate Reanalysis Data", " 4.3 Climate Reanalysis Data Information about the spatio-temporal distribution of precipitation (P) and temperature (T) is vital for water balance studies, including for modeling. Poorly gauged basins do not have dense enough ground-based monitoring network that would allow to obtain reliable meteorological fields that can be used to drive hydrological models. Station data are especially poor in complex and remote mountain catchments in developing and transition countries. The Central Asian river basins are examples of such basins. Global reanalysis data can help to cover these existing gaps. In this Chapter, we show how this can be achieved. We use ERA5 global reanalysis data to obtain temperature at 2 meters above ground and total precipitation fields on an hourly base from 1981-01-01 through 2020-12-31. ERA5 data comes with a 30km grid cell size resolution and is thus quite course. This is relevant for complex mountain terrains which feature highly variable climate over very short distances, sometimes from one valley to the next. To address this problem, a bias correct version of the monthly CHELSA high resolution climatology product is used to arrive at high resolution hourly climatology fields (the CHELSA dataset is described in (Karger et al. 2017) and the bias correction of it in (Beck et al. 2020b). Like this, high-resolution hourly data from 1981-01-01 through 2013-12-31, i.e. the period for which the CHELSA dataset is available, could be derived. These data are then used for model calibration and validation and for the computation of the reference hydro-climatological situation from 1981 through 2013. More details can be found in Chapter 5. The analysis present here is for the Gunt river catchment. The approach and methods we have developed are universally applicable for other catchments. 4.3.1 CHELSA V1.2.1 Data and Bias Correction The CHELSA V1.2.1 data covers the period 1981-01-01 until 2013-12-31. This period is considered to be the base climate period where 1981-01-01 until 1993-12-31 will be used as the parameter calibration period for the hydrological model. The model validation period is from 1994-01-01 until 2013-12-31. This period will constitute the baseline period in relation to which climatic changes in the mid 21st as well as end of 21st century will be assessed. Mean monthly temperature and total monthly precipitation between 1981-01-01 and 2013-01-01 can be accessed and downloaded for the Central Asia domain from the following online repository6. Mean monthly temperatures are stored in the ./t2m/ folder and precipitation in the ./pr/ folder there. Note that the Central Asia domain is defined as test aoi_CentralAsia_LatLon &lt;- extent(c(65,80.05,35.95,44.05)) # this is a raster::extent() object. For more information, type ?extent into the console. The CHELSA data are downscaled ERA-INTERIM model outputs for temperature and precipitation with a resolution of 30 arc seconds (Karger et al. 2017). Temperature fields are statistically downscaled ERA-INTERIM temperatures whereas for precipitation downscaling, several orographic predictors are taken into account, including for example wind fields, valley exposition, height of the atmospheric boundary layer, etc. Because of its highly resolved climatologies, the CHELSA data has been shown to be particularly useful for studies in regions with complex topography. However, it has also been shown that the original CHELSA precipitation data is underestimating high mountain precipitation. This can be explained by the phenomenon of snow undercatch which explains measured precipitation deficits by sublimation and blowing snow transport at high altitude meteorological stations. An example of this is shown in Figure 4.32 for high elevation gauges in Spain. In a recent intercomparison project carried out in Spain, it has been shown that undercatch poses significant problems in accurately measuring solid precipitation (Buisán et al. 2017) in mountainous regions. Both, ERA-INTERIM and CHELSA themselves assimilate station data in their models and hence are affected by these erroneous measurements. Figure 4.32: Measured snow undercatch values in high-mountain stations in Spain. The values were determined within the World Meteorological Organization Solid Precipitation Intercomparison Experiment (WMO-SPICE). See text for more information and reference. Beck et al. (2020a) has recognized this and released monthly correction factors for the CHELSA data (see Figure 4.33). Figure 4.33: Figure from (Beck et al. 2020a), Supplementary Material. Plate d): Best estimate of global bias correction factors. Plate e): Lower bound estimate of global bias correction factors. Plate f): Upper bound of global bias correction factors. As is clearly visible, bias correction factors in high-mountain Asia, including the parts of Central Asia are significant. The bias corrected CHELSA precipitation (tp) raster data is available via this Dropbox link. Using these temperature as well bias-corrected precipitation data, we can easily compute and display the monthly norm climatology fields over the Central Asia domain7. Here, we just load them and visualize the mean monthly patterns for a consistency check. t2m_meanMonthlyClimate_CHELSA &lt;- raster::brick(&#39;./data/CentralAsiaDomain/CHELSA_V1.2.1/t2m_climatology/t2m_climatology_CA.tif&#39;) names(t2m_meanMonthlyClimate_CHELSA) &lt;- month.abb t2m_meanMonthlyClimate_CHELSA &lt;- t2m_meanMonthlyClimate_CHELSA / 10 - 273.15 # now, the CHELSA data is in deg. C temperature_colors &lt;- brewer.pal(9, &quot;RdYlBu&quot;) %&gt;% colorRampPalette() gplot(t2m_meanMonthlyClimate_CHELSA) + geom_tile(aes(fill = value)) + facet_wrap(~ variable) + scale_fill_gradientn(colours = rev(temperature_colors(5))) + coord_equal() + guides(fill=guide_colorbar(title=&#39;T [deg. C.]&#39;)) Figure 4.34: CHELSA v1.2.1 mean monthly temperature climatology is shown. In a similar way, the bias corrected precipitation climatology can be plotted. Figure 4.35 nicely shows the main precipitation months of the key mountain ranges in the region. The Figure shows that March and April are normally the main precipitation months. # Load file pbcorr_pr_meanMonthlyClimate_CHELSA &lt;- raster::brick(&#39;./data/CentralAsiaDomain/CHELSA_V1.2.1/pr_climatology/pr_climatology_CA.nc&#39;) # Layer names names(pbcorr_pr_meanMonthlyClimate_CHELSA) &lt;- month.abb # Color palette precipitation_colors &lt;- brewer.pal(9, &quot;YlGnBu&quot;) %&gt;% colorRampPalette() # Plot gplot(pbcorr_pr_meanMonthlyClimate_CHELSA) + geom_tile(aes(fill = value)) + facet_wrap(~ variable) + scale_fill_gradientn(colours = precipitation_colors(500)) + coord_equal() + guides(fill=guide_colorbar(title=&#39;P [mm/yr]&#39;)) Figure 4.35: CHELSA v1.2.1 mean monthly precipitation climatology is shown. How can the quality of the CHELSA data in the complex Central Asia domain be assessed? We explore this question the validity of the CHELSA dataset to be able to adequately represent the high-mountain climate in the Pamirs. The key questions here to be answered are does the magnitude of the precipitation yield physically meaningful results, and does the climatology adequately reproduce the seasonal cycle observed one at the stations? Let us address the first question investigating bias corrected precipitation values and comparing these discharge for the Gunt river basin. If \\(P &gt;Q\\) where \\(P\\) is the long-term mean precipitation and \\(Q\\) is the long-term mean discharge, we can confidently say that the bias corrected CHELSA precipitation product is meaningful from a water balance perspective (see also Chapter 3.3 for more information). # Load catchment shp gunt_Shapefile &lt;- st_read(&#39;./data/AmuDarya/Gunt/GeospatialData/Gunt_Basin_poly.shp&#39;,quiet = TRUE) gunt_Shapefile &lt;- gunt_Shapefile %&gt;% subset(fid==2) gunt_Shapefile_LatLon &lt;- st_transform(gunt_Shapefile,crs = st_crs(4326)) areaGunt &lt;- gunt_Shapefile %&gt;% st_area() %&gt;% as.numeric() # Areas of Interest aoi_CentralAsia_LatLon &lt;- extent(c(65,80.05,35.95,44.05)) # in lat/lon aoi_Basin_LatLon &lt;- gunt_Shapefile_LatLon %&gt;% extent() # GUNT aoi_Basin_UTM &lt;- gunt_Shapefile %&gt;% extent() # GUNT, in UTM fLoc &lt;- &#39;./data/AmuDarya/Gunt/ReanalysisData/tp_bcorr_NORM_CHELSA_Gunt.nc&#39; chelsaP_GUNT_corr_raster &lt;- brick(fLoc, varname=&quot;corr_P_annual&quot;) chelsaP_GUNT_corr__spdf &lt;- as(chelsaP_GUNT_corr_raster, &quot;SpatialPixelsDataFrame&quot;) chelsaP_GUNT_corr__df &lt;- as.data.frame(chelsaP_GUNT_corr__spdf) colnames(chelsaP_GUNT_corr__df) &lt;- c(&quot;value&quot;, &quot;x&quot;, &quot;y&quot;) # Plot the raster for inspection and analysis ggplot() + geom_tile(data=chelsaP_GUNT_corr__df, aes(x=x, y=y, fill=value), alpha=0.8)+ geom_sf(data=gunt_Shapefile,color=&quot;black&quot;,fill=NA) + scale_fill_gradientn(colours = precipitation_colors(5)) + xlab(&quot;Longitude&quot;) + ylab(&quot;Latitude&quot;) + guides(fill=guide_colorbar(title=&quot;P Norm [mm]&quot;)) + ggtitle(&quot;Bias Corrected CHELSA v1.2.1 Norm Precipitation, Gunt River Basin&quot;) Figure 4.36: Long-term mean precipitation climatology of the Gunt river basin in the Pamir mountains. The catchment is delineated by the black polygon. The mean long-term precipitation in the catchment is 349 mm/year. # Extract raster values inside basin polygon and convert to equivalent water height rasterRes &lt;- chelsaP_GUNT_corr_raster %&gt;% res() rasterCellArea &lt;- rasterRes[1] * rasterRes[2] # in m^2 basin_P &lt;- raster::extract(chelsaP_GUNT_corr_raster,gunt_Shapefile)[[1]] * rasterCellArea / 1000 basin_P &lt;- basin_P %&gt;% sum() / areaGunt * 1000 # now in mm #print(paste0(&quot;The bias corrected CHELSA v.1.2 norm precipitation in Gunt River Basin is &quot;, basin_P %&gt;% round(0), &#39; mm&#39;)) From the perspective of the water balance, the basin-wide long-term mean precipitation estimate passes the test since P (353 mm) &gt; Q (234 mm) where the latter is the long-term discharge norm at the Khorog gauging station at the outlet of the basin (see also Chapter 3 for more information on Gunt river basin). The water balance components are also discussed in Chapter 3.3. As an aside, the bias corrected precipitation climatology shows an interesting feature of the Gunt river basin (see Figure 4.36). Namely, there is a stark precipitation gradient between the western part of the basin where the bulk of the precipitation is observed and the hyper-arid Pamir plateau region to the east, where annual precipitation is around or below 200 mm. What about the seasonality of the CHELSA product? Can it adequately reproduce the observed precipitation seasonality? If this would not be the case, we would have to reject the validity of the product and explore other high-resolution climatologies such as WorldClim V2 or CHPClim V1 (see (Beck et al. 2020c) for more information on these products). Lets explore again for Gunt river basin. First, we load and prepare all the required station precipitation and geospatial data. Then, we compute the monthly norms of these data for the period 1981-01-01 through 2013-12-31. # Get data (this time, we access the monthly norm data through the specification of the variable name, that we want to load, i.e. varname=&quot;corr_P_monthly&quot;) fLoc &lt;- &#39;./data/AmuDarya/Gunt/ReanalysisData/tp_bcorr_monthly_CHELSA_Gunt.nc&#39; chelsaP_GUNT_monthly_corr_raster_proj &lt;- brick(fLoc, varname=&quot;corr_P_monthly&quot;) # Extract Gunt river basin cells and compute monthly totals chelsa_monthly_norm_P &lt;- extract(chelsaP_GUNT_monthly_corr_raster_proj,gunt_Shapefile)[[1]] * rasterCellArea / 1000 chelsa_monthly_norm_P &lt;- (chelsa_monthly_norm_P %&gt;% colSums() / areaGunt * 1000) %&gt;% unname() # now in mm chelsa_monthly_norm_P &lt;- chelsa_monthly_norm_P %&gt;% as_tibble() %&gt;% rename(CHELSA_norm_P=value) # Load Gunt Station Data record fPath = &#39;./data/AmuDarya/Gunt/StationData/gunt_data_cleaned.Rds&#39; data &lt;- read_rds(fPath) # Prepare the station data precipitation record P_38950 &lt;- data %&gt;% filter(type==&quot;P&quot; &amp; code==38950) %&gt;% dplyr::select(date,data) %&gt;% rename(P_38950=data) P_38953 &lt;- data %&gt;% filter(type==&quot;P&quot; &amp; code==38953) %&gt;% dplyr::select(date,data) %&gt;% rename(P_38953=data) P_38954 &lt;- data %&gt;% filter(type==&quot;P&quot; &amp; code==38954) %&gt;% dplyr::select(date,data) %&gt;% rename(P_38954=data) P_38956 &lt;- data %&gt;% filter(type==&quot;P&quot; &amp; code==38956) %&gt;% dplyr::select(date,data) %&gt;% rename(P_38956=data) P &lt;- full_join(P_38950,P_38953,by=&quot;date&quot;) P &lt;- full_join(P,P_38954,by=&quot;date&quot;) P &lt;- full_join(P,P_38956,by=&quot;date&quot;) # add a month identifier P &lt;- P %&gt;% filter(date&gt;=as.Date(&quot;1981-01-01&quot;) &amp; date&lt;=as.Date(&quot;2013-12-31&quot;)) %&gt;% mutate(mon = month(date)) %&gt;% dplyr::select(-date) P &lt;- P %&gt;% pivot_longer(-mon) %&gt;% group_by(mon) # Now, make a nice plot which compares with the monthly means average over all stations in the catchment. station_monthly_norm_P &lt;- P %&gt;% summarise(Station_norm_P = mean(value,na.rm=TRUE)) %&gt;% dplyr::select(-mon) # Join data norm_P_data &lt;- chelsa_monthly_norm_P %&gt;% add_column(mon=seq(1,12,1),station_monthly_norm_P,.before = 1) # Prepare for plotting norm_P_data_long &lt;- norm_P_data %&gt;% pivot_longer(-mon) # Plot ggplot(norm_P_data_long,aes(x=mon,y=value,color = name)) + geom_line() + xlab(&quot;Month&quot;) + ylab(&quot;mm/month&quot;) Figure 4.37: Monthly precipitation norms of the bias corrected CHELSA v1.2.1 dataset and the mean monthly precipitation averaged over the four meteorological stations in the Gunt river basin. The seasonality of the precipitation is in excellent agreement between the observed data and the bias corrected CHELSA precipitation product. One should not be misled by the offset in absolute terms between the two datasets since since one is data derived from stations at particular locations and the other is average high-resolution gridded data. A better way to plot the comparison of the seasonality of the two products would be to center and standardize the two datasets. This can be achieved the following way. norm_P_data_zscore &lt;- norm_P_data %&gt;% mutate(Station_norm_P_zscore = (Station_norm_P - mean(Station_norm_P))/ sd(Station_norm_P)) %&gt;% mutate(CHELSA_norm_P_zscore = (CHELSA_norm_P - mean(CHELSA_norm_P)) / sd(CHELSA_norm_P)) norm_P_data_zscore_long &lt;- norm_P_data_zscore %&gt;% dplyr::select(-Station_norm_P,-CHELSA_norm_P) %&gt;% pivot_longer(-mon) ggplot(norm_P_data_zscore_long,aes(x=mon,y=value,color = name)) + geom_line() + xlab(&quot;Month&quot;) + ylab(&quot;[-]&quot;) Figure 4.38: Centered and standardized precipitation norms for the comparison of the seasonality of the two products.The bias corrected CHELSA product reproduces the precipitation seasonlity in an excellent manner. Note that the values on the y-axis do not have any units as a result of the standarization. To summarize, with the CHELSA v1.2.1 dataset, we have downloaded and prepared a high spatial resolution climatology for the Central Asia domain. As this product is derived from reanalysis data that mixes station data with climate model output, the precipitation product was bias corrected for snow undercatch that causes the original reanalysis data to greatly underestimate high-elevation precipitation data. As show for Gunt river basin in the Pamirs, the resulting temperature and precipitation climatologies compare in an excellent manner with seasonalities observed at local meteorological stations. For hydrological modeling, however, we need not only high spatial resolution data but also high temporal resolution data, ideally at hourly time-steps, to drive the hydrological model. In the next Section @ref(Data::ERA5), we show how hourly ERA5 reanalysis data fields can be resampled and rescaled so that mean or total monthly values match CHELSA v.1.2.1 at particular raster cells. Like this, we will arrive at a dataset with high temporal and spatial resolution. 4.3.2 ERA5 Download and Data Resampling with PBCORR CHELSA 4.3.2.1 ERA5 Background ERA5 data from 1981-01-01 through 2013-12-31 for the Central Asia domain is available for download under this link. Hourly temperature 2 meters above the surface (t2m) and hourly precipitation (tp) totals can be downloaded there8. More information about ERA5 data can be found on the official website of the European Center of Medium Weather Forecast. With the Lobelia Past Climate Explorer, one can easily get key statistics of any of the ERA5 raster cells globally. Try it! Go to the Central Asia domain, search for your place of interest (i.e. your home, the Gunt river basin, Chirchik river basin, etc.) and click on the point where you would like to see the statistics. See also Figure 4.39 knitr::include_graphics(&quot;_bookdown_files/FIG_DATA/lobelia_era5.jpg&quot;) Figure 4.39: Screenshot from the Lovelia ERA5 website with the ERA5 temperature statistics of Khorog selected. ERA5 data can be downloaded from the Copernicus Climate Change Service (C3S) Climate Data Store. We use the ERA5-Land hourly data from 1981 product. Note that if you want to download the data yourself from the Data Store, you need to register there. knitr::include_graphics(&#39;_bookdown_files/FIG_DATA/CDStore_ERA5.jpg&#39;) Figure 4.40: Screenshot from the Copernicus Climate Data Store. One needs to register first if you want to download data yourself from the Climate Data Store (click on Login/register button. The product which we download is highlighted. If you click on it, then the detailed data page comes up. As per the ECMWF description, “ERA5-Land is a reanalysis dataset providing a consistent view of the evolution of land variables over several decades at an enhanced resolution compared to ERA5. ERA5-Land has been produced by replaying the land component of the ECMWF ERA5 climate reanalysis. Reanalysis combines model data with observations from across the world into a globally complete and consistent dataset using the laws of physics. Reanalysis produces data that goes several decades back in time, providing an accurate description of the climate of the past.” For more information, please visit the detailed data description webpage. The variables t2m and tp which you can access via this link are described described in greater detail on the site. For reference, these data specifications are copied in the Table below. ERA5-Land t2m and pr variables descriptions. Name Units Description 2m temperature (t2m) K Temperature of air at 2m above the surface of land, sea or in-land waters. 2m temperature is calculated by interpolating between the lowest model level and the Earth’s surface, taking account of the atmospheric conditions. Temperature measured in kelvin can be converted to degrees Celsius (°C) by subtracting 273.15. Total precipitation (tp) m Accumulated liquid and frozen water, including rain and snow, that falls to the Earth’s surface. It is the sum of large-scale precipitation (that precipitation which is generated by large-scale weather patterns, such as troughs and cold fronts) and convective precipitation (generated by convection which occurs when air at lower levels in the atmosphere is warmer and less dense than the air above, so it rises). Precipitation variables do not include fog, dew or the precipitation that evaporates in the atmosphere before it lands at the surface of the Earth. This variable is accumulated from the beginning of the forecast time to the end of the forecast step. The units of precipitation are depth in meters. It is the depth the water would have if it were spread evenly over the grid box. Care should be taken when comparing model variables with observations, because observations are often local to a particular point in space and time, rather than representing averages over a model grid box and model time step. 4.3.2.2 ERA5-Land Download Note: This Section just demonstrates how you can order data from Copernicus Climate Data Store. It is not required for you to carry this out in order to access the Central Asia 2m temperature and total precipitation data that has been prepared for this course and is available for download here. The manual data download from the Climate Data Store is tedious. We show here how, with an R-script, you can place a data order and then, once the data is prepared, download it from the Data Store. The following code segments show to download daily ERA5 t2m and tp data that is segmented into yearly netCDF-files. Like this, these become manageable for later processing. Note that the two code segments start the scripts on the side of the Climate Data Store data service. Following the submission of jobs and after a wait of a couple of hours, the data can then be downloaded at https://cds.climate.copernicus.eu/cdsapp#!/yourrequests manually and stored in the relevant location. Note that for these scripts to run, you need to give your own credentials that you obtain after login in the Climate Data Store. Under the assumption that you have typed in your credentials (user name, ID and key), 2 meter temperature data can be downloaded in the following way. # Required library library(ecmwfr) # Prepare data download of ERA5 by adding login credentials to the keychain. wf_set_key(user = &#39;&lt;Your user name&gt;&#39;, key = &quot;Your key&quot;, service = &quot;webapi&quot;) downloadList_t2m &lt;- 1981:2013 # The years for which CHELSA v1.2.1 is available are chosen for download. These will be used for the calibration and vaildation of the hydrological model and constitute the reference hydrological period. for (yr in downloadList_t2m){ CentralAsia_ERA5_hourly_request &lt;- list( format = &quot;netcdf&quot;, variable = c(&quot;2m_temperature&quot;), year = as.character(yr), month = c(&quot;01&quot;, &quot;02&quot;, &quot;03&quot;, &quot;04&quot;, &quot;05&quot;, &quot;06&quot;, &quot;07&quot;, &quot;08&quot;, &quot;09&quot;, &quot;10&quot;, &quot;11&quot;, &quot;12&quot;), day = c(&quot;01&quot;, &quot;02&quot;, &quot;03&quot;, &quot;04&quot;, &quot;05&quot;, &quot;06&quot;, &quot;07&quot;, &quot;08&quot;, &quot;09&quot;, &quot;10&quot;, &quot;11&quot;, &quot;12&quot;, &quot;13&quot;, &quot;14&quot;, &quot;15&quot;, &quot;16&quot;, &quot;17&quot;, &quot;18&quot;, &quot;19&quot;, &quot;20&quot;, &quot;21&quot;, &quot;22&quot;, &quot;23&quot;, &quot;24&quot;, &quot;25&quot;, &quot;26&quot;, &quot;27&quot;, &quot;28&quot;, &quot;29&quot;, &quot;30&quot;, &quot;31&quot;), time = c(&quot;00:00&quot;, &quot;01:00&quot;, &quot;02:00&quot;, &quot;03:00&quot;, &quot;04:00&quot;, &quot;05:00&quot;, &quot;06:00&quot;, &quot;07:00&quot;, &quot;08:00&quot;, &quot;09:00&quot;, &quot;10:00&quot;, &quot;11:00&quot;, &quot;12:00&quot;, &quot;13:00&quot;, &quot;14:00&quot;, &quot;15:00&quot;, &quot;16:00&quot;, &quot;17:00&quot;, &quot;18:00&quot;, &quot;19:00&quot;, &quot;20:00&quot;, &quot;21:00&quot;, &quot;22:00&quot;, &quot;23:00&quot;), area = c(45, 64, 34, 81), dataset_short_name = &quot;reanalysis-era5-land&quot;, target = paste0(&quot;CentralAsia_ERA5_hourly_&quot;,as.character(yr),&#39;.nc&#39;)) file &lt;- wf_request(user = &quot;Your User ID&quot;, # user ID (for authentication) request = CentralAsia_ERA5_hourly_request, # the request name (can be anything) transfer = FALSE, # put it in the queue and download manually from the website path = &quot;.&quot;, job_name = paste0(&quot;t2m_CentralAsia_ERA5_hourly_&quot;,as.character(yr),&#39;.nc&#39;)) } Similarly, total precipitation is downloaded with the following code. downloadList_tp &lt;- 1981:2013 for (yr in downloadList_tp){ CentralAsia_ERA5_hourly_request &lt;- list( format = &quot;netcdf&quot;, variable = c(&quot;total_precipitation&quot;), year = as.character(yr), month = c(&quot;01&quot;, &quot;02&quot;, &quot;03&quot;, &quot;04&quot;, &quot;05&quot;, &quot;06&quot;, &quot;07&quot;, &quot;08&quot;, &quot;09&quot;, &quot;10&quot;, &quot;11&quot;, &quot;12&quot;), day = c(&quot;01&quot;, &quot;02&quot;, &quot;03&quot;, &quot;04&quot;, &quot;05&quot;, &quot;06&quot;, &quot;07&quot;, &quot;08&quot;, &quot;09&quot;, &quot;10&quot;, &quot;11&quot;, &quot;12&quot;, &quot;13&quot;, &quot;14&quot;, &quot;15&quot;, &quot;16&quot;, &quot;17&quot;, &quot;18&quot;, &quot;19&quot;, &quot;20&quot;, &quot;21&quot;, &quot;22&quot;, &quot;23&quot;, &quot;24&quot;, &quot;25&quot;, &quot;26&quot;, &quot;27&quot;, &quot;28&quot;, &quot;29&quot;, &quot;30&quot;, &quot;31&quot;), time = c(&quot;00:00&quot;, &quot;01:00&quot;, &quot;02:00&quot;, &quot;03:00&quot;, &quot;04:00&quot;, &quot;05:00&quot;, &quot;06:00&quot;, &quot;07:00&quot;, &quot;08:00&quot;, &quot;09:00&quot;, &quot;10:00&quot;, &quot;11:00&quot;, &quot;12:00&quot;, &quot;13:00&quot;, &quot;14:00&quot;, &quot;15:00&quot;, &quot;16:00&quot;, &quot;17:00&quot;, &quot;18:00&quot;, &quot;19:00&quot;, &quot;20:00&quot;, &quot;21:00&quot;, &quot;22:00&quot;, &quot;23:00&quot;), area = c(45, 64, 34, 81), dataset_short_name = &quot;reanalysis-era5-land&quot;, target = paste0(&quot;CentralAsia_ERA5_hourly_&quot;,as.character(yr),&#39;.nc&#39;)) file &lt;- wf_request(user = &quot;11732&quot;, # user ID (for authentication) request = CentralAsia_ERA5_hourly_request, # the request transfer = FALSE, # put it in the queue and download manually from the website path = &quot;.&quot;, job_name = paste0(&quot;tp_CentralAsia_ERA5_hourly_&quot;,as.character(yr),&#39;.nc&#39;)) } 4.3.2.3 Rescaling ERA5 to monthly CHELSA values ERA5 original data can easily be rescaled (bias corrected) to CHELSA data values for any river basin in the Central Asia domain using the convenience function riversCentralAsia::biasCorrect_ERA5_CHELSA(). Type ?biasCorrect_ERA5_CHELSA to get more information about the function and its arguments. The code block below shows how to rescale (bias correct) total precipitation. It assumes that the ERA5 directory contains two subfolders ERA5/tp/ and ERA5/t2m where the corresponding data are stored. # Specify biasCorrect_ERA5_CHELSA() function arguments basinName &lt;- &#39;Gunt&#39; dataType_ERA5 &lt;- &#39;tp&#39; # note that ERA5 tp units are in m! ## Directories of relevant climate files - function arguments dir_ERA5_hourly &lt;- &#39;../HydrologicalModeling_CentralAsia_Data/CentralAsiaDomain/ERA5/&#39; dir_CHELSA &lt;- &#39;../HydrologicalModeling_CentralAsia_Data/CentralAsiaDomain/CHELSA_V1.2.1/&#39; # start and end times - function arguments startY &lt;- 1981 endY &lt;- 2013 biasCorrect_ERA5_CHELSA(dir_ERA5_hourly,dataType_ERA5,dir_CHELSA,startY,endY,basinName,gunt_Shapefile_LatLon) By simply switching the data type (effectively, it just looks for the files in the corresponding directory), temperature can be rescaled to the CHELSA field values for each month. # Specify biasCorrect_ERA5_CHELSA() function arguments basinName &lt;- &#39;Gunt&#39; dataType_ERA5 &lt;- &#39;t2m&#39; # note that ERA5 tp units are in m! ## Directories of relevant climate files - function arguments dir_ERA5_hourly &lt;- &#39;../HydrologicalModeling_CentralAsia_Data/CentralAsiaDomain/ERA5/&#39; dir_CHELSA &lt;- &#39;../HydrologicalModeling_CentralAsia_Data/CentralAsiaDomain/CHELSA_V1.2.1/&#39; # start and end times - function arguments startY &lt;- 1981 endY &lt;- 2013 biasCorrect_ERA5_CHELSA(dir_ERA5_hourly,dataType_ERA5,dir_CHELSA,startY,endY,basinName,gunt_Shapefile_LatLon) In order to understand if the rescaling of the ERA5 data lead to meaningful results, we can compare monthly precipitation totals from the final ERA5 data to monthly station values, average over the four meteorological stations that we have. basinName &lt;- &#39;Gunt&#39; dataType_ERA5 &lt;- &#39;tp&#39; # note that ERA5 tp units are in m! ## Directories of relevant climate files - function arguments dir_ERA5 &lt;- &#39;../HydrologicalModeling_CentralAsia_Data/CentralAsiaDomain/ERA5/&#39; # Note: ERA5 data in m # basin basin_shp_latlon &lt;- gunt_Shapefile_LatLon # start and end times - function arguments startY &lt;- 1981 endY &lt;- 2013 # Date sequence sTime &lt;- paste0(startY,&#39;-01-01 01:00:00&#39;) eTime &lt;- paste0(endY,&#39;-12-31 23:00:00&#39;) dateSeq_ERA &lt;- seq(as.POSIXct(sTime), as.POSIXct(eTime), by=&quot;hour&quot;) dateSeq_ERA &lt;- tibble(date=dateSeq_ERA, data=NA) dateSeq_ERA &lt;- dateSeq_ERA %&gt;% mutate(month = month(date)) %&gt;% mutate(year = year(date)) # Create Basin subdirectory (if not already existing) to store dedicated annual files there mainDir &lt;- paste0(dir_ERA5,dataType_ERA5,&#39;/&#39;,basinName,&#39;/&#39;) # Start data fetching for (yr in startY:endY){ # progress indicator print(paste0(&#39;PROCESSING YEAR &#39;,yr)) # file handling file2Process_ERA &lt;- paste0(dataType_ERA5,&#39;_ERA5_hourly_&#39;,basinName,&#39;_bcorr_&#39;,yr,&#39;.nc&#39;) era_data&lt;- brick(paste0(mainDir,file2Process_ERA)) # extract the data over the basin dateSeq_ERA$data[dateSeq_ERA$year==yr] &lt;- raster::extract(era_data,basin_shp_latlon,fun=mean) } dateSeq_ERA_mon &lt;- dateSeq_ERA %&gt;% dplyr::select(date,data) if (dataType_ERA5==&#39;t2m&#39;){ era5_data_mon &lt;- dateSeq_ERA_mon %&gt;% timetk::summarize_by_time(.date_var = date,.by = &#39;month&#39;,era5_data_mon = mean(data),.type = &#39;ceiling&#39;) station_data_mon &lt;- data %&gt;% filter(type==&#39;mean(T)&#39; &amp; resolution==&#39;mon&#39;) %&gt;% dplyr::select(date,data,code) %&gt;% filter(date&gt;=(dateSeq_ERA_mon$date %&gt;% first())) %&gt;% filter(date&lt;=(dateSeq_ERA_mon$date %&gt;% last())) %&gt;% pivot_wider(names_from = &#39;code&#39;,values_from = data) %&gt;% mutate(station_data_mon = rowMeans(dplyr::select(.,-date),na.rm=TRUE)) %&gt;% dplyr::select(date,station_data_mon) } else { era5_data_mon &lt;- dateSeq_ERA_mon %&gt;% timetk::summarize_by_time(.date_var = date,.by = &#39;month&#39;,era5_data_mon = sum(data),.type = &#39;ceiling&#39;) era5_data_mon$era5_data_mon &lt;- era5_data_mon$era5_data_mon * 1000 # in mm now station_data_mon &lt;- data %&gt;% filter(type==&#39;P&#39; &amp; resolution==&#39;mon&#39;) %&gt;% dplyr::select(date,data,code) %&gt;% filter(date&gt;=(dateSeq_ERA_mon$date %&gt;% first())) %&gt;% filter(date&lt;=(dateSeq_ERA_mon$date %&gt;% last())) %&gt;% pivot_wider(names_from = &#39;code&#39;,values_from = data) %&gt;% mutate(station_data_mon = rowMeans(dplyr::select(.,-date),na.rm=TRUE)) %&gt;% dplyr::select(date,station_data_mon) } era5_data_mon$date &lt;- (era5_data_mon$date - 3600) %&gt;% as.Date() data_validation &lt;- full_join(era5_data_mon,station_data_mon,by=&#39;date&#39;) data_validation %&gt;% pivot_longer(-date) %&gt;% plot_time_series(.date_var = date,.value = value,.smooth = FALSE, .color_var = name,.title = &quot;&quot;,.x_lab = &quot;Year&quot;,.y_lab = &quot;mm/month&quot;,.interactive = FALSE) Figure 4.41: Comparison of mean monthly precipitation values of the Gunt meteorlogical Stations with the rescaled ERA5 data in the basin. The timeseries compare favorably, both in terms of seasonality and interannual variability. 4.3.2.4 Downscaling ERA5 to Basin Elevation Bands and Data Export to RS MINERVE Database file The next and last step for the preparation of the input data required for hydrological modeling involves the downscaling of the prepared hourly ERA5 data to the individual subbasins and elevation bands inside a catchment. The process of generating the input files is shown by using data from the Gunt river basin. The Gunt river basin outline is shown in Figure 4.42. We want to downscale the prepared hourly ERA5 climate fields over the domain of the Gunt river basin to the individual subbasins and the elevation bands of these subbasins (see Figures 4.42 to ?? below). In other words, we want to generate hourly climate timeseries of mean temperature and precipitation levels for each subbasin and all of the elevation bands in each subbasin. The hydrological processes of each of these subbasin elevation bands will then be modeled with a separate model. All these individual hydrological models will be interconnected according to the existing flow topology in the basin. Like this, we arrive at a lumped (lumped over the subbasin-specific elevation bands), semi-distributed (distributed between the subbasins and elevation bands) hydrological model of the entire catchment. knitr::include_graphics(&quot;_bookdown_files/FIG_DATA/Gunt_Outline.jpg&quot;) Figure 4.42: The outline of the Gunt river catchment is shown with the key river sections of all tributariies from the corresponding subbasins. knitr::include_graphics(&quot;_bookdown_files/FIG_DATA/Gunt_Subbasins.jpg&quot;) Figure 4.43: Gunt river catchment with the individual subbasins is shown. These subbasins are further divided into elevation band zones and the mean climate finally extracted over these subbasin-specfic elevation bands (see Figure (ref?)(fig:guntSubasinsElevationBands)). knitr::include_graphics(&quot;_bookdown_files/FIG_DATA/Gunt_Subbasins_ElevationBands.jpg&quot;) Figure 4.44: The Gunt river catchment elevation bands are shown in red color. The entire catchment was divided into 4 altitude zones with an interval spacing of 1’000 meters. In real world climate impact studies, elevation bands are normally generated with an interval spacing of 200 - 500 meters. knitr::include_graphics(&#39;_bookdown_files/FIG_DATA/Gunt_Topology.jpg&#39;) Figure 4.45: The naming concention of the subbasins is shown. For each subbasin, the bands are identified with a suffix .._b1 for elevation band 1, .._b2 for elevation band 2 and so on. knitr::include_graphics(&#39;_bookdown_files/FIG_DATA/Gunt_ElevationBands_AttributeTable.jpg&#39;) Figure 4.46: Subbasin elevation band attribute table. In total there are 14 elevation bands and for each elevation band, a climate time series is created with the function riversCentralAsia::generate_ERA5_Subbasins_CSV(). The Z field is important and specifies the average elevation of the corresponding elevation band for each subbasin. The code block assumes that the previously described steps have been carried and that the corresponding files are stored in the correct relative path locations. First, ERA5 precipitation is downscaled with generate_ERA5_Subbasin_CSV(...) with dataType='tp' set. Second, dataType='t2m' for the downscaling of the 2 meter above surface ERA5 temperature. In a third step, we add the discharge data to the resulting data frame in the last column. Finally, the observed discharge data is added. Note here that we add the gap filled time series of the observational record. The process of gap filling discharge data is described in Section 4.1.2 above. # Housekeeping dir_ERA5 &lt;- &#39;../HydrologicalModeling_CentralAsia_Data/CentralAsiaDomain/ERA5/&#39; catchmentName &lt;- &#39;Gunt&#39; elBands_shp &lt;- st_read(&#39;../HydrologicalModeling_CentralAsia_Data/AmuDarya/Gunt/GeospatialData/Gunt_ElevationBands_Subbasins_RSMinerve.shp&#39;) startY &lt;- 1981 endY &lt;- 2013 # 1. Downscaling precipitation dataType &lt;- &#39;tp&#39; # Precipitation gunt_db_tp &lt;- generate_ERA5_Subbasin_CSV(dir_ERA5_hourly,catchmentName,dataType,elBands_shp,startY,endY) # 2. Downscaling temperature dataType &lt;- &#39;t2m&#39; # Temperature gunt_db_t2m &lt;- generate_ERA5_Subbasin_CSV(dir_ERA5_hourly,catchmentName,dataType,elBands_shp,startY,endY) # 3. Joining the two data frames. gunt_db &lt;- gunt_db_t2m %&gt;% add_column(gunt_db_tp %&gt;% dplyr::select(-Station),.name_repair = &#39;unique&#39;) # 4. Adding the observed discharge data in the required format to the data frame gunt_db_Q &lt;- q_17050_mon_filled %&gt;% dplyr::select(date,data) gunt_db_Q$date &lt;- gunt_db_Q$date %&gt;% as.POSIXct() + 22 * 60 * 60 # just adding time so that we are indeed at the end of the month datesChar_Q &lt;- posixct2rsminerveChar(gunt_db_Q$date) %&gt;% rename(Station=value) datesChar_Q &lt;- datesChar_Q %&gt;% add_column(Q_17050 = (gunt_db_Q$data %&gt;% as.character)) gunt_db_Q &lt;- full_join(gunt_db,datesChar_Q,by=&#39;Station&#39;) # this works well # now finish off by giving the required attributes in the table for the discharge station gunt_db_Q$Q_17050[1] = &#39;Q_17050&#39; gunt_db_Q$Q_17050[2] = meteoStations$utm.x[2] gunt_db_Q$Q_17050[3] = meteoStations$utm.y[2] gunt_db_Q$Q_17050[4] = meteoStations$masl[2] gunt_db_Q$Q_17050[5] = &#39;Q&#39; gunt_db_Q$Q_17050[6] = &#39;Flow&#39; gunt_db_Q$Q_17050[7] = &#39;m3/s&#39; gunt_db_Q$Q_17050[8] = &#39;Constant before&#39; The resulting data frame can be stored with write.table(..,..,sep=',',row.names = FALSE,col.names=FALSE,quote=FALSE) as a csv-file and the read into RS MINERVE later. A screenshot of the csv-file is below in Figure 4.47. Elevation band-specific temperature (T) and precipitation (P) data are stored in columns. The last column contains the discharge data Q. If more than one observation station are available, more discharge columns would correspondingly need to be added. In row 1 - 8, elevation band specific information is stored. More specifically, the following information is contained: Row 1: Name of elevation band and discharge station(s) Row 2: Longitude of centroid of elevation band and location of gauging station(s) (in meters, UTM) Row 3: Latitude of centroid of elevation band and location of gauging station(s) (in meters, UTM) Row 4: Mean elevation across elevation band and elevation of gauging station(s) Row 5: Observation type indicator (T: temperature, P: precipitation and Q: discharge) Row 6: Category Row 7: Measurement units Row 8: Interpolation method for downsampling course resolution time series9 knitr::include_graphics(&quot;_bookdown_files/FIG_DATA/gunt_RSMINERVE_CSV.jpg&quot;) Figure 4.47: Screenshot of the resulting large csv file that can be read into RS MINERVE. From rows 9 onward, the actual data are stored where a time stamp in the first column corresponds to the corresponding observation data and time. Please note that the total size of this repository is 2 GB, approximately. Prior to downloading these data, you should ensure that you have enough storage space on your local machine. Furthermore, it is advised that you create the directory structure in the following way: FILES/PROJECT -Code —yourRProject.Rproj —code1.R —… -HydrologicalModeling_CentralAsia_Data — where FILES/PROJECT is the folder where you keep your documents on your computer and project denotes the folder name of your project, Code is the folder where all your code lives and HydrologicalModeling_CentralAsia_Data is the folder where you store all the accompanying data that can downloaded.↩︎ The code for the computation of the CHELSA long-term monthly mean temperature and precipitation climatologies is provided in the Appendix C in the corresponding Sect.↩︎ Please note that one raster brick that encompasses the data for one year is approx 333 MB to download!↩︎ This is for example used to downsample monthly discharge as provided by the hydrometeorological agency to hourly discharge (we use hourly simulation time steps in the hydrological model).↩︎ "],["data-on-climate-projections.html", "4.4 Data on Climate Projections", " 4.4 Data on Climate Projections In order to look into the future with a hydrological model for exploring climate scenarios, it needs to be fed with those scenarios. The production of these scenarios in a way resembles the steps described for the reanalysis data but also differs in some marked way. Let us outline them first before diving into the details. First, the period of interest for the investigation of climate impacts needs to be defined. Future climate scenarios need then need to be identified that cover the area and period of interest. The future climate scenarios are outputs from global climate models (GCMs). These GCMs normally generate monthly data as a function of representative concentration pathways (called RCPs). They describe possible futures as a function of different atmospheric CO2-concentration scenarios. We focus the discussion here on the RCP 4.5 and RCP 8.5 concentration scenarios. According to the IPCC, RCP 4.5 is an intermediate scenario where CO2 emissions peak around 2040, then decline. Compared to this RCP 8.5 is generally considered to be a worst case climate change scenario. Very detailed information regarding these scenarios, their underlying assumptions and assumptions can be found on the dedicated website served by the Intergovernmental Panel for Climate Change (IPCC). See the link here. knitr::include_graphics(&quot;_bookdown_files/FIG_DATA/greenhouseGasConcetrations_RCP.jpg&quot;) Figure 4.48: The development of greenhouse gas concentrations during the 21st century as a function of the corresponding scenario. We focus here on RCP4.5 and RCP8.5. Source: (Vuuren et al. 2011) 4.4.1 High Resolution Monthly Climate Time Series for 2006 - 2100 The recently released high-resolution climate scenario datasets described by (Karger et al. 2020) are utilized here to study climate impacts in river basins in the Central Asia region. The authors of the aforementioned paper present GCMs output for monthly precipitation, mean minimum as well as maximum monthly temperatures that has been downscaled using the CHELSA algorithm. The data can be found here https://www.envidat.ch/#/metadata/chelsa_cmip5_ts. We download data from 3 out of the 4 available downscaled GCM runs for further analysis and processing. These are outputs from the Coupled Model Intercomparison Project phase 5 (CMIP5) and are gridded monthly time series from the years 2006 through 2100 with a spatial resolution of 0.049 degrees (see again (Karger et al. 2020) for all the details). The three models from which data were taken are: CMCC-CM run by the Centro Euro-Mediterraneo per I Cambiamenti Climatici (CMCC); MIROC5 run by the University of Tokyo; and ACCESS1-3 run by the Commonwealth Scientific and Industrial Research Organization (CSIRO) and Bureau of Meteorology (BOM), Australia (Karger et al. 2020). First, the data is downloaded and stored locally for RCP 4.5 and RCP 8.5. Then, as in the case of the original CHELSA data, the monthly gridded precipitation timeseries are bias corrected using the monthly correction factors as report by (Beck et al. 2020d) and finally, mean monthly temperature fields are generated from the minimum and maximum monthly temperatures. The resulting data available for the Central Asia domain are available for download via this link10. As in the case of the ERA5 data, these GCM data need to be made available for all the elevation bands. By providing a proper shapefile that contains sub-basin specific elevation bands of a catchment, the function riversCentralAsia::downscale_ClimPred_monthly_BasinElBands() can be used to compute this. This function processes the climate projection raster bricks with precipitation_flux and air_temperature (tasmin, tasmean and tasmax) and computes elevation band statistics. As discussed, these GCM data are high resolution monthly future climate fields. All good. But how do we arrive at climate fields that have a high hourly temporal resolution at the same time so that we can drive the RS MINERVE hydrological model with the required input data? The solution is shown in the next section. 4.4.2 Using a Weather Generator to Simulate Daily Future Climate 4.4.2.1 Background and Input File Generation Stochastic weather generators are probabilistic models that are used to simulate weather data at a specific site or region by analyzing historical weather data and then generating a time-series of weather variables with statistical properties identical to the historical data (Birt et al. 2010). We demonstrate the use of RMAWGEN, a stochastic weather generator available for the R environment. Further information on RMAWGEN can be found here https://github.com/ecor/RMAWGEN and under the links provided via this GitHub link. While the weather generator is relatively poorly documented, a useful albeit very technical presentation of the tool is available here. RMAWGEN allows for the generation of daily temperature and precipitation scenarios that are conditioned on monthly totals as given by the GCM model outputs. This is the key idea. In order to arrive at the final hourly data, we then simply distribute the daily data evenly over the hours of that particular day. Using again an example of the Gunt river basin, the following code block shows how to use the function prepare_RMAWGEN_input_data() for the preparation of the relevant RMAWGEN input data files. # Load and process RS MINERVE csv database file filePath &lt;- &#39;./data/AmuDarya/Gunt/RSMinerve/&#39; fileName &lt;- &#39;Gunt_1981_2013.csv&#39; era5_gunt &lt;- read.table(paste0(filePath,fileName),sep=&#39;,&#39;) %&gt;% as_tibble() rmawgen_input_files &lt;- prepare_RMAWGEN_input_data(era5_gunt) rmawgen_input_files %&gt;% list2env(.,envir = .GlobalEnv) # Send list variables to global environment (it is a type of unlist). The function returns a list rmawgen_input_files with all key information that can be unlisted using rmawgen_input_files %&gt;% list2env(.,envir = .GlobalEnv). This includes: STATION_NAMES: Character vector of station names (effectively, these are just the subbasin names extracted from the RS MINERVE input file). ELEVATION: Mean elevation of the stations STATION_LATLON: Matrix with station names, latitude and longitude LOCATION: Same as the station name, used above. PRECIPITATION: Tibble containing daily precipitation for each station TEMPERATURE_MIN: Minimum daily temperature (calculated from the ERA5 hourly data stored in the RS MINERVE csv file) TEMPERATURE_MAX: Maximum daily temperature (ditto) A weather generator is a type of model that uses input data to simulate output variables. Exactly like for example a hydrological runoff model, it depends on input parameters and different input parameter configuration lead to different model performance / outcomes. Hence, before generating future climate scenarios for which we do not have a validation set available to assess the performance of different parameter configurations of the weather generator, we assess model performance by using ERA5 data for calibration and model validation. The simple question to be answered here is whether or not the model is able to reproduce the statistical characteristics of the ERA5 dataset in the region of interest. If yes, we can safely assume that the weather generator can in a more or less realistic manner generate stochastic future climate scenarios that are conditioned on monthly GCM temperature and precipitation fields. A typical parameter set of RMAWGEN for testing different models can be specified in the following way. Housekeeping set.seed(123456) # Random generator seed is set to make results reproducible. # Monthly climate is calculated if it is set to NULL. PREC_CLIMATE &lt;- NULL # Calibration Period year_min &lt;- 1981 year_max &lt;- 2013 origin &lt;- origin &lt;- &quot;1981-1-1&quot; # This is the starting date for which we have data available. # n GPCA iterations for variable and VAR residuals n_GPCA_iter &lt;- 5 n_GPCA_iteration_residuals &lt;- 5 # Autoregression orders (p) and exogenous vars p_test &lt;- 1 p_prec &lt;- 3 exogen &lt;- NULL exogen_sim &lt;- exogen # number of weather realizations nscenario &lt;- 1 # Simulation period year_min_sim &lt;- year_min # we are not yet simulating future climate but use the available data for model assessment, hence year_min_sim = year_min year_max_sim &lt;- year_max # we are not yet simulating future climate but use the available data for model assessment, hence year_max_sim = year_max # Minimum Precipitation value below which no precip is considered valmin &lt;- 1.0 4.4.2.2 Model Calibration and Validation Different parameter combinations can now be tested and the model performance assessed as a function of these configurations. Detailed information about the core functions of RMAWGEN can be obtained by consulting their help, i.e. by for example typing ?ComprehensivePrecipitationGenerator. # P03GPCA_prec generationP03GPCA_prec &lt;- ComprehensivePrecipitationGenerator( station = STATION_NAMES, prec_all = PRECIPITATION, year_min = year_min, year_max = year_max, p = p_prec, n_GPCA_iteration = n_GPCA_iter, n_GPCA_iteration_residuals = n_GPCA_iteration_residuals, exogen = exogen, exogen_sim = exogen_sim, sample = &quot;monthly&quot;, mean_climate_prec = PREC_CLIMATE, no_spline = FALSE, year_max_sim = year_max_sim, year_min_sim = year_min_sim ) model1 &lt;- generationP03GPCA_prec # P01GPCA generationP01GPCA_prec &lt;- ComprehensivePrecipitationGenerator( station = STATION_NAMES, prec_all = PRECIPITATION, year_min = year_min, year_max = year_max, p = p_test, n_GPCA_iteration = n_GPCA_iter, n_GPCA_iteration_residuals = n_GPCA_iteration_residuals, exogen = exogen, exogen_sim = exogen_sim, sample = &quot;monthly&quot;, mean_climate_prec = PREC_CLIMATE, no_spline = FALSE, year_max_sim = year_max_sim, year_min_sim = year_min_sim ) model2 &lt;- generationP01GPCA_prec # P03 generationP03_prec &lt;- ComprehensivePrecipitationGenerator( station = STATION_NAMES, prec_all = PRECIPITATION, year_min = year_min, year_max = year_max, p = p_prec, n_GPCA_iteration = 0, n_GPCA_iteration_residuals = 0, exogen = exogen, exogen_sim = exogen_sim, sample = &quot;monthly&quot;, mean_climate_prec = PREC_CLIMATE, no_spline = FALSE, year_max_sim = year_max_sim, year_min_sim = year_min_sim ) model3 &lt;- generationP03_prec # P01 generationP01_prec &lt;- ComprehensivePrecipitationGenerator( station = STATION_NAMES, prec_all = PRECIPITATION, year_min = year_min, year_max = year_max, p = p_test, n_GPCA_iteration = 0, n_GPCA_iteration_residuals = 0, exogen = exogen, exogen_sim = exogen_sim, sample = &quot;monthly&quot;, mean_climate_prec = PREC_CLIMATE, no_spline = FALSE, year_max_sim = year_max_sim, year_min_sim = year_min_sim ) model4 &lt;- generationP01_prec The performance of the 4 models can be assessed with statistical tests and graphically, with qq-plots and through the comparison of simulated versus observed monthly precipitation totals. Results are shown in the Figures below. knitr::include_graphics(&quot;./_bookdown_files/FIG_DATA/RMAWGEN_Models_monthlyTotalsComparison.jpg&quot;) Figure 4.49: The comparison of the 4 stochastic weather generator models in terms of their abilities to reproduce monthly precipitation totals is shown for one station (elevation band) in the Gunt river basin. Results indicate that precipitation totals from the models 1 and 2 are overestimated. In comparison, models 3 and 4 are more balanced and not biased. knitr::include_graphics(&quot;./_bookdown_files/FIG_DATA/RMAWGEN_Models_qqplot.jpg&quot;) Figure 4.50: Q-Plots of the four model results. If data plots along the diagonal line, it is an indication that the probability distribution of the simluated precipptation (y-axis) is the same as the probability distribution of the observed ERA5 precipitation (x-axis). QQ-plots of models 1 and 2 show a heavy bias whereas model 4 shows an overal satisfactory fit and thus performance. Figure 4.49 and Figure 4.50 show that the Model 4 provides an overall well-balanced performance. We will thus us this models to generate the future climate fields. 4.4.2.3 Simulating Future Daily Climate We will use the model4 configuration to produce simulated future climate runs. If you download these data, it is advisable to maintain the directory structure as given via the Dropbox link. Please note that the entire directory is 4.79 GB of data.↩︎ "],["snow-cover-data.html", "4.5 Snow Cover Data", " 4.5 Snow Cover Data "],["RSMinerveMODELS.html", "Chapter 5 Hydrological-Hydraulic Modeling", " Chapter 5 Hydrological-Hydraulic Modeling Hydrological-hydraulic models are used for different purposes, including for the design of basin plans while taking into consideration different development options, for the study of effective water resources management schemes while taking into consideration the different types of sectoral users and uses of water in a basin (ideally also including minimal environmental flows), and for the detailed study of basin-scale climate impacts and their repercussions. This Chapter introduces combined hydrologic and hydraulic modeling using the freely available graphical modeling environment RSMinerve (CREALP 2021; Foehn et al. 2020; Garcia Hernandez et al. 2020)11. Figure 5.1 shows a screen shot of a model of the Gunt river basin setup in RSMinerve as a teaser. Figure 5.1: Screenshot of an RSMinerve model setup for Gunt river basin. It should be noted that other modeling packages exist for hydrological-hydraulic modeling. Among others, these include WEAP: Water Evaluation and Planning System, SWAT: Soil &amp; Water Assessment Tool, and Mike Hydro Basin As some if not most of these modeling packages are license-based, the advantage of using the combination of QGIS, R and RSMinerve is that this is a completely free software suit for any user and use scenario. Departing from catchment GIS data, RS Minerve allows to quickly develop, test and calibrate different types of lumped conceptual precipitation runoff models while taking into account the evolution of water balances in glacier, snow, surface and subsurface compartments over time and in different sections in the catchment under consideration. This is demonstrated here for the Gunt River Basin in the Pamirs (see Chapter 3.1 for more information). The main goal in this Chapter is to familiarize the student on how to setup such model, calibrate and validate it for past and current climate conditions and then to study climate change impacts in the basin. The reader is advised to consult the user manual and familiarizes himself or herself with the walk through examples that are discussed and presented there. They give a solid first hands-on introduction about the basic functionalities of the modeling environment.↩︎ "],["section-rsminerve-prerequisites.html", "5.1 Prerequisites", " 5.1 Prerequisites Before delving into this section of the course material, you should have at least: RS Minerve installed How to install RS Minerve. This tutorial was written using RS Minerve 2.9.1.0. Read chapters 1 and 2 from the RS Minerve user manual (Foehn et al. 2020) (the manual can be downloaded from the CREALP website, see Section How to install RS Minerve). Familiarize yourself with RS Minerve by following Example 1 in the RS Minerve user manual. Read the HBV model description in the RS Minerve technical manual (Chapter 2.7) (Garcia Hernandez et al. 2020). You may have to dig deeper into the user manual and the technical manual within the frame of the course but the above points are the minimum requirement to get started. If you fulfill the prerequisites, you should be able to do the tutorial with the minimal description in the modeling section. However, detailed step-by-step descriptions are linked for each task. "],["section-rsminerve-why-model-introduction.html", "5.2 Recap: Hydrological response", " 5.2 Recap: Hydrological response Why do we do hydrological modeling? Typically we want to know how much river flow we can expect at a given time in the future so we can plan our water consumption ahead, harness against floods or implement measures against droughts. So how can we forecast river discharge? For example, one could use the long term seasonal average discharge as the likely future discharge. However, in some years we have high discharge and in some years we have low discharge (Figure 5.2). How to tell when you have which discharge? Figure 5.2: Discharge of the Nauvalisoy river. Let’s go back to the water balance: Where does the water in the river come from? Precipitation. When we compare precipitation and discharge time series we see that the two are related (Figure 5.3). Figure 5.3: Discharge (red) and precipitation (grey) in the Nauvalisoy catchment. Higher precipitation in the catchment is positively correlated with higher discharge in the river with a delay. Or in other words: The discharge is the catchments hydrological response to precipitation. The relationship can be described as follows: \\[ Q \\propto P \\cdot K \\] In words this reads: Discharge (Q) is proportional to precipitation (P) times a transfer function (K) that defines the delay of the signal. The transfer function K describes how precipitation becomes river discharge and depends on the catchment characteristics, importantly on the storage capacity of the catchment. In reality, the transformation of precipitation to discharge is quite complex but we can simplify the reality and come up with a basic conceptual model of our river catchment: a large bath tube with the area of the catchment and an outlet at the bottom called the linear reservoir model (Figure 5.4). Figure 5.4: From the real-life system (left) to a linear reservoir (right). The linear reservoir model describes the discharge from the reservoir as linearly proportional to the storage in the reservoir: \\[ Q(t) = 1/k \\cdot S(t) \\] Where Q is discharge, S is the storage and k is a constant called storage coefficient depending on the storage capacity of the reservoir. If k is small, the stored water will run out quickly, if k is large, the stored water will run out slowly. (t) indicate time dependent variables, i.e. variables that change over time. Substitute the linear reservoir equation into the water balance: \\[ P = Q + dS/dt \\Leftrightarrow P = Q + k \\cdot dQ/dt \\] Where P is the excess precipitation, i.e. the precipitation that is not intercepted by plants or ponds but contributes to discharge. The above is a differential equation and can be re-arranged: \\[(P-Q) \\cdot dt = k \\cdot dQ\\] and integrated and solved for Q(t) (dig out your mathematics!). In the example in the box below we discredited the water balance with the linear relationship between discharge and storage change in the reservoir: \\[\\frac{P_{t_1}+P_{t_1}}{2} - \\frac{Q_{t1}+Q_{t_2}}{2} = \\frac{k}{t_2-t_1} \\cdot (Q_{t_2}-Q_{t_1})\\] and solved the equation for \\(Q_{t_2}\\) (Inspiration from (Pedersen, Peters, and Helweg 1980). We assume that at the beginning of our experiment, we don’t have a discharge as the bucket is empty. \\[Q(t) = P(t) \\cdot (1-e^{(-t/k)})\\] This is the equation for the rising discharge curve. Once you turn off the recharge of the reservoir (once you stop pouring water into the bucket), the discharge from the bucket can be described as: \\[Q(t) = Q^* \\cdot e^{-\\tau/k}\\] Which describes the receding limb of the hydrograph (What is a hydrograph) with \\(\\tau\\) being the time since the recharge stopped. Understanding the linear reservoir model is a prerequisite for hydrological modeling. That is why we propose the following exercise for you to do at home as a preparation for class. Task 1: Exerimental setup To understand the flow of water through a reservoir, you will build a bucket model yourself at home from materials that you have at hand and measure recharge as well as discharge from the bucket. You will then make a numerical model of your real bucket model and simulate discharge. Think through the experiment and answer the following questions: What will determine the flow through your bucket? What do you need to measure? How can you measure it? What materials you will need to set up the experiment? Where will you get the water from and can you re-use it after the experiment? Solution to task 1 Task 2: Perform the experiment Set up the experiment and pour water into your improvised reservoir. Take care to pour with a continuous speed over a short time period. You may have to repeat the experiment a few times (see also notes on measurement accuracy in the Solutions of task 1). The outcome of the experiment is a time series of discharge measurements. You can watch a video of the experiment here if you cannot perform the experiment yourself. Task 3: Simulate the experiment The figure below shows the measured discharge from the experimental data shown in the table above. We have implemented a discretized version of a linear reservoir model for you which shows the simulated discharge (grey bars) of a recharge event (blue bars). Play with the storage parameter by moving the slider and see how the simulated discharge changes based on the storage parameter of the simulated catchment. Can you reproduce the measured discharge? If you cannot find a perfect match between measurements and simulation results: What could be the reasons? (If you see an empty window below this text, go to the web app directly to do the exercise). Solution to task 3 Task 4: The non-linear reservoir What happens to the discharge curve if the outlet of the reservoir is not at the bottom but somewhere in the middle of the receptacle wall? What happens if the receptacle is filled with sand? What happens if you add another linear reservoir to the outlet of the first one, i.e. make a cascade of linear reservoirs? Solution "],["data-preparation-and-setup.html", "5.3 Data Preparation and Setup", " 5.3 Data Preparation and Setup The data used in this Chapter can be accessed on the public GitHub repository Applied Modeling of Hydrological Systems in Central Asia and the ./data/AmuDarya/Gunt/ folder in there. It is advisable that the user downloads the data and sets up a similar data directory structure on their local machines. 5.3.1 Examining and Understanding the GIS Data As demonstrated in the Examples of the User Manual, RS Minerve does not necessarily require GIS input files for setting up models. These can also be setup and wired by hand. However, individual subbasins that contribute to flow in a larger catchment have different characteristics that need to be derived from spatial data, i.e. GIS files. For example, subbasin areas, river stretch lengths, terrain slopes, etc., are all required data in an RS Minerve model. These data could either be typed in by hand for each subbasin or automatically inserted via the GIS Import option. The latter method is much more convenient and is demonstrated here12. First, we load the required packages in R. # Loading required packages ## Tidy data wrangling library(tidyverse) library(here) library(timetk) library(tidymodels) library(lubridate) library(timetk) ## riversCentralAsia Package library(&#39;riversCentralAsia&#39;) ## Spatial data processing library(raster) library(sf) Now, we can visualize the GIS files that are required in RS Minerve in the following Figure 5.5. # Load data fPath &lt;- &#39;./data/AmuDarya/Gunt&#39; gunt_DEM &lt;- raster(paste0(fPath,&#39;/GeospatialData/17050_Gund_Basin_DEM.tif&#39;)) gunt_elBands_shp_utm &lt;- st_read(paste0(fPath,&#39;/GeospatialData/Gunt_ElevationBands_Subbasins_RSMinerve.shp&#39;),quiet=TRUE) gunt_subbasins_shp_utm &lt;- st_read(paste0(fPath,&#39;/GeospatialData/Gunt_Subbasins_RSMinerve.shp&#39;),quiet=TRUE) gunt_subbasin_junctions_shp_utm &lt;- st_read(paste0(fPath,&#39;/GeospatialData/Gunt_Junctions_RSMinerve.shp&#39;),quiet=TRUE) gunt_rivers_shp_utm &lt;- st_read(paste0(fPath,&#39;/GeospatialData/Gunt_Rivers_RSMinerve.shp&#39;),quiet=TRUE) # Downsample DEM and create hillshade gunt_DEM_lr &lt;- raster::aggregate(gunt_DEM,fact=10) # this is in UTM 42N gunt_slope &lt;- terrain(gunt_DEM_lr, opt=&#39;slope&#39;) gunt_aspect &lt;- terrain(gunt_DEM_lr, opt=&#39;aspect&#39;) gunt_DEM_hillshade &lt;- hillShade(gunt_slope, gunt_aspect, 40, 270) # Convert to dataframe for ggplotting gunt_DEM_spdf &lt;- as(gunt_DEM_lr, &quot;SpatialPixelsDataFrame&quot;) gunt_DEM_df &lt;- as.data.frame(gunt_DEM_spdf) colnames(gunt_DEM_df) &lt;- c(&quot;value&quot;, &quot;x&quot;, &quot;y&quot;) hillshade_Gunt_spdf &lt;- as(gunt_DEM_hillshade, &quot;SpatialPixelsDataFrame&quot;) hillshade_Gunt_df &lt;- as.data.frame(hillshade_Gunt_spdf) colnames(hillshade_Gunt_df) &lt;- c(&quot;value&quot;, &quot;x&quot;, &quot;y&quot;) # Used for subbasins naming subbasins &lt;- # just put everything in a regular dataframe tibble(basin=c(&quot;Shakhdara&quot;,&quot;Gunt&quot;,&quot;Tokusbulak&quot;,&quot;Alishur&quot;), lat = c(37.4,37.75,37.6,37.65), lon = c(72.0,71.8,72.6,73.25)) subbasins_coord_latlon = SpatialPoints(cbind(subbasins$lon, subbasins$lat), proj4string=CRS(&quot;+proj=longlat&quot;)) subbasins_coord_UTM &lt;- spTransform(subbasins_coord_latlon, CRS(&quot;+init=epsg:32642&quot;)) %&gt;% coordinates() %&gt;% as_tibble() %&gt;% rename(x=coords.x1,y=coords.x2) subbasins &lt;- bind_cols(subbasins,subbasins_coord_UTM) # Plotting ggplot() + geom_tile(data=gunt_DEM_df, aes(x=x, y=y, fill=value), alpha=0.8) + geom_sf(data=gunt_subbasins_shp_utm,color=&quot;black&quot;,fill=NA,linetype=&quot;11&quot;,size=0.25) + geom_sf(data=gunt_rivers_shp_utm,color=&quot;blue&quot;,fill=NA) + geom_sf(data=gunt_elBands_shp_utm,color=&quot;black&quot;,fill=NA,linetype=&quot;11&quot;,size=.2) + geom_sf(data=gunt_subbasin_junctions_shp_utm,color=&quot;red&quot;,fill=&quot;red&quot;) + scale_fill_gradientn(colours = terrain.colors(100)) + xlab(&quot;Longitude&quot;) + ylab(&quot;Latitude&quot;) + guides(fill=guide_legend(title=&quot;Alt. [masl]&quot;)) + ggtitle(&quot;Gunt catchment and subbasins&quot;) + geom_label(data = subbasins,aes(x = x, y = y, label = basin),vjust = 0,hjust = 0) Figure 5.5: Gunt river basin overview, showing the digital elevation model, the 4 subcatchments, the elevation bands with 1’000 meters intervals, the main tributary rivers and the corresponding junctions. Figure 5.5 shows the four subbasins. After the confluence with Tokusbulak river, Alishur river becomes Gunt river in Junction 1 (called junct_1 in the corresponding shapefile below). The confluence is noted by the corresponding red dot. Further downstream, approximately 5 km before the Gunt-Khorog gauging station (Hydromet Code 17050), indicated by the leftmost outlet point (red dot, called junct_2 in the junct_down attribute field of the shapefile), Shakhdara river feeds into Gunt. The subbasin-specific elevation bands are indicated by dotted lines. As described in the RS Minerve User Manual in Chapter 5 (Foehn et al. 2020), the following three shapefiles need to be prepared for later loading into RS Minerve: subbasins.shp, junctions.shp and rivers.shp (see Figure 5.6). Figure 5.6: Required GIS input files for RS Minerve. See corresponding user manual for more information. The shapefile attributes that you downloaded of the required data can easily be displayed. First, we analyze the subbasins `gunt_elBands_shp_utm` file. The file contains the following attributes as per the requirements specified in Figure 5.6: name and junct_down. The attributes Z, ModelGuid and geometry are additional fields. The field Z that contains information of the mean elevation of the subbasin. Note that the shapefile is does not only contain the shape of the four subbasins Alishu, Tokusbulak, Gunt and Shakdara but also the elevation bands of each subbasin. # view meta data summary of the subbasins file gunt_elBands_shp_utm &lt;- sf::st_read(paste0(fPath,&#39;/GeospatialData/Gunt_ElevationBands_Subbasins_RSMinerve.shp&#39;),quiet=TRUE) gunt_elBands_shp_utm ## Simple feature collection with 14 features and 3 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 725348.3 ymin: 4093789 xmax: 945535.6 ymax: 4216482 ## projected CRS: WGS 84 / UTM zone 42N ## First 10 features: ## name junct_down Z geometry ## 1 basin_l1_b2 junct_1 3764.159 MULTIPOLYGON (((796644.2 41... ## 2 basin_l1_b3 junct_1 4503.951 MULTIPOLYGON (((827142.4 41... ## 3 basin_l1_b4 junct_1 5218.727 MULTIPOLYGON (((827592.4 41... ## 4 basin_m1_b2 junct_1 3764.159 MULTIPOLYGON (((801343.9 41... ## 5 basin_m1_b3 junct_1 4503.951 MULTIPOLYGON (((838641.8 41... ## 6 basin_m1_b4 junct_1 5218.727 MULTIPOLYGON (((840091.7 41... ## 7 basin_l2_b1 junct_2 2765.624 MULTIPOLYGON (((727648.2 41... ## 8 basin_l2_b2 junct_2 3764.159 MULTIPOLYGON (((726448.3 41... ## 9 basin_l2_b3 junct_2 4503.951 MULTIPOLYGON (((757446.5 40... ## 10 basin_l2_b4 junct_2 5218.727 MULTIPOLYGON (((760396.3 40... This is because in RS Minerve, we will model each elevation zone of each subbasin with a separate rainfall-runoff model. These are then linked to each other via the topological ordering that is given by the rivers and junctions. The naming of the subbasins is somewhat arbitrary and tailored to the particular catchment under consideration. The prefix ‘basin_’ is everywhere the same. The basic idea is to identify a main river stem (denoted with mX) where the X is a number that increases from 1 towards the downstream after each confluence with a right or left tributary. In our case, Alishur river is denoted with m1. Gunt river, which emerges after the confluence with Tokusbulak (see Figure 5.5), is denoted with m2. Finally, Gunt river after the confluence with Shakhdara is m3. Each left tributary is denoted with lX and each right tributary with rX, where X is again a number starting from 1 and gradually increasing. In our case, Tokusbulak thus is l1 (the first left tributary joining the main river). Finally, Shakhdara is l2. Note, in the Gunt river basin, we do not have major right tributaries. ‘m1_b2’ the refers to the main upstream river, Alishur in this case, and its elevation band 2. m1_b3 would then be the third elevation band accordingly and so on. Why is there no m1_b1 feature? This is simple to understand. Since the entire basin is subdivided into n equally spaced elevation zones, the higher lying subbasins might no longer contain all elevation bands. For both, Alishur (main m1) and Tokusbulak (l1) this is the case where band b1 is simply not represented because of the high elevations of the subbasins. # view meta data summary of the junctions file gunt_subbasin_junctions_shp_utm &lt;- sf::st_read(paste0(fPath,&#39;/GeospatialData/Gunt_Junctions_RSMinerve.shp&#39;),quiet=TRUE) gunt_subbasin_junctions_shp_utm ## Simple feature collection with 3 features and 2 fields ## geometry type: POINT ## dimension: XYZM ## bbox: xmin: 724198.4 ymin: 4151736 xmax: 795694.3 ymax: 4178534 ## z_range: zmin: 2074.126 zmax: 3118.366 ## m_range: mmin: 0 mmax: 0 ## projected CRS: WGS 84 / UTM zone 42N ## name river_down geometry ## 1 junct_2 gunt_ds POINT ZM (728998.1 4151736 ... ## 2 junct_3 &lt;NA&gt; POINT ZM (724198.4 4152236 ... ## 3 junct_1 gunt_us POINT ZM (795694.3 4178534 ... For the junctions shapefile, we define their names as well the field river_down, i.e. the name of the river downstream of the junction. Finally, for the rivers shapefile, we define the rivers name and the downstream junction junct_down. The terminal junction corresponds to the outlet of the model. It can be easily seen that junct_3 is the outlet since there is no more downstream river, i.e. &lt;NA&gt;. # view meta data summary of the rivers file gunt_rivers_shp_utm &lt;- st_read(paste0(fPath,&#39;/GeospatialData/Gunt_Rivers_RSMinerve.shp&#39;),quiet=TRUE) gunt_rivers_shp_utm ## Simple feature collection with 5 features and 3 fields ## geometry type: LINESTRING ## dimension: XYZM ## bbox: xmin: 724198.4 ymin: 4119538 xmax: 923686.9 ymax: 4196533 ## z_range: zmin: 2074.126 zmax: 4040.01 ## m_range: mmin: -1.797693e+308 mmax: 0 ## projected CRS: WGS 84 / UTM zone 42N ## name junct_down LENGTH geometry ## 1 shakhdara junct_2 112475.030 LINESTRING ZM (801893.9 414... ## 2 gunt_ds junct_3 5338.169 LINESTRING ZM (728998.1 415... ## 3 tokusbulak junct_1 49114.525 LINESTRING ZM (829392.3 415... ## 4 alishur junct_1 179844.135 LINESTRING ZM (923686.9 418... ## 5 gunt_us junct_2 15945.881 LINESTRING ZM (795694.3 417... 5.3.2 Loading GIS Data and Model Creation As described in the RSMinerve user manual in Chapter 5 there, these three shapefiles can then be loaded into RSMinerve and automatically translated into an RSMinerve model via the Creation option (Foehn et al. 2020). In Figure 5.7, a screen shot is shown where in a somewhat arbitrary manner, all subbasins got assigned individual HBV models. Note the highlighted Creation button. Figure 5.7: Using the Creation Menu, the information from the shapefiles gets translated into a number of interlinked subcatchment and elevation band-specific models. Since the shapefile containing the elevation bands for each subbasin also contains mean subbasin elevation, we can extract the Altitude (Z) from the corresponding feature. Similarly, the area of the subbasins and the length of the rivers can all be computed from the shapefiles. The corresponding boxes should be ticked. After these steps, the student can click the Create Model button in the lower left corner of the window and then switch to the Model New tab to examine the resulting model. A screen shot showing the results is shown in Figure 5.8. Figure 5.8: The result of the GIS-based creation of the Gunt river basin model in RSMinerve. Note that even without proper arrangement of the individual model elements, the subbasins and their models are already visible. After rearranging the individual model elements and loading a background .jpeg-image (see Figure 5.1, the setup step has been completed. 5.3.3 Climate Data RSMinerve simulates discharge in response to climate forcing. We assume that the basic steps of GIS basin delineation have been carried out as described in Chapter ?? and the Section ?? there. We thus depart here assuming that the basin outlet (i.e. the location of the discharge gauge), the basin shape file as well as the digital elevation model are available.↩︎ "],["section-hbv-model.html", "5.4 The HBV model", " 5.4 The HBV model The HBV (Hydrologiska Byråns Vattenavdelning) model is a conceptual rainfall-runoff model which is suitable for many snow-fed river catchments (Bergström 1980; Lindström et al. 1997). You have studied the short description of the HBV model in the RS Minerve technical manual (Garcia Hernandez et al. 2020). A brief reminder of the HBV model implemented in RS Minerve is given in Figure 5.9. The model consists of a snow function which separates precipitation into solid and liquid parts and manages melting of snow and 3 storage reservoirs similar to the linear reservoir we saw in the recap section. Figure 5.9: The HBV model concept (source: RS MINERVE Technical Manual). Exercise: Model understanding - Drivers What are the model drivers (i.e. what input data do you need to provide)? Hint: Study Figure 5.9. Solution Figure 5.10 from the RS Minerve technical manual [rsminerve-tm] gives an overview over the model parameters and states. Figure 5.10: The HBV model parameters (source: RS MINERVE Technical Manual). Exercise: Model understanding - States What are the model states (i.e. what variables describe the amount of water in the system over time)? Hint: Values for initial water content in the different model compartments need to be defined prior to the simulation. Solution 5.4.1 HBV example: Simple model of the Nauvalisoy river catchment 5.4.1.1 Set up the model Open RS Minerve and create an HBV model with the correct area (Step-by-step guide). Add a climate station and link it to the HBV model (Step-by-step guide) . Climate data has been prepared in the file ./data/SyrDarya/Chirchiq/RSMinerve/ERA5_Nauvalisoy_1981_2013.csv. Import the data in RS Minerve and link it to the station data (Step-by-step guide). Change the model settings to calculate evaporation using and adapt the coordinates in the settings tab Step-by-step guide. Exercise: RS Minerve - Data visualization Use the Selection and plots tool in the Modules toolbar of RS Minerve and display the hourly model inputs precipitation and temperature for the example year 1984. What is the approximate temperature range over the year? What is the annual precipitation? Hint: Adapt the simulation period in the model tab accordingly. Don’t forget to set the initial conditions of the model to the last hour of 1983. Solution Exercise: RS Minerve - Compare evaporation models Compare the different methods to calculate hourly evaporation in 1984 and their impact on the simulated discharge. Is the uniform evaporation model suitable for seasonal discharge models? Hint: Export simulation results to the data base (how to). Solution "],["model-calibration-and-validation-1.html", "5.5 Model Calibration and Validation", " 5.5 Model Calibration and Validation 5.5.1 Basic Principles The large number of parameters of the HBV model can be adjusted within reasonable ranges to make the simulated river discharge more similar to the measured discharge. This process is sometimes called history matching because past measurements are used to adapt the model parameters. The following two exercises are important for understanding hydrological models. They are relevant whether you work as a modeler or as a decision maker. Try to first come up with a few own ideas and then study the solutions carefully. They will be discussed in the lectures in more detail. Exercise: Model calibration - Common difficulties What are typical difficulties inherent to the calibration of common semi-distributed (or fully distributed) hydrological models? Hint: Compare the number of parameters of the HBV model to the number of model states. Solution Exercise: Model calibration - Strategies to handle the difficulties Based on the list of common difficulties in the solution of exercise 5, think about possible strategies to address these challenges. Hint: A modeler proverb says “as simple as possible but as complex as necessary.” Solution 5.5.1.1 Calibration vs. validation As you have learned, semi- and fully distributed hydrological models are typically not well defined, i.e. you will find multiple sets of parameter combinations which will all yield reasonable model performance. An important strategy to gain confidence into a model is to split your time series into a calibration period and a validation period. During calibration, you adjust the model parameters to get a satisfying fit with the measured discharge. During the validation period you check how the calibrated model compares to discharge data that it hasn’t seen before, i.e. you judge the ability of the model to handle potentially different model input than it has seen during calibration. For example, if you have 30 years of discharge time series available you can use the first 10 years for model calibration (i.e. you run the model calibration procedure only over the first 10 years of the time series) and use the last 20 years for model validation. A good model should perform satisfactorily not only during calibration but also during validation. In hydrology, it is recommended to do cross-validation, i.e. to calibrate and validate a model with several combinations of calibration and validation periods and to summarize the overall model performance. 5.5.1.2 Performance indicators Model performance can be expressed in different ways. Some indicators are better suited to calibrate high flows (e.g. root mean squared error) and some are better suited to calibrate low flows (e.g. Nash coefficient for logarithm values). Others evaluate the models ability to reproduce the exceedance of a given discharge which is especially important for flood modelling. The calibration result depends on the choice of the performance indicators. For the automated calibration in RS Minerve, the user can choose a combination of several performance indicators. Care has to be taken to choose them according to the models goal. Task: Read up on performance indicators The RS Minerve technical manual (Garcia Hernandez et al. 2020) summarizes the most important performance indicators typically used in hydrological modelling. For the more ambitious modellers, the manual includes references to the literature. Read Chapter 3 of the technical manual. 5.5.1.3 The general modelling process Hydrological modelling is an iterative process. Figure 5.11: The modelling process is iterative. 5.5.2 Practical Steps 5.5.2.1 Manual calibration of synthetic discharge The following section describes the steps for manual model calibration. We use the example of the Nauvalisoy river catchment and calibrate it with synthetic discharge measurements to deepen our understanding of the HBV model. For the synthetic discharge measurements it is theoretically possible to find a perfect match between simulated and measured data which is typically not the case for real measurements. Figure 5.12: The simulated discharge (green) differs from the reference discharge (blue) before model calibration. Exercise: RS Minerve - Manual calibration of synthetic discharge Add a comparator and a source object to your Nauvalisoy model and load river discharge measurements (filename), then run the model how to. With default parameters, the outcome of this step should look like Figure 5.12. Should your result differ significantly from the one shown here, compare the parameters of the HBV mode to the ones in the file ./data/SyrDarya/Chirchiq/RSMinerve/nauvalisoy_PAR_precalibration.txt. You can import the parameter set to RS Minerve via Import P in the model properties toolbar. Hints: Start adjusting the parameters of the HBV model to make the simulated discharge curve more similar to the reference discharge. Start with the parameters of the snow function and then proceed downwards through the compartments of the HBV model. Focus on parts of the curves. Try to figure out which parameters influence the timing of the discharge peak, which the height of the peak and which the form of the recession curve. Give yourself some time for this task before you go peaking in the results. The parameters of the calibrated model are linked here. Exercise: RS Minerve - Load real discharge measurements Import the database file for Nauvalisoy including measured discharge at the outlet of the catchment ./data/SyrDarya/Chirchiq/RSMinerve/ERA5Q_Nauvalisoy_1981_2013.csv into RS Minerve and connect it to your model. Adapt the simulation time period to start on Jan 1 1981 at 1 a.m. until Dec 31 2013 at 11 p.m (the full period with available data). The simulation will take place at the maximum resolution of the forcing data, i.e. at hourly time steps. The discharge measurements are available as averages over 8-11 days. We choose an output interval of 2 days. As the initial water content in the system is not known, a warm-up period of 1 year is typically used to minimize the impact of the unknown initial conditions on the simulation results. You can implement the warm-up period in the comparator object under Parameters. Use the pre-calibration parameter set in ./data/SyrDarya/Chirchiq/RSMinerve/nauvalisoy_PAR_precalibration.txt. Figure 5.13 shows the resulting discharge time series in the comparator object. knitr::include_graphics(&quot;_bookdown_files/FIG_RS_MINERVE/fig_Nauvalisoy_real_discharge_base.png&quot;) Figure 5.13: Simulated and measured discahrge at the outlet of the Nauvalisoy catchment. The simulated discharge peak arrives several months before the observed discharge peak. You may invest 10 minutes to try it out but by merely adjusting the parameters of our HBV model, we will not be able to shift the simulated discharge time series sufficiently to reproduce the measured discharge. We are facing a conceptual model error which we have to identify and eliminate. Figure 5.14 shows the relationship between elevation and temperature of the 4 meteo stations in the Gunt river catchment. The elevation dependence of temperature is highly important for snow-melt driven river catchments. ## MeteoStation locations and elevation meteoStations &lt;- # just put everything in a regular dataframe tibble(code = c(38953, 38954, 38956, 38950), masl = c(3746, 2075, 3438, 2566)) # Load Gunt data data &lt;- read_rds(&#39;./data/AmuDarya/Gunt/StationData/gunt_data_cleaned.Rds&#39;) %&gt;% dplyr::filter(str_detect(type, &quot;T&quot;)) %&gt;% # Filter for data types that contain T mutate(code = as.numeric(code)) %&gt;% # Cast code column to numeric left_join(meteoStations) # Add the station altitude to the data # Plotting the dataframe data %&gt;% dplyr::filter(!str_detect(type, &quot;meanmax&quot;), !str_detect(type, &quot;meanmin&quot;)) %&gt;% dplyr::mutate(Variable = type) %&gt;% group_by(code, Variable) %&gt;% summarize(norm = mean(norm, na.rm = TRUE), masl = first(masl)) %&gt;% ggplot(aes(masl, norm, colour = Variable)) + geom_point() + geom_smooth(method = &quot;lm&quot;) + labs(x = &quot;Altitude [masl]&quot;, y = &quot;T [deg C]&quot;) + theme_bw() Figure 5.14: Elevation dependence of temperature in the Gunt river catchment. 5.5.2.2 Manual calibration of the Nauvalisoy model "],["application-investigating-climate-change-impacts.html", "5.6 Application: Investigating Climate Change Impacts", " 5.6 Application: Investigating Climate Change Impacts "],["discussion.html", "5.7 Discussion", " 5.7 Discussion "],["HydroModelsEmpiricalModels.html", "Chapter 6 Discharge Forecasting with Predictive Inference ", " Chapter 6 Discharge Forecasting with Predictive Inference "],["prerequisites.html", "Prerequisites", " Prerequisites This is the first ‘practical’ Chapter of the book and comes with software requirements. For the analysis of the available data we use R (R Core Team 2013). R is a computer language and environment for data analysis, statistical computation and data visualization. It can be downloaded at &lt;https://www.r-project.org&gt;. Together with R, we are using RStudio as the IDE (Team’ 2020). Some core R software packages used in this Chapter need to be installed so that the analyses can be done as shown there. The installation can be done in the following way: # Core Libraries install.packages(&#39;tidyverse&#39;) # Meta - dplyr, ggplot2, purrr, tidyr, stringr, forcats install.packages(&#39;lubridate&#39;) # date and time install.packages(&#39;timetk&#39;) # Time series data wrangling, visualization and preprocessing # Extras if (!require(devtools)) install.packages(&quot;devtools&quot;, repos = &quot;http://cran.us.r-project.org&quot;) # install_github(&quot;boxuancui/DataExplorer&quot;, ref=&quot;develop&quot;) # Simplifies and automates EDA process and reporting # Data and helper functions devtools::install_github(&quot;hydrosolutions/riversCentralAsia&quot;) The packages can then be loaded and made available in your R session. library(devtools) library(tidyverse) library(lubridate) library(timetk) library(DataExplorer) library(riversCentralAsia) When other, additional packages are needed, they will be loaded in the corresponding Sections below. Please also remember the following rules when working with R dataframes in the tidyverse: Every column is variable. Every row is an observation. Every cell is a single value. A final note. In all of the following, we mostly use the powerful data manipulation and visualization techniques for time series data as provided by the timetk package. This package is in active development and greatly facilitates any work with time series data as it, among other things, nicely integrates with the R ‘tidyverse.’ "],["Chap9PredictiveInference.html", "6.1 Forecasting Using Predictive Inference", " 6.1 Forecasting Using Predictive Inference In this Section, we are concerned with predictive inference using observed data to predict future data that is not known yet but that is important to forecast with high confidence and low uncertainty. In other words, it is assumed that we can encapsulate historic patterns in a model a learn about the future with such model. In hydrology, we are dealing with time series, i.e. ordered observations in time. A generic model structure thus can be specified in the following way \\[ y(t+\\Delta t) = f(y(t),x(t)) + \\epsilon(t) \\] where \\(y(t+\\Delta t)\\) is called the forecast target (discharge at a particular gauge in our case) and is the variable that we want to forecast in the future, i.e. \\(\\Delta t\\) time away from now. \\(y(t)\\) denotes past known observations of discharge up and including time \\(t\\). Similarly, \\(x(t)\\) denotes other variables of interest, called external regressors, that might be relevant to obtain good quality forecasts such as meteorological data from local stations, including precipitation and temperature. Finally, \\(f()\\) denotes the type of model that is being used for forecasting and \\(\\epsilon\\) are the time-dependent error terms. If, for example, one would use a linear modeling approach without external regressors, such type of model could simply be written as \\[ y(t+\\Delta t) = \\beta_{0} + \\beta_{1} \\cdot y(t) + \\epsilon(t) \\] In the model specification above, the aim is to predict into the future with a lead time of \\(\\Delta t\\), i.e. for example one month ahead. The lead-time model can be written in equivalent form using lags in the following way \\[ y(t) = f(y(t-lag),x(t-lag)) + \\epsilon(t) \\] where \\(lag = \\Delta t\\). This just means that we use all the available observations until and including \\(t-lag\\) for predicting the target at time \\(t\\). We will use this specification throughout the Chapter when working with and developing new forecasting models feature engineering experimentation and ensembling Knowledge of key events, i.e. date shifts holidays Deep learning (data permitting) Boosting errors "],["forecasting-in-the-central-asian-hydromets.html", "6.2 Forecasting in the Central Asian Hydromets", " 6.2 Forecasting in the Central Asian Hydromets 6.2.1 Background The key agencies that that are charged in predicting river discharge regularly in Central Asia are the Hydrometeorological Agencies. For predicting mean discharge over a certain future period, they use different types of statistical models. The particular type of model they use depends on the available hydrological and meteorological data for a particular river whose mean discharge is to be forecasted and on the type of the forecast. Types of forecasts include daily forecasts, i.e. \\(\\Delta t = 1 \\text{ day}\\), pentadal forecasts, i.e. \\(\\Delta t = 5 \\text{ days}\\), decadal forecasts, i.e. \\(\\Delta t = 10 \\text{ days}\\), monthly forecasts, i.e. \\(\\Delta t = 1 \\text{ month}\\), and seasonal forecasts, i.e. \\(\\Delta t = 6 \\text{ months}\\). To this date, these types of forecasts are performed at regular intervals by the operational hydrologists. Normally, this requires normally a lot of manual work. Recently, selected Hydrometeorological Agencies use automated software to automatize this type of work13. In Uzbekistan, for example, the following list of forecast objects exists in the Hydromet. List of Uzbek forecast target objects. Forecast type abbreviations are dl: daily forecast, m: monthly forecast, s: seasonal forecast. Country abbreviations are UZ: Uzbekistan, KG: Kyrgyzstan and TJ: Tajikistan. Source: Uzbek Hydrometeorological Service. River Gauge (Target Object) Gauge Code Country Types of Forecasts Chirchik Inflow to Charvak Res. 16924 UZ dl, m, s Chirchik Inflow to Charvak Res. + Ugam River 16924 + 16300 UZ m, s Akhangaran Irtash 16230 UZ m, s Chadak Dzhulaysay 16202 UZ m, s Gavasay Gava 16193 UZ m, s Padsha-Ata Tostu 16176 KG m, s Kara Darya Inflow to Andizhan water reservoir 16938 UZ/KG dl, m, s Isfayramsoy Uch-Kurgan 16169 KG m, s Sokh Sarykanda 16198 UZ dl (May - Sep.), m, s Sanzar Kyrk 16223 UZ m, s Naryn Inflow to Toktogul water reservoir 16936 KG m, s Vaksh Inflow to Nurek water reservoir 17084 (Vakh river - Darband gauge) TJ m, s Kafirnigan Tartki 17137 TJ m, s Tupalang Inflow to Tupalang water reservoir 17194 UZ m, s Sangardak Keng-Guzar 17211 UZ m, s Akdarya Inflow to Gissarak water reservoir 17464 (Akfariya river - Hissarak gauge) UZ m, s Yakkabagdarya Tatar 17260 UZ m, s Uryadariya + Kichik Uryadariya Inflow to Pachkamar water reservoir 17279 (Uryadariya river - Bazartepe gauge) + 17275 (Kichik Uryadariya - Gumbulak gauge) UZ m, s Zeravshan Inflow to Rovatkhodzha hydro work 17461 UZ m, s It should be noted that all of the Hydromets have such type of lists with different forecast target and types. As can be seen from the above list, Uzbekistan does neither issue pentade nor decadal forecasts, i.e. types of forecasts which are widely used in the Kyrgyz Hydromet in contrast. Finally, seasonal forecasts in the Uzbek Hydromet are issued twice prior to the irrigation season with 3. - 5. March being the first issues data range and 3. - 5. April being the second one. Converse to this, monthly forecasts are issues between the 25. - 27. day each month. Finally, decadal and pentade forecasts are issued each morning at the day of the end of the corresponding pentade or decade. It is important to emphasize again that there is currently no standardized way in the region to produce these forecasts. While in some instances, a particular approach and method works very well, it fails to produce acceptable forecasts in other basins. However, as we shall see, certain techniques work very well for particular forecast horizons which then explains why such type of technique has become widely used in the region. 6.2.2 Forecasting for What and Whom? Why is all this work is required? What is the purpose of predicting mean discharge into the future at regular intervals? Important recipients of the forecast products include the Water Authorities which are in charge of delivering adequate amounts of water for irrigation at the right time and location. It all starts with pre-season irrigation planning. The main irrigation season in most of Central Asia is from April 1. through the end of September. Previous to the start of the irrigation season, irrigation plans are drafted based on computed irrigation water demand of all the water users that are connected to a particular irrigation system. These irrigation system are defined in terms of canal topology where demand gets aggregated from the bottom up to the Rayvodkhozes and the Oblvodkhozes. The later then starts with the pre-season irrigation planning given the water irrigation system-level demand. These plans specify decadal water discharge for each irrigation system and the corresponding canals. Demand is one thing, but expected supply from the Central Asian rivers another. In order to be able to match the irrigation water demand, the water authorities do what is needed and receive from the Hydromets first the seasonal discharge forecasts. Given this forecast of irrigation-season water availability, the water authorities then adjust their plans given the particular expected circumstances. If, for example, an exceptionally dry year is expected, they activate limit plans and reduce planned water distributions according to forecasted quantities. If a wet year is expected, they do not perform these adjustments and maybe even release water from reservoirs previous to the irrigation season to ensure enough storage capacity in the reservoirs. With the beginning of the irrigation season, another seasonal forecast is carried out by the Hydromets and communicated to the Central Asian water authorities. Given the updated forecast, planning is revised and adjusted accordingly. Then, finally, throughout the irrigation season pentadal, decadal and monthly forecasts are produced constantly to carefully balance water demand with supplies while the season is under way. Needless to say that other important customers for hydrometeorological forecasts exist, including for example airports, road departments that need to ensure road safety, agricultural clusters that are interested in frost and hail warnings, local authorities that need to be alerted in the case of extreme local weather conditions, etc. The software used is called iEasyHydro and currently operationalized in the Kyrgyz Hydromet. Other Hydromets are testing the software (as of 2020). More information about iEasyHydro can be obtained by contacting the author.↩︎ "],["Chap9DataPreparation.html", "6.3 Data and Preparation", " 6.3 Data and Preparation 6.3.1 Available Data The riversCentralAsia Package provides available data of the gauging and meteorological stations in the Chirchik River Basin. Before starting any type of modeling, it is important to get a good understanding of the data that we are dealing with and whether there exist problems with the raw data that need to be addressed prior to modeling. Problems usually include data gaps and outliers as data record that one obtains are usually ever complete nor clean of errors. The steps performed here are thus required steps for any type of successful modeling and should be performed with great care. We concentrate our efforts here on discharge records and data from meteorological stations in the Chirchik River Basin. The techniques shown here for decadal (10-days) data naturally extend to monthly data and other basins. 6.3.2 Gap Filling Discharge Data In the following, we will work with decadal discharge data from the two main tributaries, i.e. the Chatkal and (Gauge 16279) Pskem rivers (Gauge 16290) and the data of the inflow to the Charvak reservoir (Gauge 16924). The goal is to analyze the data and prepare for modeling. First, let us load the relevant discharge data. data &lt;- ChirchikRiverBasin # load data q_dec_tbl &lt;- data %&gt;% filter(code == &#39;16279&#39; | code == &#39;16290&#39; | code == &#39;16924&#39;) # Note for the new name of the object, we choose to add periodicity (_dec_) and data type (_tbl for tibble/dataframe) to the data name. This just helps to stay organized and is good practice in R programming. q_dec_tbl ## # A tibble: 9,072 x 14 ## date data norm units type code station river basin resolution ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; ## 1 1932-01-10 48.8 38.8 m3s Q 16279 Khudaydod Chatkal Chirch… dec ## 2 1932-01-20 48.4 37.5 m3s Q 16279 Khudaydod Chatkal Chirch… dec ## 3 1932-01-31 42.4 36.6 m3s Q 16279 Khudaydod Chatkal Chirch… dec ## 4 1932-02-10 43.7 36.4 m3s Q 16279 Khudaydod Chatkal Chirch… dec ## 5 1932-02-20 44.2 36.3 m3s Q 16279 Khudaydod Chatkal Chirch… dec ## 6 1932-02-29 47.7 36.9 m3s Q 16279 Khudaydod Chatkal Chirch… dec ## 7 1932-03-10 54.1 39.4 m3s Q 16279 Khudaydod Chatkal Chirch… dec ## 8 1932-03-20 63.2 47.6 m3s Q 16279 Khudaydod Chatkal Chirch… dec ## 9 1932-03-31 103 60.5 m3s Q 16279 Khudaydod Chatkal Chirch… dec ## 10 1932-04-10 103 86.4 m3s Q 16279 Khudaydod Chatkal Chirch… dec ## # … with 9,062 more rows, and 4 more variables: lon_UTM42 &lt;dbl&gt;, ## # lat_UTM42 &lt;dbl&gt;, altitude_masl &lt;dbl&gt;, basinSize_sqkm &lt;dbl&gt; You can get more information about the available data by typing ?ChirchikRiverBasin. It is advisable to check at this stage for missing data in time series and to fill gaps where present. As can be seen in Figure 6.1 , close inspection of the time series indeed reveals some missing data in the 1940ies. q_dec_tbl %&gt;% plot_time_series(date,data, .facet_vars = code, .smooth = FALSE, .interactive = TRUE, .x_lab = &quot;year&quot;, .y_lab = &quot;m^3/s&quot;, .title = &quot;&quot; ) Figure 6.1: Discharge data of selected gauges in the upstream zone of runoff formation in the Chirchik River Basin. Data Source: Uzbek Hydrometeorological Service. Figure 6.1 and the following Figures are interactive, so you can zoom in to regions of interest. Missing data are also confirmed by the warning that the function timetk::plot_time_series() throws (suppressed here). Statistics of the missing data can be easily obtained. As the Table below shows, we can do this analysis for each discharge station separately. q_dec_tbl %&gt;% group_by(code) %&gt;% summarize(n.na = sum(is.na(data)), na.perc = n.na/n()*100) ## # A tibble: 3 x 3 ## code n.na na.perc ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 16279 15 0.496 ## 2 16290 39 1.29 ## 3 16924 42 1.39 Summarizing the number of observation with missing data reveals 15 data points for station 16279 (0.5 % of total record length) and 39 for station 16290 (1.3 % of total record length). As there are only very few gaps in the existing time series, we use a simple method to fill these. Wherever there is a gap, we fill in the corresponding decadal norm as stored in the norm column in the object q_dec_tbl. The visualization of the results confirms that our simple gap filling approach is indeed satisfactory (Figure 6.2). q_dec_filled_tbl &lt;- q_dec_tbl q_dec_filled_tbl$data[is.na(q_dec_filled_tbl$data)] = q_dec_filled_tbl$norm[is.na(q_dec_filled_tbl$data)] # Gap filling step q_dec_filled_tbl %&gt;% plot_time_series(date, data, .facet_vars = code, .smooth = FALSE, .interactive = TRUE, .x_lab = &quot;year&quot;, .y_lab = &quot;m^3/s&quot;, .title = &quot;&quot; ) Figure 6.2: Gap filled Pskem and Chatkal river discharges. A note of caution here. This simple gap filling technique reduces variance in the time series. It should only be used when the percentage of missing data is low. As will be discussed in the next Section 4.1.3 below, better techniques have to be utilized when there exist substantial gaps and in the case of less regular data. Finally, we discard the norm data which we used for gap filling of the missing discharge data and convert the data to wide format (see the Table below) to add to it meteorological data in the next Section. q_dec_filled_wide_tbl &lt;- q_dec_filled_tbl %&gt;% # again we use the name convention of objects as introduced above mutate(code = paste0(&#39;Q&#39;,code %&gt;% as.character())) %&gt;% # Since we convert everything to long form, we want to keep information as compact as possible. Hence, we paste the type identifier (Q for discharge here) in from of the 5 digit station code. dplyr::select(date,data,code) %&gt;% # ... and then ditch all the remainder information pivot_wider(names_from = &quot;code&quot;,values_from = &quot;data&quot;) # in order to pivot to the long format, we need to make a small detour via the wide format. q_dec_filled_long_tbl &lt;- q_dec_filled_wide_tbl %&gt;% pivot_longer(-date) # and then pivot back q_dec_filled_wide_tbl ## # A tibble: 3,024 x 4 ## date Q16279 Q16290 Q16924 ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1932-01-10 48.8 38.3 87.1 ## 2 1932-01-20 48.4 37.7 86.1 ## 3 1932-01-31 42.4 36.2 78.6 ## 4 1932-02-10 43.7 35.6 79.3 ## 5 1932-02-20 44.2 35 79.2 ## 6 1932-02-29 47.7 37.1 84.8 ## 7 1932-03-10 54.1 43.1 97.2 ## 8 1932-03-20 63.2 47 110 ## 9 1932-03-31 103 72.1 175 ## 10 1932-04-10 103 73.2 176 ## # … with 3,014 more rows As a result, we now have a complete record of decadal discharge data for the two main tributaries of the Chirchik river and the inflow time series to Charvak Reservoir from the beginning of 1932 until and including 2015, i.e. 84 years. The same type of preparatory analysis will now be carried out for the meteorological data. 6.3.3 Gap Filling Meteorological Data Here, we use precipitation and temperature data from Pskem (38462), Chatkal (38471) and Charvak Reservoir (38464) Meteorological Stations (see Chapter ?? for more information on these stations). We also have data from Oygaing station (Station Code 38339) but the record only starts in 1962 and the time resolution is monthly. Therefore, we do not take this station into account here for the time being. We start with precipitation and plot the available data. p_dec_tbl &lt;- data %&gt;% filter(type==&quot;P&quot; &amp; code!=&quot;38339&quot;) p_dec_tbl %&gt;% plot_time_series(date,data, .facet_vars = code, .interactive = TRUE, .smooth = FALSE, .title = &quot;&quot;, .y_lab = &quot;mm/decade&quot;, .x_lab = &quot;year&quot; ) Figure 6.3: Raw decadal precipitation data from Pskem (38462), Charvak Reservoir (38471) and Chatkal Meteo Station (38471). The precipitation data from these 3 stations shows some significant data gaps. The Chatkal Meteorological Station that is located in Kyrgyzstan apparently did not work in the post-transition years and continuous measurements were only resumed there in 1998. Let us see what happens if we were to use the same simple gap filling technique that we introduced above for discharge. p_dec_filled_tbl &lt;- p_dec_tbl p_dec_filled_tbl$data[is.na(p_dec_filled_tbl$data)] = p_dec_filled_tbl$norm[is.na(p_dec_filled_tbl$data)] p_dec_filled_tbl %&gt;% plot_time_series(date,data, .facet_vars = code, .interactive = TRUE, .smooth = FALSE, .title = &quot;&quot;, .y_lab = &quot;mm/decade&quot;, .x_lab = &quot;year&quot; ) Figure 6.4: Precipitation Data gap-filled with norms. The filled values from 1990 - 2000 in the case of the Station 38471 indicate that the norm-filling technique is not good. Closely inspect the significant data gap in the 1990ies at Station 38741 (tip: play around and zoom into the time series in the 1990ies in Figure 6.3 and comparing it with the resulting gap-filled timeseries in Figure ??. We see that our technique of gap filling with long-term norms is not suitable for this type of data and the significant gap size. The effect of variance reduction is also clearly visible. Hence, we resort to a more powerful gap filling technique that uses a (regression) model to impute the missing values from existing ones at the neighboring stations, i.e. Stations 38462 and 38464. To do so, we utilize an R package that is tightly integrated in the tidyverse14. library(simputation) # First, we bring the data into the suitable format. p_dec_wide_tbl &lt;- p_dec_tbl %&gt;% mutate(code = paste0(&#39;P&#39;,code %&gt;% as.character())) %&gt;% dplyr::select(date,data,code) %&gt;% pivot_wider(names_from = &quot;code&quot;,values_from = &quot;data&quot;) # Second, we impute missing values. p_dec_filled_wide_tbl &lt;- p_dec_wide_tbl %&gt;% impute_rlm(P38471 ~ P38462 + P38464) %&gt;% # Imputing precipitation at station 38471 using a robust linear regression model impute_rlm(P38462 ~ P38471 + P38464) %&gt;% # Imputing precipitation at station 38462 using a robust linear regression model impute_rlm(P38464 ~ P38462 + P38471) # Imputing precipitation at station 38464 using a robust linear regression model p_dec_filled_long_tbl &lt;- p_dec_filled_wide_tbl %&gt;% pivot_longer(c(&#39;P38462&#39;,&#39;P38464&#39;,&#39;P38471&#39;)) p_dec_filled_long_tbl%&gt;% plot_time_series(date,value, .facet_vars = name, .interactive = TRUE, .smooth = FALSE, .title = &#39;&#39;, .y_lab = &quot;mm/decade&quot;, .x_lab = &quot;year&quot; ) (#fig:EMrawData_P_rlm)Precipitation Data gap filled with a robust linear regression modeling approach As you can see, we use simple linear regression models to impute missing value in the target time series using observations from the neighboring stations. Through simple visual inspection, it becomes clear that this type of regression model for gap filling is better suited than the previous approach chosen. Let us check whether we could successfully fill all gaps with this robust linear regression approach. p_dec_filled_long_tbl %&gt;% group_by(name) %&gt;% summarize(n.na = sum(is.na(value)), n.na.perc = n.na/n()*100) ## # A tibble: 3 x 3 ## name n.na n.na.perc ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 P38462 12 0.402 ## 2 P38464 12 0.402 ## 3 P38471 3 0.100 It turns out that we still have very few gaps to deal with. We can see them by simply visualizing the wide tibble. The problem persisted at times when two or more values were missing across the available stations at the same time and where thus the linear regression could not be carried out. p_dec_filled_wide_tbl %&gt;% head(10) ## # A tibble: 10 x 4 ## date P38462 P38464 P38471 ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1933-01-10 NA NA 2 ## 2 1933-01-20 NA NA 10 ## 3 1933-01-31 NA NA 5 ## 4 1933-02-10 NA NA 33 ## 5 1933-02-20 NA NA 8 ## 6 1933-02-28 NA NA 10 ## 7 1933-03-10 NA NA 31 ## 8 1933-03-20 NA NA 50 ## 9 1933-03-31 NA NA 6 ## 10 1933-04-10 23 21.3 13 p_dec_filled_wide_tbl %&gt;% tail() ## # A tibble: 6 x 4 ## date P38462 P38464 P38471 ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2015-11-10 72 81 19 ## 2 2015-11-20 122 76 43 ## 3 2015-11-30 7 2 3 ## 4 2015-12-10 NA NA NA ## 5 2015-12-20 NA NA NA ## 6 2015-12-31 NA NA NA We can solve the issues related to the missing values at the start of the observation record by using the same technique as above and by only regressing P38462 and P38464 on P38471. p_dec_filled_wide_tbl &lt;- p_dec_filled_wide_tbl %&gt;% impute_rlm(P38462 ~ P38471) %&gt;% # Imputing precipitation at station 38462 using a robust linear regression model impute_rlm(P38464 ~ P38471) # Imputing precipitation at station 38464 using a robust linear regression model p_dec_filled_wide_tbl %&gt;% head(10) ## # A tibble: 10 x 4 ## date P38462 P38464 P38471 ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1933-01-10 5.60 5.08 2 ## 2 1933-01-20 18.3 16.7 10 ## 3 1933-01-31 10.4 9.46 5 ## 4 1933-02-10 54.9 50.3 33 ## 5 1933-02-20 15.2 13.8 8 ## 6 1933-02-28 18.3 16.7 10 ## 7 1933-03-10 51.8 47.3 31 ## 8 1933-03-20 82.0 75.0 50 ## 9 1933-03-31 12.0 10.9 6 ## 10 1933-04-10 23 21.3 13 Converse to this, the complete set of observations is missing for December 2015. We will thus remove these non-observations from our tibble. p_dec_filled_wide_tbl &lt;- p_dec_filled_wide_tbl %&gt;% na.omit() p_dec_filled_wide_tbl %&gt;% tail() ## # A tibble: 6 x 4 ## date P38462 P38464 P38471 ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2015-10-10 5 1 0 ## 2 2015-10-20 89 108 58 ## 3 2015-10-31 34 40 12 ## 4 2015-11-10 72 81 19 ## 5 2015-11-20 122 76 43 ## 6 2015-11-30 7 2 3 p_dec_filled_long_tbl &lt;- p_dec_filled_wide_tbl %&gt;% pivot_longer(-date) Inspecting the temperature data, we see similar data issues as in the precipitation data set. t_dec_tbl &lt;- data %&gt;% filter(type==&quot;T&quot;) t_dec_tbl %&gt;% plot_time_series(date,data, .facet_vars = code, .interactive = TRUE, .smooth = FALSE, .title = &#39;&#39;, .y_lab = &quot;deg. Celsius&quot;, .x_lab = &quot;year&quot; ) (#fig:EMrawData_T)Raw temperature data from the meteorological stations Pskem (38462) and Chatkal (38471) # First, we bring the data into the suitable format. t_dec_wide_tbl &lt;- t_dec_tbl %&gt;% mutate(code = paste0(&#39;T&#39;,code %&gt;% as.character())) %&gt;% dplyr::select(date,data,code) %&gt;% pivot_wider(names_from = &quot;code&quot;,values_from = &quot;data&quot;) # Second, we impute missing values. t_dec_filled_wide_tbl &lt;- t_dec_wide_tbl %&gt;% impute_rlm(T38471 ~ T38462) %&gt;% # Imputing precipitation at station 38471 using a robust linear regression model impute_rlm(T38462 ~ T38471) # Imputing precipitation at station 38462 using a robust linear regression model t_dec_filled_long_tbl &lt;- t_dec_filled_wide_tbl %&gt;% pivot_longer(c(&#39;T38462&#39;,&#39;T38471&#39;)) t_dec_filled_long_tbl%&gt;% plot_time_series(date,value, .facet_vars = name, .interactive = TRUE, .smooth = FALSE, .title = &#39;&#39;, .y_lab = &quot;deg. Celsius&quot;, .x_lab = &quot;year&quot; ) (#fig:EMrawData_T_rlm)Temperature data gap filled with robust linear regression modeling. There are some irregularities in the temperature time series of Chatkal Meteorological Station in the first decade of the 20th century (tip: zoom in to see these more clearly). Note that these were not introduced by the gap filling technique that we used but are most likely wrong temperature readings. We will return to these in the outlier analysis below in Section 4.1.4. t_dec_filled_long_tbl %&gt;% group_by(name) %&gt;% summarize(n.na = sum(is.na(value)), n.na.perc = n.na/n()*100) ## # A tibble: 2 x 3 ## name n.na n.na.perc ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 T38462 3 0.100 ## 2 T38471 3 0.100 To see where the missing value are, we find them easily again by looking at the head and tail of the tibble. t_dec_filled_wide_tbl %&gt;% head() ## # A tibble: 6 x 3 ## date T38462 T38471 ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1933-01-10 -6.9 -16.6 ## 2 1933-01-20 -6.1 -15.5 ## 3 1933-01-31 -6.3 -15.6 ## 4 1933-02-10 -2 -8.6 ## 5 1933-02-20 -3.3 -12.5 ## 6 1933-02-28 -0.1 -8.5 t_dec_filled_wide_tbl %&gt;% tail() ## # A tibble: 6 x 3 ## date T38462 T38471 ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2015-11-10 2.4 -2.5 ## 2 2015-11-20 2 -2.2 ## 3 2015-11-30 4.6 -3.7 ## 4 2015-12-10 NA NA ## 5 2015-12-20 NA NA ## 6 2015-12-31 NA NA Finally, we remove the non observations again as above with the function na.omit. t_dec_filled_wide_tbl &lt;- t_dec_filled_wide_tbl %&gt;% na.omit() t_dec_filled_long_tbl &lt;- t_dec_filled_wide_tbl %&gt;% pivot_longer(-date) To deal with the missing values at the end of the observational record, we could also have used any other technique. Using the norm values however would have artificially reduced the variance in both cases as explained above. Furthermore and at least in the case of temperature, it is also questionable to what extent a norm calculated over the last 84 years is still representative given global warming. We will look in this important and interesting topic in the next section. 6.3.4 Anomalies and Outliers We use the function timetk::plot_anomaly_diagnostics to investigate anomalies in the time series. For discharge, we first log-transform the raw data with the following transformation to reduce the variance of the original data. \\[ \\hat{q}(t) = log(q(t) + 1) \\] where \\(\\hat{q}(t)\\) denotes the transformed discharge. Prior to the log transformation, 1 is added so as to avoid cases where discharge would be 0 and the logarithmic transform thus undefined. The transformation can easily be done with the log1p() function in R. Backtransformation via the function expm1() simply involves taking the exponent and subtracting 1 from the result. Figure ?? shows the result. The exceptionally wet year 19169 shows up as anomalous in the Chatkal River Basin and at the downstream Charvak Reservoir inflow gauge. , ?? and ?? show anomalies diagnostics of the available data. q_dec_filled_long_tbl %&gt;% plot_anomaly_diagnostics(date, value %&gt;% log1p(), .facet_vars = name, .frequency = 36, .interactive = TRUE, .title = &quot;&quot;) Figure 6.5: Anomaly diagnostics of discharge data. The transparent grey band shows the width of the normal range. The highly anomalous wet year of 1969 is clearly visible in the discharge record of the Chatkal river basin (Station 16279). The investigation of precipitation anomalies shows a succession of regular anomalous wet events over time. It is interesting to see that the winter 1968/69 regularly anomalous at all three stations (Figure 6.6, zoom in to investigate). p_dec_filled_long_tbl %&gt;% plot_anomaly_diagnostics(date, value, .facet_vars = name, .interactive = TRUE, .title = &quot;&quot;) Figure 6.6: Anomaly diagnostics of precipitation data. While intuitively, we would have expected an eceptionally mild winter in 1968/69 due to the precipitation excess, the corresponding anomaly does not show up in the temperature record (Figure 6.7). t_dec_filled_long_tbl %&gt;% plot_anomaly_diagnostics(date,value, .facet_vars = name, .interactive = TRUE, .title = &quot;&quot;) Figure 6.7: Anomaly diagnostics of temperature data. Apart from the identification of extremal periods since as the 1969 discharge year in the Chatkal river basin, the diagnostics of anomalies also helps to identify likely erroneous data records. In Figure @ref(anomalies_T) for example, when we zoom into the data of the series T38471 in the first decade of the 21st century, problems in relation to positive anomalies during the winter are visible in 4 instances. One explanation would be that in at least some instances, the data are erroneously recorded as positive values when in fact they were negative (see dates ‘2002-01-31,’ ‘2005-01-10’ and ‘2007-02-28,’ Chatkal Station 38471). 6.3.5 Putting it all together Finally, we are now in the position to assemble all data that we will use for empirical modeling. The data is stored in long and wide form and used accordingly where required. For example, in Section @ref{TimeSeriesReg}, we are working with the wide data format to investigate model features in linear regression. Note that we also add a column with a decade identifier. Its use will become apparent in the Section 6.5 below. # Final concatenation data_wide_tbl &lt;- right_join(q_dec_filled_wide_tbl,p_dec_filled_wide_tbl,by=&#39;date&#39;) data_wide_tbl &lt;- right_join(data_wide_tbl,t_dec_filled_wide_tbl,by=&#39;date&#39;) # Add period identifiers (decades in this case) s &lt;- data_wide_tbl$date %&gt;% first() e &lt;- data_wide_tbl$date %&gt;% last() decs &lt;- decadeMaker(s,e,&#39;end&#39;) decs &lt;- decs %&gt;% rename(per=dec) data_wide_tbl &lt;- data_wide_tbl %&gt;% left_join(decs,by=&#39;date&#39;) # Creating long form data_long_tbl &lt;- data_wide_tbl %&gt;% pivot_longer(-date) # Cross checking completeness of record data_long_tbl %&gt;% group_by(name) %&gt;% summarize(n.na = sum(is.na(value)), n.na.perc = n.na/n()*100) ## # A tibble: 9 x 3 ## name n.na n.na.perc ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 P38462 0 0 ## 2 P38464 0 0 ## 3 P38471 0 0 ## 4 per 0 0 ## 5 Q16279 0 0 ## 6 Q16290 0 0 ## 7 Q16924 0 0 ## 8 T38462 0 0 ## 9 T38471 0 0 ## Temp storage of data (remove later) # fPath &lt;- &#39;/Users/tobiassiegfried/Dropbox (hydrosolutions)/1_HSOL_PROJECTS/PROJECTS/SDC/DKU_WRM_COURSE_CA/Course Materials/Handbook/Applied_Hydrological_Modeling_Bookdown/temp/&#39; # saveRDS(data_wide_tbl,file=paste(fPath,&#39;data_wide_tbl&#39;,sep=&quot;&quot;)) # saveRDS(data_long_tbl,file=paste(fPath,&#39;data_long_tbl&#39;,sep=&quot;&quot;)) A consistent data record from 1933 until and including November 2015 is now prepared^Please note that by using left_join above, we have cut off discharge data from the year 1932 since we do not have meteorological data there.^. Let us analyze these data now. Please note that if you do not have the required package installed locally, you should install it prior to its use with the following command install.packages('simputation')↩︎ "],["Chap9-DataAnalysis.html", "6.4 Data Analysis", " 6.4 Data Analysis In this Section, the goal is to explore and understand the available time series data and their relationships and to take the necessary steps towards feature engineering. Features are predictors that we want to include in our forecasting models that are powerful in the sense that they help to improve the quality of forecasts in a significant manner. Sometimes, the modeler also wants to include synthetic features, i.e. predictors that are not observed but for example derived from observations. Different techniques are demonstrated that allow us to get familiar with the data that we are using. While we are interested to model discharge of Chatkal and Pskem rivers, it should be emphasized that all the techniques utilized for forecasting easily carry over to other rivers and settings. Let us start with a visualisation of the complete data record. Using timetk::plot_time_series and groups, we can plot all data into separate, individual facets as shown in Figure ??. data_long_tbl %&gt;% group_by(name) %&gt;% plot_time_series(date, value, .smooth = FALSE, .interactive = FALSE, .facet_ncol = 2, .title = &quot;&quot; ) Figure 6.8: Complete Data hydro-meteorological record for the zone of runoff formation in the Chirchik river basin. 6.4.1 Data Transformation It is interesting to observe that discharge values range over 2 - 3 orders of magnitude between minimum and maximum flow regimes. As can be seen in Figure 6.9, discharge and precipitation data are heavily skewed. When this is the case, it is generally advisable to consider data transformations as they help to improve predictive modeling accuracy of regression models. data_long_tbl %&gt;% group_by(name) %&gt;% ggplot(aes(x=value,colour = name)) + geom_histogram(bins=50) + facet_wrap(~name, scales = &quot;free&quot;) + theme(legend.position = &quot;none&quot;) Figure 6.9: Histograms of available raw data. Let us for example look at a very simple uniform non-parametric transformation, i.e. a logarithmic transformation (see Figure @ref(fig:histogramsData_transformed). As compared to parametric transformation, the logarithmic transformation is simple to apply for data greater than zero and does not require us to keep track of transformation parameters as, for example, is the case when we center and scale the data. data_wide_tbl %&gt;% mutate(across(Q16279:P38471,.fns = log1p)) %&gt;% # transforms discharge and precipitation time series pivot_longer(-date) %&gt;% ggplot(aes(x=value,colour = name)) + geom_histogram(bins=50) + facet_wrap(~name, scales = &quot;free&quot;) + theme(legend.position = &quot;none&quot;) (#fig:histogramsData_transformed)Histograms of transformed discharge and precipitation data together with the raw temperature data. Please note that with the base-R command log1p, 1 is added prior to the logarithmic transformation to avoid cases where the transformed values would not be defined, i.e. where discharge or precipitation is 0. More information about the log1p() function can be obtained by simply typing ?log1p. Recovering the original data after the log1p transformation is simply achieved by taking the exponent of the transformed data and subtracting 1 from the result. The corresponding R function is expm1(). Clearly, the log-transformed discharge values are no longer skewed (Figure @ref(fig:histogramsData_transformed)). We now see interesting bimodal distributions. At the same time, the variance of the transformed variables is greatly reduced. These are two properties that will help us construct a good model as we shall see below. Finally, the transformed discharge time series are shown in Figure @ref(). data_long_tbl %&gt;% filter(name==&#39;Q16279&#39; | name==&#39;Q16290&#39;) %&gt;% plot_time_series(date, log(value+1), .facet_vars = name, .smooth = FALSE, .interactive = FALSE, .title = &quot;&quot;, .y_lab = &quot;[-]&quot;, .x_lab = &quot;year&quot; ) (#fig:dischargeData_log1p)log1p() transformed discharge data. 6.4.2 Detecting Trends Lower frequency variability in time series, including trends, can be visualized by using the .smooth = TRUE option in the plot_time_series() function. To demonstrate this here, we have a closer look at the temperature data in our data record (Figure @ref(fig:T_trends)). data_long_tbl %&gt;% filter(name == &#39;T38462&#39; | name == &#39;T38471&#39;) %&gt;% plot_time_series(date, value, .smooth = TRUE, .facet_vars = name, .title = &quot;&quot;, .y_lab = &quot;deg. C.&quot;, .x_lab = &quot;year&quot; ) (#fig:T_trends)Temperature time series and trends. In both time series, a slight upward trend is visible that picks up over the most recent decades. We can look at these trends in greater detail, for example at monthly levels (Figure @ref(fig:T_monthly_trends)). data_long_tbl %&gt;% filter(name == &#39;T38462&#39;) %&gt;% summarise_by_time(.date_var = date, .by=&quot;month&quot;,value=mean(value)) %&gt;% tk_ts(frequency = 12) %&gt;% forecast::ggsubseriesplot(year.labels = FALSE) + geom_smooth(method = &quot;lm&quot;,color=&quot;red&quot;) + #ggtitle(&#39;Development of Monthly Mean Temperatures from 1933 - 2015 at Station 38462&#39;) + xlab(&#39;month&#39;) + ylab(&#39;Degrees Celsius&#39;) (#fig:T_monthly_trends)Sample development of Monthly Mean Temperatures from 1933 - 2015 at Station 38462. In the Figure above, a significant winter warming over the period of data availability is confirmed at Pskem meteorological station. As shown in Chapters 2 and ??, these trends are observed throughout the Central Asian region and are an indication of the changing climate there. We will have to take into account such type of trends in our modeling approach. 6.4.3 Auto- and Crosscorrelations A time series may have relationships to previous versions of itself - these are the ‘lags.’ The autocorrelation is a measure of the strength of this relationship of a series to its lags. The autocorrelation function ACF looks at all possible correlations between observation at different times and how they emerge. Contrary to that, the partial autocorrelation function PACF only looks at the correlation between a particular past observation and the current one. So in other words, ACF includes direct and indirect effects whereas PACF only includes the direct effects between observations at time t and the lag. As we shall see below, PACF is super powerful to identify relevant lagged timeseries predictors in autoregressive models (AR Models). Figure @ref(fig:Q_autocorr) shows the ACF and PACF over the interval of 72 lags (2 years). The AC function shows the highly seasonal characteristics of the underlying time series. It also shows the pronounced short-term memory in the basin, i.e. the tendency to observe subsequent values of high flows and subsequent values of low flow - hence the smoothness of the curve. This time of autocorrelation behavior is typical for basins with large surface storage in the form of lakes, swamps, snow and glaciers, permafrost and groundwater reserves (A. 2019). The Chatkal river basin certainly belongs to that category. data_long_tbl %&gt;% filter(name == &#39;Q16279&#39;) %&gt;% plot_acf_diagnostics(date, value, .show_white_noise_bars = TRUE, .lags = 72, .title = &quot;&quot; ) (#fig:Q_autocorr)Autocorrelation function (ACF) and partial autocorrelation function (PACF) are shown for the discharge time series at station 16279. But is there also autocorrelation of the annual time series? Let us test. Q16279_annual &lt;- data_long_tbl %&gt;% filter(name == &#39;Q16279&#39;) %&gt;% dplyr::select(-name) %&gt;% summarize_by_time(.date_var = date, .by=&quot;year&quot;, sum=sum(value)*3600*24*10/10^9) Q16279_annual %&gt;% plot_time_series(date,sum, .smooth = FALSE, .title = &quot;&quot;, .x_lab = &quot;year&quot;, .y_lab = &quot;Discharge [cubic km per year]&quot;) Figure 6.10: Testing autocorrelation at annual scales for discharge at station 16279. Q16279_annual %&gt;% plot_acf_diagnostics(.date_var = date, .value = sum, .lags = 50, .show_white_noise_bars = TRUE, .title = &quot;&quot;, .x_lab = &quot;year&quot;) Figure 6.10: Testing autocorrelation at annual scales for discharge at station 16279. The above Figure @(fig:annualAutoCorr) shows a fast decaying autocorrelation function for the annualized time series where even lag 1 values are no longer correlated in a significant manner. The PAC function, on the other hand, demonstrates that lag 1 is really critical in terms of direct effects (Figure @ref(fig:Q_autocorr)). After that, the PACF tapers off quickly. To utilize these findings in our modeling approach that uses lagged regression is important, as we shall see below. We can also study cross-correlations between two different time series. In other words, in cross-correlation analysis between two different time series, we estimate the correlation one variable and another, time-shifted variable. For example, we can cross-correlate discharge at Gauge 16279 (Chatkal river) to discharge at Gauge 16290 (Pskem River) as shown in Figure @ref(fig:crosscor_Q). As is easily visible, the discharge behavior of the two rivers is highly correlated. data_wide_tbl %&gt;% plot_acf_diagnostics(date,Q16279, .ccf_vars = Q16290, .show_ccf_vars_only = TRUE, .show_white_noise_bars = TRUE, .lags = 72, .title = &quot;&quot; ) (#fig:crosscor_Q)Cross-correlation analysis of the two discharge time series Q16279 and Q16290. Converse to this, discharge shows a lagged response to temperature which is clearly visible in the cross-correlation function. data_wide_tbl %&gt;% plot_acf_diagnostics(date,T38471, .ccf_vars = Q16279, .show_ccf_vars_only = TRUE, .show_white_noise_bars = TRUE, .lags = 72, .title = &quot;&quot; ) (#fig:ccf_TQ)Cross-correlation between temperature at station 38471 and the discharge at station 16279. A less pronounced cross-correlation exists between precipitation and discharge when measured at the same stations (Figure @ref(ccf_PQ)). data_wide_tbl %&gt;% plot_acf_diagnostics(date,P38471, .ccf_vars = Q16279, .show_ccf_vars_only = TRUE, .show_white_noise_bars = TRUE, .lags = 72, .title = &quot;&quot; ) (#fig:ccf_PQ)Cross-correlation between temperature at station 38471 and the discharge at station 16279. 6.4.4 Time Series Seasonality There is a pronounced seasonality in the discharge characteristics of Central Asian rivers. One of the key reason of this is the annual melt process of the winter snow pack. Figure @ref(seasonalityDiagnostic_Q) shows the seasonality of the log-transformed discharge. These observations can help in investigating and detecting time-based (calendar) features that have cyclic or trend effects. data_long_tbl %&gt;% filter(name==&quot;Q16279&quot; | name==&quot;Q16290&quot;) %&gt;% plot_seasonal_diagnostics(date, log(value+1), .facet_vars = name, .feature_set = c(&quot;week&quot;,&quot;month.lbl&quot;,&quot;year&quot;), .interactive = FALSE, .title = &quot;&quot; ) (#fig:seasonalDiagnostic_Q)Seasonal diagnostics of log1p discharge. Weekly (top row), monthly (middle row) and yearly diagnostics (bottom row) are shown for the two discharge time series. Figure @ref(seasonalDiagnostic_P) shows the seasonal diagnostic for the log-transfomred precpitation time series. The significant interannual variability is visible. In the annualized time series, no trend is available. data_long_tbl %&gt;% filter(name==&quot;P38462&quot; | name==&quot;P38464&quot; | name==&quot;P38471&quot;) %&gt;% plot_seasonal_diagnostics(date, log1p(value), .facet_vars = name, .feature_set = c(&quot;week&quot;,&quot;month.lbl&quot;,&quot;year&quot;), .interactive = FALSE, .title = &quot;&quot; ) (#fig:seasonalDiagnostic_P)Seasonal Diagnostics of precipitation Weekly (top row), monthly (middle row) and yearly diagnostics (bottom row) are shown for the available precipitation data in the zone of runoff formation of the two tributary rivers. Finally, Figure @ref() displays the seasonal diagnostics of the temperature time series. Notice that we use untransformed, raw values here for plotting. data_long_tbl %&gt;% filter(name==&quot;T38462&quot; | name==&quot;T38471&quot;) %&gt;% plot_seasonal_diagnostics(date, value, .facet_vars = name, .feature_set = c(&quot;week&quot;,&quot;month.lbl&quot;,&quot;year&quot;), .interactive = FALSE, .title = &quot;&quot;, ) (#fig:seasonalDiagnostic_T)Seasonal Diagnostics of temperature Weekly (top row), monthly (middle row) and yearly diagnostics (bottom row) are shown for the available temperature data in the zone of runoff formation of the two tributary rivers. "],["Chap9FeatureEngineering.html", "6.5 Investigating and Engineering Predictors", " 6.5 Investigating and Engineering Predictors All the data that we have available have been analyzed by now and we can now move to generating a good and solid understanding of the relevance of predictors for statistical modeling. To start with, we will keep things deliberately simple. Our approach is tailored to the particular local circumstances and the needs and wants of the hydrometeorological agencies that are using such types of model to issue high quality forecasts. First, the plan here to start with the introduction and discussion of the current forecasting techniques that are used operationally inside the Kyrgyz Hydrometeorological agency. These models and their forecast quality will serve as benchmark to beat any of the other models introduced here. At the same time, we will introduce a measure with which to judge forecast quality. Secondly, we evaluate the simplest linear models using time series regression. This will also help to introduce and explain key concepts that will be discussed in the third and final section below. Finally, we show the application of more advanced forecasting modeling techniques that use state-of-the-art regression type algorithms. The forecasting techniques will be demonstrated by focussing on the Pskem river. The techniques extend to other rivers in the region and beyond in a straight forward manner. 6.5.1 Benchmark: Current Operational Forecasting Models in the Hydrometeorological Agencies 6.5.2 Time Series Regression Models The simplest linear regression model can be written as \\[ y_{t} = \\beta_{0} + \\beta_{1} x_{t} + \\epsilon_{t} \\] where the coefficient \\(\\beta_{0}\\) is the intercept term, \\(\\beta_{1}\\) is the slope and \\(\\epsilon_{t}\\) is the error term. The subscripts \\(t\\) denote the time dependency of the target and the explanatory variables and the error. \\(y_{t}\\) is our target variable, i.e. discharge in our case, that we want to forecast. At the same time, \\(x_{t}\\) is an explanatory variable that is already observed at time \\(t\\) and that we can use for prediction. As we shall see below, we are not limited to the inclusion of only one explanatory variable but can think of adding multiple variables that we suspect to help improve forecast modeling performance. To demonstrate the effects of different explanatory variables on our forcasting target and the quality of our model for forecasting discharge at stations 16290, the function plot_time_series_regression from the timetk package is used. First, we we only want to specify a model with a trend over the time \\(t\\). Hence, we fit the model \\[ y_{t} = \\beta_{0} + \\beta_{1} t + \\epsilon_{t} \\] model_formula &lt;- as.formula(log1p(Q16290) ~ as.numeric(date) ) model_data &lt;- data_wide_tbl %&gt;% dplyr::select(date,Q16290) model_data %&gt;% plot_time_series_regression( .date_var = date, .formula = model_formula, .show_summary = TRUE, .title = &quot;&quot; ) ## ## Call: ## stats::lm(formula = .formula, data = .data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.3940 -0.7068 -0.2114 0.7193 2.0274 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.056e+00 1.489e-02 272.39 &lt;2e-16 *** ## as.numeric(date) -2.997e-06 1.675e-06 -1.79 0.0736 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7998 on 2983 degrees of freedom ## Multiple R-squared: 0.001073, Adjusted R-squared: 0.0007378 ## F-statistic: 3.203 on 1 and 2983 DF, p-value: 0.07359 Figure 6.11: Linear regression trend model. Note that the timetk::plot_time_series function is a convenience wrapper to make our lives easy in terms of modeling and immediately getting a resulting plot. The same model could be specified in the traditional R-way, i.e. as follows model_data %&gt;% lm(formula = model_formula) %&gt;% summary() ## ## Call: ## lm(formula = model_formula, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.3940 -0.7068 -0.2114 0.7193 2.0274 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.056e+00 1.489e-02 272.39 &lt;2e-16 *** ## as.numeric(date) -2.997e-06 1.675e-06 -1.79 0.0736 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7998 on 2983 degrees of freedom ## Multiple R-squared: 0.001073, Adjusted R-squared: 0.0007378 ## F-statistic: 3.203 on 1 and 2983 DF, p-value: 0.07359 The adjusted R-squared shows the medicore performance of our simple model as it cannot capture any of the seasonal variability. Furthermore we see that the trend coefficient is negative which indicates a decrease in mean discharge. However, as the p-value confirms, the trend is only significant at the 0.1 level. The first step in improving our model is to account for seasonality. In the case of decadal time series, we can add categorical variables (as factor variables) decoding the corresponding decades. Similarly, in the case of monthly data, we could use month names or factors 1..12 to achieve the same. The same reasoning extends to other periods (quarters, weekdays, etc.). We will use a quarterly model to explain the concept since the inclusion of 4 indicator variables for the individual quarters is easier to grasp than to work with 36 decadal indicators. # Computing quarterly mean discharge values q16290_quarter_tbl &lt;- model_data %&gt;% summarize_by_time(date,value=mean(log1p(Q16290)),.by = &quot;quarter&quot;) # adding quarters identifier q16290_quarter_tbl &lt;- q16290_quarter_tbl %&gt;% mutate(per = quarter(date) %&gt;% as.factor()) model_formula &lt;- as.formula(value ~ as.numeric(date) + per ) q16290_quarter_tbl %&gt;% plot_time_series_regression(date, .formula = model_formula, .show_summary = TRUE, .title = &quot;&quot; ) ## ## Call: ## stats::lm(formula = .formula, data = .data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.51477 -0.14936 0.00228 0.14410 0.66161 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.255e+00 2.204e-02 147.691 &lt; 2e-16 *** ## as.numeric(date) -3.158e-06 1.255e-06 -2.516 0.0123 * ## per2 1.551e+00 3.106e-02 49.919 &lt; 2e-16 *** ## per3 1.396e+00 3.107e-02 44.938 &lt; 2e-16 *** ## per4 2.562e-01 3.107e-02 8.248 3.98e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2001 on 327 degrees of freedom ## Multiple R-squared: 0.9217, Adjusted R-squared: 0.9207 ## F-statistic: 962.4 on 4 and 327 DF, p-value: &lt; 2.2e-16 (#fig:trend_season_model)Example quarterly linear model with trend and seasonality. What we did here is to compare a continuous variable, i.e. the discharge, across 4 categories. Hence, we can write down the model in the following way: \\[ y_{t} = \\beta_{0} + \\beta_{1} \\delta_{t}^{Qtr2} + \\beta_{2} \\delta_{t}^{Qtr3} + \\beta_{3} \\delta_{t}^{Qtr4} + \\epsilon_{t} \\] Using ‘one hot encoding,’ we include only N-1 (here, 3) variables out of the N (here,4) in the regression because we can safely assume that if we are in Quarter 4, all the other indicator variables are simply 0. If we are in quarter 1 (Qtr1), the model would just be \\[ y_{t} = \\beta_{0} + \\epsilon_{t} \\] If we are in Quarter 2 (Qtr2), the model would be \\[ y_{t} = \\beta_{0} + \\beta_{1} + \\epsilon_{t} \\] since \\(\\delta_{t}^{Qtr2} = 1\\). Hence, whereas \\(\\beta_{0}\\) is to be interpreted as the estimated mean discharge in Quarter 1 (called (Intercept) in the results table below), \\(\\beta_{1}\\) (called qtr2 in the results table below) is the estimated difference of mean discharge between the two categories/quarters We can get the values and confidence intervals of the estimates easily in the following way lm_quarterlyModel &lt;- q16290_quarter_tbl %&gt;% lm(formula = model_formula) meanQtrEstimates &lt;- lm_quarterlyModel %&gt;% coefficients() meanQtrEstimates %&gt;% expm1() ## (Intercept) as.numeric(date) per2 per3 ## 2.493125e+01 -3.158103e-06 3.714894e+00 3.039112e+00 ## per4 ## 2.920533e-01 lm_quarterlyModel %&gt;% confint() %&gt;% expm1() ## 2.5 % 97.5 % ## (Intercept) 2.383084e+01 2.608043e+01 ## as.numeric(date) -5.627148e-06 -6.890525e-07 ## per2 3.435387e+00 4.012016e+00 ## per3 2.799662e+00 3.293653e+00 ## per4 2.154539e-01 3.734800e-01 The same reasoning holds true for the model with decadal observations to which we return now again. First, we add decades as factors to our data_wide_tbl. Now, we can specify and calculate the new model. model_formula &lt;- as.formula(log1p(Q16290) ~ as.numeric(date) + # trend components per # seasonality (as.factor) ) model_data &lt;- data_wide_tbl %&gt;% dplyr::select(date,Q16290,per) model_data %&gt;% plot_time_series_regression( .date_var = date, .formula = model_formula, .show_summary = TRUE, .title = &quot;&quot; ) ## ## Call: ## stats::lm(formula = .formula, data = .data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.3225 -0.6972 -0.2318 0.7268 2.0364 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.943e+00 2.991e-02 131.827 &lt; 2e-16 *** ## as.numeric(date) -3.065e-06 1.670e-06 -1.836 0.0665 . ## per 6.150e-03 1.406e-03 4.374 1.26e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7974 on 2982 degrees of freedom ## Multiple R-squared: 0.00744, Adjusted R-squared: 0.006774 ## F-statistic: 11.18 on 2 and 2982 DF, p-value: 1.46e-05 (#fig:dec_trend_season_model)Decadal linear regression model with trend and seasonality. What we see is that through the inclusion of the categorical decade variables, we have greatly improved our modeling results since we can now capture the seasonality very well (Tip: zoom into the time series to compare highs and lows and their timing for the target variable and its forecast). However, despite the excellent adjusted R-squared value of 0.9117, our model is far from perfect as it is not able to account for inter-annual variability in any way. Let us quickly glance at the errors. lm_decadalModel &lt;- model_data %&gt;% lm(formula = model_formula) obs_pred_wide_tbl &lt;- model_data%&gt;% mutate(pred_Q16290 = predict(lm_decadalModel) %&gt;% expm1()) %&gt;% mutate(error = Q16290 - pred_Q16290) ggplot(obs_pred_wide_tbl, aes(x = Q16290, y = pred_Q16290, colour = per )) + geom_point() + geom_abline(intercept = 0, slope = 1) (#fig:scatterplot_Obs_vs_Fcst)Scatterplot of observed versus calculated values. We do not seem to make a systematic error as also confirmed by inspecting the histogram or errors (they are nicely centered around 0). ggplot(obs_pred_wide_tbl,aes(x=error)) + geom_histogram(bins=100) In Section @ref(Chap9_Autocorrelations) above, we saw that the PAC function is very high at lag 1. We exploit this fact be incorporating in the regression equation the observed previous discharge, i.e. \\(y_{t-1}\\) at time \\(t-1\\) to predict discharge at time \\(t\\). Hence, our regression can be written as \\[ y_{t} = \\beta_{0} + \\beta_{1} t + \\beta_{2} y_{t-1} + \\sum_{j=2}^{36} \\beta_{j} \\delta_{t}^{j} + \\epsilon_{t} \\] where the \\(\\delta_t^{j}\\) correspondingly are the 35 indicator variables as discussed above in the case of quarterly time series where we had 3 of these variables included. Before we can estimate this model, we prepare a tibble with the relevant data as shown in the table below (note that we simply renamed the discharge column to Q out of convenience). model_data &lt;- data_wide_tbl %&gt;% dplyr::select(date,Q16290,per) %&gt;% rename(Q=Q16290) %&gt;% mutate(Q = log1p(Q)) %&gt;% mutate(Q_lag1 = lag(Q,1)) model_data ## # A tibble: 2,985 x 4 ## date Q per Q_lag1 ## &lt;date&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1933-01-10 3.37 1 NA ## 2 1933-01-20 3.23 2 3.37 ## 3 1933-01-31 3.17 3 3.23 ## 4 1933-02-10 3.21 4 3.17 ## 5 1933-02-20 3.26 5 3.21 ## 6 1933-02-28 3.28 6 3.26 ## 7 1933-03-10 3.30 7 3.28 ## 8 1933-03-20 3.53 8 3.30 ## 9 1933-03-31 3.57 9 3.53 ## 10 1933-04-10 3.92 10 3.57 ## # … with 2,975 more rows Notice that to accommodate the \\(y_{t-1}\\) in the data, we simply add a column that contains a lagged version of the discharge time series itself (see column Q_lag1). Now, for example, for our regression we have a first complete set of data points on \\(t = &#39;1933-01-20&#39;\\), with \\(Q=3.226844\\), \\(dec=2\\) and \\(Q_{lag1}=3.374169\\). Notice how the last value corresponds to the previously observed and now known \\(y_{t-1}\\). # Specification of the model formula model_formula &lt;- as.formula(Q ~ as.numeric(date) + per + Q_lag1) # Note that we use na.omit() to delete incomplete data records, ie. the first observation where we lack the lagged value of the discharge. model_data %&gt;% na.omit() %&gt;% lm(formula = model_formula) %&gt;% summary() ## ## Call: ## lm(formula = model_formula, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.09294 -0.12965 -0.03140 0.07656 1.17980 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.955e-01 1.801e-02 10.850 &lt;2e-16 *** ## as.numeric(date) -6.402e-08 3.925e-07 -0.163 0.87 ## per -7.010e-03 3.355e-04 -20.897 &lt;2e-16 *** ## Q_lag1 9.838e-01 4.353e-03 225.992 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1873 on 2980 degrees of freedom ## Multiple R-squared: 0.9453, Adjusted R-squared: 0.9452 ## F-statistic: 1.716e+04 on 3 and 2980 DF, p-value: &lt; 2.2e-16 It looks like we have made a decisive step in the right direction by incorporating the previously observed discharge value. Also, notice that some of the decade factors have lost their statistical significance meaning that the seasonality can now be captured in part also by the lagged version of the time series. Let us visualize the results quickly (tip: also zoom in to explore the fit). model_data %&gt;% na.omit() %&gt;% plot_time_series_regression( .date_var = date, .formula = model_formula, .show_summary = FALSE, # We do show the summary since we have plotted the summary output already above. .title = &quot;&quot; ) (#fig:lag_trend_season_Model)Linear regression model results with trend, seasonality and lag 1 predictors. This is clearly an astonishing result. Nevertheless, we should keep a couple of things in mind: What about the rate of change of the discharge and the acceleration of discharge? Would the incorporation of these features help to improve the model? We have not assess the quality of the forecasts using the stringent quality criteria as they exist in the Central Asian Hydrometeorological Services. How does our forecast perform under this criteria? Does the incorporation of precipitation and temperature data help to improve our forecast skills? We did not test our model on out-of-sample data. Maybe our model does not generalize well? We will discuss these and related issues soon when using more advanced models but for the time being declare this a benchmark model due to its simplicity and predictive power. We will work on these questions now and focus first on the incorporation of the rate of change in discharge and the acceleration of discharge over time. First, we add \\(Q_{lag2}\\) to our model data and then compute change and acceleration accordingly. model_data &lt;- model_data %&gt;% mutate(Q_lag2 = lag(Q,2)) %&gt;% mutate(change = Q_lag1 -Q_lag2) %&gt;% # that is the speed of change in discharge mutate(change_lag1 = lag(change,1)) %&gt;% mutate(acc = change - change_lag1) %&gt;% na.omit() # and that is the acceleration of discharge model_data ## # A tibble: 2,982 x 8 ## date Q per Q_lag1 Q_lag2 change change_lag1 acc ## &lt;date&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1933-02-10 3.21 4 3.17 3.23 -0.0614 -0.147 0.0860 ## 2 1933-02-20 3.26 5 3.21 3.17 0.0413 -0.0614 0.103 ## 3 1933-02-28 3.28 6 3.26 3.21 0.0551 0.0413 0.0138 ## 4 1933-03-10 3.30 7 3.28 3.26 0.0152 0.0551 -0.0399 ## 5 1933-03-20 3.53 8 3.30 3.28 0.0187 0.0152 0.00348 ## 6 1933-03-31 3.57 9 3.53 3.30 0.231 0.0187 0.212 ## 7 1933-04-10 3.92 10 3.57 3.53 0.0432 0.231 -0.187 ## 8 1933-04-20 4.05 11 3.92 3.57 0.348 0.0432 0.305 ## 9 1933-04-30 4.47 12 4.05 3.92 0.132 0.348 -0.216 ## 10 1933-05-10 4.88 13 4.47 4.05 0.416 0.132 0.284 ## # … with 2,972 more rows # Specification of the model formula model_formula &lt;- as.formula(Q ~ as.numeric(date) + per + Q_lag1 + change + acc) model_data %&gt;% na.omit() %&gt;% plot_time_series_regression( .date_var = date, .formula = model_formula, .show_summary = TRUE, # We do show the summary since we have plotted the summary output already above. .title = &quot;&quot; ) ## ## Call: ## stats::lm(formula = .formula, data = .data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.01892 -0.09223 -0.02294 0.05501 1.16531 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.574e-01 1.626e-02 15.830 &lt;2e-16 *** ## as.numeric(date) -1.834e-07 3.480e-07 -0.527 0.598 ## per -2.859e-03 3.318e-04 -8.616 &lt;2e-16 *** ## Q_lag1 9.496e-01 4.087e-03 232.331 &lt;2e-16 *** ## change 5.650e-01 2.002e-02 28.221 &lt;2e-16 *** ## acc -2.215e-01 1.806e-02 -12.265 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1658 on 2976 degrees of freedom ## Multiple R-squared: 0.9571, Adjusted R-squared: 0.957 ## F-statistic: 1.328e+04 on 5 and 2976 DF, p-value: &lt; 2.2e-16 model &lt;- model_data %&gt;% lm(formula=model_formula) model %&gt;% summary() ## ## Call: ## lm(formula = model_formula, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.01892 -0.09223 -0.02294 0.05501 1.16531 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.574e-01 1.626e-02 15.830 &lt;2e-16 *** ## as.numeric(date) -1.834e-07 3.480e-07 -0.527 0.598 ## per -2.859e-03 3.318e-04 -8.616 &lt;2e-16 *** ## Q_lag1 9.496e-01 4.087e-03 232.331 &lt;2e-16 *** ## change 5.650e-01 2.002e-02 28.221 &lt;2e-16 *** ## acc -2.215e-01 1.806e-02 -12.265 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1658 on 2976 degrees of freedom ## Multiple R-squared: 0.9571, Adjusted R-squared: 0.957 ## F-statistic: 1.328e+04 on 5 and 2976 DF, p-value: &lt; 2.2e-16 # Here, we add the the prediction to our tibble so that we can assess model predictive quality later. model_fc_wide_tbl &lt;- model_data %&gt;% mutate(pred = predict(model)) %&gt;% mutate(obs = expm1(Q), pred = expm1(pred)) %&gt;% dplyr::select(date,obs,pred,per) model_fc_wide_tbl ## # A tibble: 2,982 x 4 ## date obs pred per ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1933-02-10 23.7 23.6 4 ## 2 1933-02-20 25.1 25.9 5 ## 3 1933-02-28 25.5 28.0 6 ## 4 1933-03-10 26 28.1 7 ## 5 1933-03-20 33. 28.3 8 ## 6 1933-03-31 34.5 38.1 9 ## 7 1933-04-10 49.3 38.9 10 ## 8 1933-04-20 56.4 58.0 11 ## 9 1933-04-30 86 65.3 12 ## 10 1933-05-10 130. 102. 13 ## # … with 2,972 more rows Another, albeit small improvement in the forecast of predicting discharge 1-step ahead! It is now time to properly gauge the quality of this seemingly excellent model. Does it conform to local quality standards that apply to decadal forecasts? The Figure @ref(benchmarkModel_obs_pred_comparison) shows the un-transformed data. We see that we are not doing so well during the summer peak flows. As we shall see further below, these are the notoriously hard to predict values, even just for 1-step ahead decadal predictions. model_fc_wide_tbl %&gt;% dplyr::select(-per) %&gt;% pivot_longer(-date) %&gt;% plot_time_series(date, value, name, .smooth = F, .title = &quot;&quot;) 6.5.3 Assessing the Quality of Forecasts How well are we doing with our simple linear model? Let us assess the model quality using the local practices. For the Central Asian Hydromets, a forecast at a particular decade \\(d\\) is considered to be excellent if the following holds true \\[ |Q_{obs}(d,y) - Q_{pred}(d,y)| \\le 0.674 \\cdot \\sigma[\\Delta Q(d)] \\] where \\(Q_{obs}(d,y)\\) is the observed discharge at decade \\(d\\) and year \\(d\\), \\(Q_{pred}(d,y)\\) is the predicted discharge at decade \\(d\\) and year \\(y\\), \\(|Q_{obs}(d) - Q_{pred}(d)|\\) thus the absolute error and \\(\\sigma[\\Delta Q(d)] = \\sigma[Q(d) - Q(d-1)]\\) is the standard deviation of the difference of decadal observations at decade \\(d\\) and \\(d-1\\) over the entire observation record (hence, the year indicator \\(y\\) is omitted there). The equation above can be reformulated to \\[ \\frac{|Q_{obs}(d,y) - Q_{pred}(d,y)|}{\\sigma[\\Delta Q(d)]} \\le 0.674 \\] So let us assess the forecast performance over the entire record using the `riversCentralAsia::assess_fc_qual`` function. Note that the function returns a list of three objects. First, it returns a tibble of the number of forecasts that are of acceptable quality for the corresponding period (i.e. decade or month) as a percentage of the total number of observations that are available for that particular period. Second, it returns the period-averaged mean and third a figure that shows forecast quality in two panels. So, for our model which we consiedered to be performing well above, we get the following performance specs plot01 &lt;- TRUE te &lt;- assess_fc_qual(model_fc_wide_tbl,plot01) ## Registered S3 method overwritten by &#39;GGally&#39;: ## method from ## +.gg ggplot2 te[[3]] In other words, roughly two thirds of our in-sample forecasts comply will be considered good enough when measured according to the quality criterion. Furthermore, the model performs worse than average during the second quarter (Q2) decades, i.e. from decade 10 through 17. This is an indication that providing good forecasts in Q2 might be hard. It is, however, generally not considered to be good practice to assess model quality on in-sample data. Rather, model performance should be assessed on out-of-sample data that was not used for model training and is thus data that is entirely unseen. 6.5.4 Generating and Assessing Out-of-Sample Forecasts We start off with our model_data tibble and want to divide it into two sets, one for model training and one for model testing. We refer to these sets as training set and test set. For the creation of these sets, we can use the timetk::time_series_split() function. 6.5.5 Machine Learning Models "],["save-data-for-hot-start.html", "6.6 Save Data for Hot Start", " 6.6 Save Data for Hot Start "],["Operationalization.html", "Chapter 7 Operationalization of Models - Opportunities and Challenges", " Chapter 7 Operationalization of Models - Opportunities and Challenges Caution! - Work in progress. "],["co-design-phase.html", "7.1 Co-design Phase", " 7.1 Co-design Phase "],["modeling-phase.html", "7.2 Modeling Phase", " 7.2 Modeling Phase "],["testing-phase.html", "7.3 Testing Phase", " 7.3 Testing Phase "],["operational-deployment.html", "7.4 Operational Deployment", " 7.4 Operational Deployment "],["ChapterAppendix.html", "Chapter 8 (APPENDIX) Appendix ", " Chapter 8 (APPENDIX) Appendix "],["appendix-a-open-source-resources.html", "8.1 Appendix A: Open-source resources", " 8.1 Appendix A: Open-source resources 8.1.1 Literature Many authors of scientific literature are on the web platform researchgate where they can privately share their work with students (users need to register for an account). 8.1.2 QGIS QGIS is a free and open source Geographical Information System that offers very similar tools as their commercial counterparts. The latest version of QGIS can be downloaded from the QGIS website. We recommend to install the stable long-term support version. 8.1.2.1 Resources for learning QGIS A general tutorial for beginners is the QGIS training manual. It includes a short chapter on the use of QGIS for hydrological analysis (Chapter 17.16). For this course you should be familiar with the QGIS window and know the difference between raster and vector data. If you have used QGIS or a similar GIS software before you will not need to do a tutorial prior to this course. 8.1.3 R and RStudio R is a free and open source statistical programming language. It’s large user community ensure active development and up-to-date help resources available on the internet. RStudio is a free user interface for R. To install R and RStudio follow the installation guide on ModernDive - Statistical Inference via Data Science 8.1.3.1 Resources for learning R and R studio “Help! I’m new to R and RStudio and I need to learn them! What do I do?” If you’re asking yourself this, this book is for you: ModernDive - Statistical Inference via Data Science. A thorough guide for data science in R: R for Data Science 8.1.4 RS Minerve 8.1.4.1 How to download and install RS Minerve Go to the software download page of CREALP’s website https://www.crealp.ch/fr/accueil/outils-services/logiciels/rs-minerve/telechargement-rsm.html (last accessed March 18, 2021) and click on Version actuelle to download the latest installer for Windows (see Figure 8.1). This will start the download process for the installer RSMinerve-install.exe. Figure 8.1: Download RS Minerve from the CREALP website https://www.crealp.ch/fr/accueil/outils-services/logiciels/rs-minerve/telechargement-rsm.html (last accessed March 18, 2021). You should also download the user manual (RS MINERVE user manual, written in English) and the example files used for the tutorials in the user manual (Exemple de fichiers, a zip file with data) as well as the technical manual (RS MINERVE technical manual, written in English). Once the installer is downloaded, install RS Minerve with a double-click on the installer and follow the Setup guide. Open RS Minerve once you have it installed. Back to the prerequisites for RS Minerve modelling "],["appendix-b-quick-guides.html", "8.2 Appendix B: Quick Guides", " 8.2 Appendix B: Quick Guides 8.2.1 QGIS: 8.2.2 RSM: Create HBV model and edit parameters Click on the HBV model icon in the model selection panel (Figure 8.2). Figure 8.2: To generate an HBV model, click on the HBV model icon in the model selection panel. Move your cursor to the white area in the center and create a HBV model with a click. The HBV model icon will now appear in the white model panel (Figure 8.3). Figure 8.3: A HBV model has been generated. Edit the parameters of the model by double-clicking on the model icon and changing parameter values in the table that appears to the right of the RS Minerve window (Figure 8.4). Figure 8.4: Double-click on the HBV model to open and edit the parameter table. Alternatively (especially if you have to change parameters for several models), activate the Parameters panel in the Model Properties toolbar (Figure 8.5). Figure 8.5: Activate the Parameters panel in the Model Properties toolbar. Select the HBV model in the Parameters panel (Figure 8.6). Figure 8.6: Select HBV in the Parameters panel. If you have several HBV models, you can select models by zone and apply edits in the parameter table in the left of the Parameters panel to all selected models (mark with tick). You can also edit paramers for individual models in the right-hand table of the Parameters panel (Figure 8.7). Figure 8.7: Edit parameters for groups of models (left parameter table) or for individual models (right parameter table). Save your model by clicking the floppy disk icon in the toolbar. You can also export parameters to a text file via the button Export P in the Model Properties toolbar, edit the text file and import the edited parameter file through Import P. The area of the HBV model should be 99’000’000 m2. Back to the Nauvalisoy model guide. 8.2.3 RSM: Add and link climate station {#section-quickguides-add-climate-station)} Move your cursor to the V-Station icon in the models panel (the first icon in Figure 8.2). Click on the icon to activate it and click next to the HBV model in your model pane to place the V-Station. With a double-click on the newly created V-Station, you can visualize the parameters of the virtual weather station. Connect the station to the HBV model by activating the Connections mode in the Editing tools toolbar (Figure 8.8). Figure 8.8: Create a virtual weather station. Click on the station, hold down the finger move your cursor to the HBV model, then release the hold. A grey line appeart between the station and the model and a pop-up window asks you to verify the data links between the two components (Figure 8.9). Figure 8.9: Link the virtual weather station to the HBV model. Click Ok to accept the suggested data links and a blue arrow will appear between the weather station and the model. Click on the black arrow in the Editing tools toolbar to leave the Connections model. Back to the Nauvalisoy model guide. 8.2.4 RSM: Import climate data Go to the database tab and click on Open in the File database toolbar (Figure 8.10). Figure 8.10: The database tab. Select the file./data/SyrDarya/Chirchiq/RSMinerve/ERA5_Nauvalisoy_1981_2013.csv. If you don’t see the file in your file browser, make sure the file ending .csv is selected in the file browser (Figure 8.11). Figure 8.11: Make sure the file ending is .csv in the file browser. Press open to load the data into RSMinerve. Depending on your computer this may take a few seconds. Once the data is loaded, click through the database browser in the left pane to explore the data (Figure 8.12). Figure 8.12: Explore the data base. To connect the data base to the model you will need to make the data consistent with the model. To do this change the name “New group” to “Measurements” and select Input for the category (Figure 8.13). Figure 8.13: Change “New group” to “Measurements” and select Input as category. Browse the name of the station (Figure 8.14). Figure 8.14: The name of the station is “Nauvalisoy.” As our weather data is representative for the entire Nauvalisoy catchment, the station name in the data base needs to be consistent with the station name of the V-Station in the model pane. Change the name of the station in the model pane from V-Station to Nauvalisoy by clicking on the name. Choose the nauvalisoy data set as source and adapt the simulation period as shown in Figure 8.15. Figure 8.15: Choose the nauvalisoy data set to link the weather station data to the virtual weather station. The simulation times should not extend the period of the input data. Simulation time step is 1 hour and the recording time step is 1 month. Click on Validation to verify that the model has been set up correctly. Once you have adapted the model settings to calculate evaporation based on temperature (how to), no errors should be reported. You can now run the model. A warning tells you that the number of meteo stations is not sufficient. Ignore the warning for now. Nauvalisoy model guide 8.2.5 RSM: Adapt model settings Navigate to Edit in the Settings toolbar (Figure 8.16). Figure 8.16: Open the models settings tab. Open the Settings tab and choose an ET model. Adapt the coordinates of the project (choose the station coordinates mentioned in the introduction section) (Figure 8.17). Figure 8.17: Edit the evaporation calculation method and the coordinates of the project. Back to the model guide 8.2.6 RSM: Export simulation results to data base Go to Export in the Database toolbar (Figure 8.18). Define a name for the simulation results and choose a data base group to save the data to. Create a new group if you haven’t already done so. Figure 8.18: Export simulation results to the data base. The data sets are now available in the data base tab and can be visualized in the Selection and plots tab. Back to the model guide 8.2.7 RSM: Add comparator and load discharge measurements The comparator object allows the user to compare a simulated variable to a reference. In our case, we want to compare the simulated discharge of the Nauvalisoy river to the measured one. The measured discharge can be imported to RS Minerve via the database tab. Move your cursor to the comparator icon (Figure 8.19) in the model components panel. Figure 8.19: Comparator icon in RS Minerve. Activate it with a right-click and move your cursor next to the HBV model in the model pane. Place the comparator object with another click. Add a source object to your model following the same procedure as for the comparator object. Figure 8.20 shows the icon of the source object. Figure 8.20: Comparator icon in RS Minerve. You can now optionally rename your model objects. Connect the source and the HBV model to the comparator by activating the Connections mode in the Editing Tools toolbar. Right-click on the HBV model, hold the click and drag the cursor to the comparator object where you release the cursor. You will be asked in a pop-up window to specify the flow your wish to compare and if it is to be viewed as a simulation result or the reference. Connect the total discharge computed by the HBV model component as simulation result to the comparator (Figure 8.21) and close the pop-up window by pressing OK. Figure 8.21: Comparator icon in RS Minerve. Connect the source object to the comparator in the same way as the HBV object. The source will be connected as reference (Figure 8.22). Figure 8.22: Source icon in RS Minerve. Next, we have to load the (synthetic) discharge data into the database. Navigate to the database tab. In the database, go to Measurements -&gt; nauvalisoy and click Add and enter a name for the discharge station (Figure 8.23). For this example, we do not need to bother with the coordinates. Figure 8.23: Connect the outflow of the HBV model as simulation result to the comparator. Under the new discharge station, add a sensor and rename it to the source object in your model (Figure 8.24). Figure 8.24: Connect the discharge measurements (source) as reference to the comparator. Open the tab with the Values. Here we need to import the discharge data. You can do that by opening the discharge data in a spreadsheet (e.g. Excel) and copy-pasting the dates and discharge values into the Values table in RS Minerve (Here is how to do this step-by-step. Now we need to link the discharge data in the database to the source object in the model. Navigate to the model and Figure 8.25: Select the data source for the source object (under Data Source in the left window pane) and select the sensor for discharge data for the source (under Source, Series identifier in the right window pane). Validate the model to see if the model setup went correctly. Run the model if the validation did not throw an error. Back to the practical model calibration and validation section 8.2.8 RSM: Copy-paste data to database Open the file ./data/SyrDarya/Chirchiq/RSMinerve/synthetic_discharge_Nauvalisoy_river_for_calibration_exercise.csv in a spreadsheet software, e.g. Excel, Numbers, Google Sheets or OpenOffice Calc and select the rows containing dates and values (Figure 8.26). Press control C or perform a left-click and choose copy. Figure 8.26: Select the rows and columns containing the dates and data to be copied. Press control C to copy the selected cells. Then navigate to the discharge sensor on the database tab in RS Minerve where you want to add the data to. Click in the small square to the left of the first row in the Values tab and enter control P. Alternatively perform a right-click in the small square and select Paste. The dates and values from the excel sheet will now appear in the table (Figure 8.27). Save the database. Figure 8.27: Paste dates and values to the database table. Back to the practical model calibration and validation section "],["appendix-c-processing-climate-data.html", "8.3 Appendix C: Processing Climate Data", " 8.3 Appendix C: Processing Climate Data 8.3.1 Cutting CHELSA v.1.2.1 to CA Domain The CHELSA v1.2.1 data is a global dataset. The data can be cut to the Central Asia domain in the following way. dir_CHELSA &lt;- &#39;/Users/tobiassiegfried/Dropbox (hydrosolutions)/1_HSOL_PROJECTS/PROJECTS/SDC/DKU_WRM_COURSE_CA/CourseMaterials/Data/CLIMATE/CHELSA_V1_2_1/&#39; # First, we process precipitation data (prec). prec_CHELSA_files &lt;- list.files(paste0(dir_CHELSA,&#39;tp/&#39;)) for (idx in 1:length(prec_CHELSA_files)){ str2Print &lt;- paste0(&#39;Processing PREC: &#39;, idx, &#39; out of &#39;, length(prec_CHELSA_files) ) print(str2Print) fName &lt;- prec_CHELSA_files[idx] global_CHELSA_raster &lt;- raster(paste0(dir_CHELSA,&#39;prec/&#39;,fName)) centralAsia_CHELSA_raster &lt;- raster::crop(global_CHELSA_raster,aoi_CentralAsia_LatLon) # Save the centralAsia_CHELSA_raster fName2Save &lt;- paste0(substr(fName,1,19),&#39;_CA&#39;,substr(fName,20,30)) raster::writeRaster(centralAsia_CHELSA_raster, paste0(dir_CHELSA,&#39;prec/&#39;,fName2Save), &#39;GTiff&#39;, overwrite = TRUE) } # Second, we process mean temperature data (tmean). tmean_CHELSA_files &lt;- list.files(paste0(dir_CHELSA,&#39;t2m/&#39;)) for (idx in 1:length(tmean_CHELSA_files)){ str2Print &lt;- paste0(&#39;Processing TMEAN: &#39;, idx, &#39; out of &#39;, length(tmean_CHELSA_files) ) print(str2Print) fName &lt;- tmean_CHELSA_files[idx] global_CHELSA_raster &lt;- raster(paste0(dir_CHELSA,&#39;tmean/&#39;,fName)) centralAsia_CHELSA_raster &lt;- raster::crop(global_CHELSA_raster,aoi_CentralAsia_LatLon) # Save the centralAsia_CHELSA_raster fName2Save &lt;- paste0(substr(fName,1,20),&#39;_CA&#39;,substr(fName,21,35)) raster::writeRaster(centralAsia_CHELSA_raster, paste0(dir_CHELSA,&#39;tmean/&#39;,fName2Save), &#39;GTiff&#39;, overwrite = TRUE) } (raster(paste0(dir_CHELSA,&#39;tmean/&#39;,fName2Save))/10 - 273.15) %&gt;% plot() 8.3.2 Bias Correcting CHELSA v1.2.1 Precipitation Data for Snow Undercatch ## general info - function arguments basinName &lt;- &#39;Gunt&#39; dataType_ERA5 &lt;- &#39;tp&#39; #basinShape &lt;- &#39;to be passed in&#39; targetCRS &lt;- &quot;+init=epsg:32642&quot; ## Directories of relevant climate files - function arguments dir_CHELSA &lt;- &#39;/Users/tobiassiegfried/Dropbox (hydrosolutions)/1_HSOL_PROJECTS/PROJECTS/SDC/DKU_WRM_COURSE_CA/CourseMaterials/Data/CLIMATE/CHELSA_V1_2_1/&#39; # Basin AOI - function arguments, but check if all can be derived from the basinShape that is anyhow to be passed in! aoi_Basin_LatLon &lt;- gunt_Shapefile_LatLon %&gt;% extent # GUNT # === fileCorrFact &lt;- &#39;/Users/tobiassiegfried/Dropbox (hydrosolutions)/1_HSOL_PROJECTS/PROJECTS/SDC/DKU_WRM_COURSE_CA/CourseMaterials/Data/CLIMATE/PBCOR_V1/CHELSA_V12.nc&#39; # Get CHELSA file list - Note, we already have cut to the CA domain! fileList &lt;- list.files(paste0(dir_CHELSA,dataType_ERA5)) # beginning and end startY &lt;- 1981 endY &lt;- 2013 # Monthly correction factors (Beck et al., 2020) #pbcorr_monthly &lt;- nc_open(dir_CorrFact) pbcorr_monthly &lt;- brick(fileCorrFact, varname=&quot;corr_fac_monthly&quot;) pbcorr_monthly_basin_longlat &lt;- raster::crop(pbcorr_monthly,aoi_CentralAsia_LatLon) # start to loop through years and months for bias correcting the monthly values. for (yr in startY:endY){ for (mon in 1:12){ # Load corresponding tmean CHELSA Central Asia File if (mon&lt;10){ chelsa_data_orig &lt;- raster(paste0(dir_CHELSA,dataType_ERA5,&#39;/CHELSA_prec_&#39;,yr,&#39;_0&#39;,mon,&#39;_CA_V1.2.1.tif&#39;)) numbPrefix &lt;- &#39;_0&#39; } else { chelsa_data_orig &lt;- raster(paste0(dir_CHELSA,dataType_ERA5,&#39;/CHELSA_prec_&#39;,yr,&#39;_&#39;,mon,&#39;_CA_V1.2.1.tif&#39;)) numbPrefix &lt;- &#39;_&#39; } chelsa_data_orig_resamp &lt;- raster::resample(chelsa_data_orig,pbcorr_monthly_basin_longlat) chelsa_data_bcorr_resamp &lt;- overlay(chelsa_data_orig_resamp,subset(pbcorr_monthly_basin_longlat,mon), fun=function(x,y){return(x*y)}) # write bias corrected CHELSA back to the disk. raster::writeRaster(chelsa_data_bcorr_resamp, paste0(dir_CHELSA,dataType_ERA5,&#39;/CHELSA_tp_bcorr_&#39;,yr,numbPrefix,mon,&#39;_CA_V1.2.1.tif&#39;), &#39;GTiff&#39;, varname = dataType_ERA5, overwrite = TRUE) #chelsa_data_bcorr_resamp %&gt;% plot() } } 8.3.3 Computing Mean Monthly Temperature Climatology from CHELSA v1.2.1 data The following code take CHELSA_V1.2.1 mean monthly temperature raster data and computes long-term mean monthly temperatures over the Central Asia domain. The resulting file can optionally be stored on the disk. fDir &lt;- &#39;../HydrologicalModeling_CentralAsia_Data/CentralAsiaDomain/CHELSA_V1.2.1/&#39; # First, we process and display mean monthly 2 meters temperatures. varDir &lt;- &#39;t2m/&#39; t2m_files &lt;- list.files(paste0(fDir,varDir)) # Get monthly fields and average over all data for resulting climatology idxSeq &lt;- seq(1,409,12) for (idx in (1:12)){ print(months[idx]) t2m_files_sel &lt;- t2m_files[idxSeq] for (idxRaster in (1:length(t2m_files_sel))){ if (idxRaster == 1){ raster_res &lt;- raster::raster(paste0(fDir,varDir,&#39;/&#39;,t2m_files_sel[1])) } else { raster_n &lt;- raster::raster(paste0(fDir,varDir,&#39;/&#39;,t2m_files_sel[idxRaster])) raster_res &lt;- raster::addLayer(raster_n,raster_res) } raster_res_mean &lt;- calc(raster_res, fun = mean) } if (idx ==1){ raster_res_mean_stack &lt;- raster_res_mean } else { raster_res_mean_stack &lt;- raster::addLayer(raster_res_mean_stack,raster_res_mean) } idxSeq &lt;- idxSeq + 1 } names(raster_res_mean_stack) &lt;- month.abb # raster::writeRaster(raster_res_mean_stack, # paste0(fDir,&#39;t2m_climatology/t2m_climatology_CA.tiff&#39;), # options=&quot;INTERLEAVE=BAND&quot;, # &#39;GTiff&#39;, # varname = &#39;t2m&#39;, # overwrite = TRUE) 8.3.4 ERA5 "],["sectin-appendix-solutions.html", "8.4 Appendix D: Solutions to exercises", " 8.4 Appendix D: Solutions to exercises 8.4.1 Exercise on Linear Reservoir modelling 8.4.1.1 Task 1 8.4.1.1.1 What will determine the flow through your bucket? The flow through the bucket will be influenced by the volume to bottom area fraction of the bucket, the amount and speed of water added to the bucket and the size of the outlet hole. 8.4.1.1.2 What do you need to measure? You will need to measure: the discharge from your bucket over time, the recharge volume (how much water you put into the bucket over a given time), the time when you start pouring water and when you stop pouring water into the bucket, the approximate volume of your bucket. 8.4.1.1.3 How can you measure it? This depends on what you have available. You can draw a water level line outside of your outflow receptacle every 10 seconds and then determine the volume change over time. Maybe you have a scale and a smart phone so you can put your outflow receptacle on the scale and make a movie of the weight change over time (1kg of water is approximately 1l of water). For the inflow pour a well defined volume over a well defined time interval. You can do this manually unless of course you have pipes and valves lying around that you can use. You will need a watch for measuring time and a receptacle with known volume to measure volumes (or a scale). A couple of notes on measurement accuracy: Generally, the larger the volumes, the smaller the relative measurement error. Say you measure a discharge of 50ml in 1s (i.e. 50ml/s) and you can read your volume with an accuracy of 5ml and the time with an accuracy of 0.2s. Your measurement uncertainty becomes 11ml/s which is more than 20% of your discharge. If on the other hand, you measure 500ml over 10s (which is the same discharge of 50ml/s) with the same inaccuracies for volume and time your measurement uncertainty for discharge becomes 1ml/s which is only 2% of your discharge. How do you estimate measurement uncertainties: Measure several times, compute the average and the standard deviation of your measurements assuming a student-t distribution. How do you combine uncertainties of volume and time to the uncertainty of discharge: By applying Gaussian error propagation. 8.4.1.1.4 What materials you will need to set up the experiment? For the bucket (the linear reservoir): A plastic bottle, a box or a can that is no longer used. It should have an opening at the top and the material should repel water and be thin enough that you can drill a hole into the wall. A pair of pointy scissors or a knife to drill a hole into the bucket. A water source (a tap, hose or a water container larger than the one above). This will be your rain machine. A watch to measure time. Note paper and pen. A receptacle for measuring the outflow. Additional material to facilitate measurement according to availability. Figure 8.28: Example for material needed and example setup to perform the linear reservoir experiment. Add a minion with a stop watch or a smart phone to help you logging the discharge from the bucket. 8.4.1.1.5 Where will you get the water from and can you re-use it after the experiment? This depends on your situation. Back to the recap section 8.4.1.2 Task 2 The video was recorded with a smart phone. The weight of the outflow receptacle was noted down every second. The discharge is computed as the change of volume in the outflow receptacle over time. Back to the recap section 8.4.1.3 Task 3 The height of the measured discharge peak can be best reproduced with k = 0.42. However, the measured discharge peaks 1s later than the simulated discharge peak. Reasons for the discrepancy can be the shape of the linear reservoir, non-linear pouring speed, and measurement uncertainties. Back to the recap section 8.4.1.4 Task 4 ToDo Back to the recap section 8.4.2 Exercises on the HBV model 8.4.2.1 Exercise: Driving forces of the HBV model The model drivers are precipitation, temperature and evaporation (P, T and ET in Figure 5.9). You need to provide time series of the model drivers to the model. Evaporation is typically not measured at climate stations but many empirical functions are available in the literature to estimate evaporation. RS Minerve offers the possibility to calculate evaporation based on temperature measurements and catchment location (you will do that later in this tutorial). Back to the HBV model section 8.4.2.2 Exercise - HBV model states The model states are the snow water equivalent height (SWE), the relative water content in the snow pack (WH), the humidity (Hum), the upper reservoir water level (SU) and the lower reservoir water level (SL). The model states are initialised using the initial conditions (see Figure 5.10). Back to the HBV model section 8.4.2.3 Exercise: Data visualization in RS Minerve Simulate from the 01/01/1981 01:00:00 to 31/12/1983 23:00:00, then choose data from 31/12/1983 23:00:00 as the initial conditions and run the model from 01/01/1984 01:00:00 to 31/12/1984 23:00:00. Choose hourly output for the simulation results. Open the Selection and plots tab by clicking on the Selection and plots button in the Modules toolbar and select simulated P and T from the Nauvalisoy station (Figure 8.29). Figure 8.29: Hourly precipitation and temperature at the virtual Nauvalisoy weather station. Note: If you want to repeat a simulation with specific initial conditions, you can store them through Export IC in the Model Properties toolbar. The approximate temperature range is -13 in December to 34 deg C in August. The annual precipitation is about 1.4m (visualize Pcum and click on the last value of the time series). No precipitation falls during the summer months. Back to the Nauvalisoy model setup section. 8.4.2.4 Exercise: Compare evaporation methods Figure 8.30 shows the evaporation computed with various methods and the resulting discharge. Uniform evaporation should not be used for sub-annual modeling time steps. The difference between the methods by Turc, McGuinness and Oudin are within 5% of total discharge which is negligible for a regional model. Figure 8.30: Hourly precipitation and temperature at the virtual Nauvalisoy weather station. For advanced modeling, the choice of the evaporation model may be relevant but only if a validation with measured data is possible. Back to the Nauvalisoy model setup section. 8.4.2.5 Exercise: Common difficulties in model calibration Especially fully and semi-distributed hydrological models are typically over-parameterized, i.e. the number of model parameters is much larger than the number of observations for the model states. The true parameter values of the system cannot be uniquely identified based on a discharge time series alone. The outcome of the calibration depends on the measure of similarity between the simulated and the measured discharge. The model is calibrated against historical data. Its ability to predict future discharge may be limited. The model is not perfect, it remains an approximation of the real system and may not incorporate all relevant processes of the hydrologic cycle (e.g. water storage and transport in glaciers for the case of the HBV model or significant sub-surface water fluxes that are unknown (for example in Karst regions)). Discharge measurements typically have uncertainties of 20%. Particularly measurements at the lower and upper ends of the rating curve (i.e. the water table - discharge relationship) are typically prone to larger uncertainties (bonus question: think about why this is so!). Is the measurement location or the equipment not properly maintained, biases may grow over time. On the other hand, if the measurement method is updated and changed, the measured discharge may display a different pattern (for an example, look at the low flow of the discharge of the river Gunt in Figure 3.2). Back to the calibration section 8.4.2.6 Exercise: Strategies to overcome some of the model calibration difficulties Over-parameterization: Consider simplifying the model, i.e. reducing the number of parameters. If the model complexity is required, Collect data to verify individual fluxes of the model components (e.g. soil parameters, snow water equivalent, etc.). As physical measurements in the field are not always possible you may have to become creative here, e.g. use MODIS snow cover data to validate the snow/no snow partitioning of the HBV model. Also consult the literature for parameterizations of similar catchments. Use a combination of similarity measures. This will be demonstrated later on in the model calibration section. Exclude part of your data set from model calibration and use it for model validation. If the model does not perform well in the validation period, its parameters are too specific for the calibration period and the model is said to not generalize well. If this happens you should try to reduce the number of parameters in your model. To understand better which parameters are responsible for the over-fit of the historical discharge, use different calibration and validation periods and compare the resulting parameters. Through sensitivity analysis, identify the model components that are most sensitive to predicted changes of model forcings, geometry or parameterization and perform scenario analysis. Implement and validate multiple possible conceptual models. All of the models must be calibrated and validated individually. Square-root filters or data assimilation algorithms are able to account for non-correlated measurement errors (they are not implemented in RS Minerve and not topic of this course). Error bands for the measurements should be adapted when communicating model results. Most of the above points will be discussed in more detail during this course. Back to the calibration section 8.4.2.7 Exercise: Calibrate a simple HBV model {#section-appendix-solutions-calibrated parameters} The parameter set of the calibrated model are: ## ## ## | | | ## |:---------------|-----:| ## |CFMax (mm/°C/d) | 0.50| ## |CFR (-) | 0.05| ## |CWH (-) | 0.10| ## |TT (°C) | 3.00| ## |TTInt (°C) | 3.00| ## |TTSM (°C) | 0.00| ## |Beta (-) | 2.50| ## |FC (mm) | 20.00| ## |PWP (-) | 0.50| ## |SUMax (mm) | 10.00| ## |Kr (1/d) | 0.09| ## |Ku (1/d) | 0.02| ## |Kl (1/d) | 0.00| ## |Kperc (1/d) | 0.00| You can import the calibrated parameters via Import P in the Model Properties toolbar (the calibrated parameters are available in ./data/SyrDarya/Chirchiq/RSMinerve/nauvalisoy_PAR_calibrated.txt. Back to the practical calibration section "],["References.html", "Chapter 9 References", " Chapter 9 References A., Cancelliere. 2019. “Statistical Analysis of Hydrologic Variables.” In, edited by Stedinger Teegavarapu R. S. V. Salas J. D., 203–29. ASCE. Aizen, V B, E M Aizen, and V A Kuzmichonok. 2007. “Glaciers and Hydrological Changes in the Tien Shan: Simulation and Prediction.” Environmental Research Letters 2 (4): 045019. https://doi.org/10.1088/1748-9326/2/4/045019. Arora, Vivek K. 2002. “The Use of the Aridity Index to Assess Climate Change Effect on Annual Runoff.” Journal of Hydrology 265 (1): 164–77. https://doi.org/https://doi.org/10.1016/S0022-1694(02)00101-4. Beck, Hylke E., Eric F. Wood, Tim R. McVicar, Mauricio Zambrano-Bigiarini, Camila Alvarez-Garreton, Oscar M. Baez-Villanueva, Justin Sheffield, and Dirk N. Karger. 2019. “Bias correction of global high-resolution precipitation climatologies using streamflow observations from 9372 catchments Bias correction of global high-resolution precipitation climatologies using streamflow observations from 9372 catchments.” Journal of Climate 33 (4): 1299–1315. https://doi.org/10.1175/jcli-d-19-0332.1. ———. 2020a. “Bias Correction of Global High-Resolution Precipitation Climatologies Using Streamflow Observations from 9372 Catchments.” Journal of Climate 33 (4): 1299–1315. https://doi.org/10.1175/JCLI-D-19-0332.1. ———. 2020b. “Bias Correction of Global High-Resolution Precipitation Climatologies Using Streamflow Observations from 9372 Catchments.” Journal of Climate 33 (4): 1299–1315. https://doi.org/10.1175/JCLI-D-19-0332.1. ———. 2020c. “Bias Correction of Global High-Resolution Precipitation Climatologies Using Streamflow Observations from 9372 Catchments.” Journal of Climate 33 (4): 1299–1315. https://doi.org/10.1175/JCLI-D-19-0332.1. ———. 2020d. “Bias Correction of Global High-Resolution Precipitation Climatologies Using Streamflow Observations from 9372 Catchments.” Journal of Climate 33 (4): 1299–1315. https://doi.org/10.1175/JCLI-D-19-0332.1. Bergström, Sten. 1980. “Development and Application of a Conceptual Runoff Model for Scandinavian Catchments.” SMHI Report Nr Rho 7. Lund Institute of Technology/University of Lund. Birt, A. G., M. R. Valdez-Vivas, R. M. Feldman, C. W. Lafon, D. Cairns, R. N. Coulson, M. Tchakerian, W. Xi, and J. M. Guldin. 2010. “A Simple Stochastic Weather Generator for Ecological Modeling.” Environmental Modelling &amp; Software 25 (10): 1252–55. https://doi.org/https://doi.org/10.1016/j.envsoft.2010.03.006. Buisán, Samuel T., Michael E. Earle, José Luı́s Collado, John Kochendorfer, Javier Alastrué, Mareile Wolff, Craig D. Smith, and Juan I. López-Moreno. 2017. “Assessment of Snowfall Accumulation Underestimation by Tipping Bucket Gauges in the Spanish Operational Network.” Atmospheric Measurement Techniques 10 (3): 1079–91. https://doi.org/10.5194/amt-10-1079-2017. Central Intelligence Agency. n.d. “The World Factbook 2020.” https://www.cia.gov/library/publications/resources/the-world-factbook/index.html. CREALP. 2021. RS MINERVE (version 2.9.1.0). https://www.crealp.ch/fr/accueil/outils-services/logiciels/rs-minerve/telechargement-rsm.html. Foehn, A., J. Garcia Hernandez, B. Roquier, J. Fluixa-Sanmartin, T. Brauchli, J. Paredes Arquiola, and G. De Cesare. 2020. “RS MINERVE - User Manual, V2.15.” ISSN 2673-2653. Switzerland: Ed. CREALP. Garcia Hernandez, J., A. Foehn, J. Fluixa-Sanmartin, B. Roquier, T. Brauchli, J. Paredes Arquiola, and De Cesare G. 2020. “RS MINERVE - Technical Manual, V2.25.” ISSN 2673-2661. Switzerland: Ed. CREALP. Gerlitz, Lars, Eva Steirou, Christoph Schneider, Vincent Moron, Sergiy Vorogushyn, and Bruno Merz. 2018. “Variability of the Cold Season Climate in Central Asia. Part I: Weather Types and Their Tropical and Extratropical Drivers.” Journal of Climate 31 (18): 7185–7207. https://doi.org/10.1175/jcli-d-17-0715.1. GLIMS, and NSIDC. 2005, updated 2018. Global Land Ice Measurements from Space Glacier Database. Compiled and made available by the international GLIMS community and the National Snow and Ice Data Center, Boulder CO, U.S.A. DOI:10.7265/N5V98602. Karger, Dirk Nikolaus, Olaf Conrad, Jürgen Böhner, Tobias Kawohl, Holger Kreft, Rodrigo Wilber Soria-Auza, Niklaus E. Zimmermann, H. Peter Linder, and Michael Kessler. 2017. “Climatologies at High Resolution for the Earth’s Land Surface Areas.” Scientific Data 4 (1). https://doi.org/10.1038/sdata.2017.122. Karger, Dirk Nikolaus, Dirk R. Schmatz, Gabriel Dettling, and Niklaus E. Zimmermann. 2020. “High-Resolution Monthly Precipitation and Temperature Time Series from 2006 to 2100.” Scientific Data 7 (1). https://doi.org/10.1038/s41597-020-00587-y. Lindström, Göran, Barbro Johansson, Magnus Persson, Marie Gardelin, and Sten Bergström. 1997. “Development and Test of the Distributed HBV-96 Hydrological Model.” Journal of Hydrology 201 (1-4): 272–88. https://doi.org/10.1016/S0022-1694(97)00041-3. Ning, Tingting, Zhi Li, Qi Feng, Wenzhao Liu, and Zongxing Li. 2018. “Comparison of the effectiveness of four Budyko-based methods in attributing long-term changes in actual evapotranspiration.” Scientific Reports 8 (1): 12665. https://doi.org/10.1038/s41598-018-31036-x. Pedersen, John T., John C. Peters, and Otto J. Helweg. 1980. “Hydrographs by Single Linear Reservoir Model.” Technical Report TP-74. US Army Corps of Engineers, Institute of Water Resources, Hydrologic Engineering Center. Pravilova, Ekaterina. 2009. “River of Empire: Geopolitics, Irrigation, and the Amu Darya in the Late XIXth Century.” Cahiers d’Asie Centrale 17/18: 255–87. QGIS Development Team. 2021. QGIS Geographic Information System. QGIS Association. R Core Team. 2013. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. http://www.R-project.org/. Shults, Victor. 1965. Rivers of Middle Asia. 2nd Edition. Gidrometeoizdat, Leningrad. “Srtmgl1 n -ASA SRTM Version 3.0.” 2020. https://earthdata.nasa.gov/learn/articles/nasa-shuttle-radar-topography-mission-srtm-version-3-0-global-1-arc-second-data-released-over-asia-and-australia. Team’, ’RStudio. 2020. RStudio: Integrated Development Environment for r. Boston, MA: RStudio, PBC. Trabucco, Antonio, and Robert Zomer. 2019. “Global Aridity Index and Potential Evapotranspiration (ET0) Climate Database v2,” January. https://doi.org/10.6084/m9.figshare.7504448.v3. Vuuren, Detlef P. van, Jae Edmonds, Mikiko Kainuma, Keywan Riahi, Allison Thomson, Kathy Hibbard, George C. Hurtt, et al. 2011. “The Representative Concentration Pathways: An Overview.” Climatic Change 109 (1-2): 5–31. https://doi.org/10.1007/s10584-011-0148-z. Zhang, D., Z. Cong, G. Ni, D. Yang, and S. Hu. 2015. “Effects of snow ratio on annual runoff within the Budyko framework.” Hydrology and Earth System Sciences 19 (4): 1977–92. https://doi.org/10.5194/hess-19-1977-2015. Zhou, Sha, Bofu Yu, Lu Zhang, Yuefei Huang, Ming Pan, and Guangqian Wang. 2016. “A new method to partition climate and catchment effect on the mean annual runoff based on the Budyko complementary relationship.” Water Resources Research 52 (9): 7163–77. https://doi.org/10.1002/2016wr019046. "]]
