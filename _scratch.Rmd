---
title: "scratch"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Required Packages

```{r,message=FALSE}
# navigation
library(here)

# data wrangling
library(tidyverse)

# timeseries bliss
library(timetk)
library(zoo)
library(lubridate)
library(pracma)

# missing data
library(naniar)
library(visdat)
library(simputation)

# modeling
library(modeltime)
library(modeltime.ensemble)
library(randomForest)
library(tidymodels)
```

```{r}
#devtools::install_github("hydrosolutions/riversCentralAsia",force = TRUE)
library(riversCentralAsia)
```

# HOUSEKEEPING

## Install Package Locally

In the package project, do this...

```{r}
devtools::document()
devtools::install()
```

## Unload riversCentralAsia Package and make available anew and load

... and here, first detach and then load the newly made available package.

```{r}
detach("package:riversCentralAsia", unload=TRUE)
library(riversCentralAsia)
```

## Make Raw Data Available in Your Package

Store the files in the corresponding folder

    ~/Dropbox (hydrosolutions)/1_HSOL_PROJECTS/PROJECTS/SDC/DKU_WRM_COURSE_CA/Course Materials/Code/riversCentralAsia/inst/extdata

For exporting, write as csv.

```{r}
write_csv(data,'~/Dropbox (hydrosolutions)/1_HSOL_PROJECTS/PROJECTS/SDC/DKU_WRM_COURSE_CA/Course Materials/Code/riversCentralAsia/inst/extdata/data_ChirchikRiver')
```

# DATA PREPARATION

## Discharge Data

Let us load the available timeseries from all the stations that we have data from in the larger Chirchik River Basin and its tributaries.

```{r}
fPath <- '/Users/tobiassiegfried/Dropbox (hydrosolutions)/1_HSOL_PROJECTS/PROJECTS/SDC/DKU_WRM_COURSE_CA/Course Materials/Data/Q/UZB/'
## Chatkal River, Khudaydod
Q_16279 <- loadTabularData(fPath,'16279_decadal.csv',16279,'Khudaydod','Chatkal','Chirchik','Q','m3s')
## Pskem River, Mullala
Q_16290 <- loadTabularData(fPath,'16290_decadal.csv',16290,'Mullala', 'Pskem','Chirchik','Q','m3s')
## Inflow Charvak
Q_16924 <- loadTabularData(fPath,'16924_decadal.csv',16924,'Lake_Inflow','Chatkal','Chirchik','Q','m3s')
## Nauvalisoy River, Sidzhak
Q_16298 <- loadTabularData(fPath,'16298_decadal.csv',16298,'Sidzhak','Nauvalisoy','Chirchik','Q','m3s')
## Chirchik River, Chinaz
Q_16275 <- loadTabularData(fPath,'16275_monthly.csv',16275,'Chinaz','Chirchik','Chirchik','Q','m3s')
## Ugam River, Khodizhikent
Q_16300 <- loadTabularData(fPath,'16300_monthly.csv',16300,'Khodzhikent','Ugam','Chirchik','Q','m3s')
## Chirchik, Gazalkent
Q_16262 <- loadTabularData(fPath,'16262_monthly.csv',16262,'Gazalkent','Chirchik','Chirchik','Q','m3s')


# Assemble the data
data.Q <- Q_16279
data.Q <- bind_rows(data.Q,Q_16290,Q_16924,Q_16298,Q_16300,Q_16275,Q_16262)
data.Q %>% group_by(code,type,resolution) %>% plot_time_series(date,data,.facet_ncol = 2,.interactive = FALSE)

Q_16924 %>% plot_time_series(date,data,.facet_ncol = 2,.interactive = FALSE)

```

## Meteorological Data

```{r}
fPath <- '/Users/tobiassiegfried/Dropbox (hydrosolutions)/1_HSOL_PROJECTS/PROJECTS/SDC/DKU_WRM_COURSE_CA/Course Materials/Data/METEO/UZB/'

## Pskem Meteo Station
P_38462 <- loadTabularData(fPath,'38462_P_decadal.csv',38462,'Pskem Meteo Station','Pskem','Chirchik','P','mm')
T_38462 <- loadTabularData(fPath,'38462_T_decadal.csv',38462,'Pskem Meteo Station','Pskem','Chirchik','T','degC')

## Charvak Reservoir Meteo Station
P_38464 <- loadTabularData(fPath,'38464_P_decadal.csv',38464,'Charvak Reservoir Meteo Station','Chirchik','Chirchik','P','mm')

## Chatkal Meteo Station
P_38471 <- loadTabularData(fPath,'38471_P_decadal.csv',38471,'Chatkal Meteo Station','Chatkal','Chirchik','P','mm')
T_38471 <- loadTabularData(fPath,'38471_T_decadal.csv',38471,'Chatkal Meteo Station','Chatkal','Chirchik','T','degC')

# Assemble the data
data.PT <- P_38462
data.PT <- bind_rows(data.PT,T_38462,P_38464,P_38471,T_38471)
data.PT %>% group_by(code,type) %>% plot_time_series(date,data,.facet_ncol = 1,.interactive = FALSE)
```

Now, we are ready to merge all data into one big table.

```{r}
data <- bind_rows(data.Q,data.PT)
data %>% filter(type=='Q') %>% group_by(type,code,station) %>% plot_time_series(date,data,.facet_ncol = 2,.interactive = FALSE)
```

## Gauging Station Data

```{r}
gaugingStationData <- data.Q %>% select(code) %>% unique()

gaugingStationData$lon_UTM42 <- c(
                            598278, 
                            597351, 
                            584616, 
                            589674, 
                            578612, 
                            479463,
                            564916)

gaugingStationData$lat_UTM42 <- c(
                            4596628, 
                            4622724,
                            4609108, 
                            4618690, 
                            4610070, 
                            4528236,
                            4602785)

gaugingStationData$altitude_masl <- c(
                            953, 
                            911, 
                            864, 
                            941, 
                            759, 
                            254,
                            681)

gaugingStationData$basinSize_sqkm <- c(
                                  5677829056, 
                                  2521388472, 
                                  9999165900, 
                                  98555481, 
                                  864946131, 
                                  13112062045,
                                  11139775526)
gaugingStationData$basinSize_sqkm <- gaugingStationData$basinSize_sqkm / 10^6
gaugingStationData

```

Test join tibbles together

```{r}
data <- full_join(data,gaugingStationData,by='code')
```

## Store Data in Package as .rds-file

Save data in a temp location.

```{r}
ChirchikRiverBasin <- data
fLoc <- '/Users/tobiassiegfried/Dropbox (hydrosolutions)/1_HSOL_PROJECTS/PROJECTS/SDC/DKU_WRM_COURSE_CA/Course Materials/Handbook/Applied_Hydrological_Modeling_Bookdown/data/ChirchikRiverBasin.rda'
saveRDS(ChirchikRiverBasin,fLoc)
```

# Koksu Ungauged Contribution

```{r,message=FALSE}
fPath <- '/Users/tobiassiegfried/Dropbox (hydrosolutions)/1_HSOL_PROJECTS/PROJECTS/SDC/DKU_WRM_COURSE_CA/Course Materials/Data/Q/UZB/Kosku_Discharge_Derviation.csv'
koksu <- read_csv(fPath)
#koksu$Q_Koksu <- koksu$Q_Charvak_Gauge - koksu$Q_Khudaydod

# Plot the data
#ggplot(koksu, aes(Q_Khudaydod, Q_Koksu)) +
#  geom_point() +
#  geom_smooth(method = "lm")
```

We can get more information about the relationship between the two discharge stations in the following way.

```{r}
lmKoksu <- lm(Q_Koksu ~ Q_Khudaydod,koksu)
summary(lmKoksu)
```

## Make these data also available in package

First save here:

```{r}
KoksuDischarge <- koksu
fLoc <- '/Users/tobiassiegfried/Dropbox (hydrosolutions)/1_HSOL_PROJECTS/PROJECTS/SDC/DKU_WRM_COURSE_CA/Course Materials/Handbook/Applied_Hydrological_Modeling_Bookdown/data/KoksuDischarge.rda'
saveRDS(KoksuDischarge,fLoc)
```

# FINAL PACKAGE Deployment

Go to package and get this following code executed:

```{r}
fLoc <- '/Users/tobiassiegfried/Dropbox (hydrosolutions)/1_HSOL_PROJECTS/PROJECTS/SDC/DKU_WRM_COURSE_CA/Course Materials/Handbook/Applied_Hydrological_Modeling_Bookdown/data/ChirchikRiverBasin.rda'
ChirchikRiverBasin <- readRDS(fLoc)
fLoc <- '/Users/tobiassiegfried/Dropbox (hydrosolutions)/1_HSOL_PROJECTS/PROJECTS/SDC/DKU_WRM_COURSE_CA/Course Materials/Handbook/Applied_Hydrological_Modeling_Bookdown/data/KoksuDischarge.rda'
KoksuDischarge <- readRDS(fLoc)
library(usethis)
usethis::use_data(ChirchikRiverBasin,KoksuDischarge,overwrite = TRUE)
```

Finally, rerun the documentation and re-install.

```{r}
devtools::document()
devtools::install()
```

... and then release on github.

# f() DEV

## function decadal2monthly.R

Here, we want to develop a function that takes a 10-day decadal timeseries and converts it to a monthly timeseries, depending on the data type under consideration. Actually, we might not even need this - check timetk or other time-related packages....

```{r}

```

## function daily2decadal.R

```{r}

```

## function thomasFieringModel.R

```{r}

```

# EDA

## Select Subset of Data

```{r}
zorf <- data %>% filter(type=='Q',code!=16298,code!=16275,code!=16300,code!=16294,code!=16290)
zorf_pad <- zorf %>% pad_by_time(date) 
zorf %>% plot_time_series(date,data,.facet_ncol = 1,.interactive = FALSE, .smooth = FALSE)
```

## TS Summary Diagnostics

```{r}
zorf %>% tk_summary_diagnostics()
zorf %>% plot_time_series(date,data)
```

## Missing Data

```{r}
library(naniar)
vis_miss(zorf)
```

Replace those values with norm. Note, this procedure, if applied on a large set of missing values, reduces the characteristics of the time series.

```{r}
zorf$data[is.na(zorf$data)] = zorf$norm[is.na(zorf$data)]
vis_miss(zorf)
```

# Forecasting

## Train/Test Sets Splitting

```{r}
evaluation_tbl <- zorf %>% select(date, data)
splits <- evaluation_tbl %>% time_series_split(date_var = date,
                                             assess = "10 years",
                                             cumulative = TRUE)
splits
splits_plan <- splits %>% tk_time_series_cv_plan() 
splits_plan %>% plot_time_series_cv_plan(date,data,)
```

Red set is testing, black set is training set.

## Forecasting with Prophet using Modeltime/Parsnip Packages

```{r}
?prophet_reg
model_prophet_fit <- prophet_reg() %>% 
  set_engine("prophet") %>% 
  fit(data ~ date, data = training(splits))
model_prophet_fit
```

### Modeltime Process = Forecasting Workflow

```{r}
# Setup modeltime Table
model_tbl <- modeltime_table(
  model_prophet_fit
)

#Calibrate (validate?) on hold out dataset
calibration_tbl <- model_tbl %>% 
  modeltime_calibrate(
    new_data = testing(splits)
  )

# Forecast
calibration_tbl %>% modeltime_forecast(actual_data = evaluation_tbl) %>% 
  plot_modeltime_forecast()
```

### Accuracy

```{r}
calibration_tbl %>% modeltime_accuracy()
```

## Forecasting with Feature Engineering

### Identifying Possible Features

We are using the common log transformation for timeseries to visualize 'possible' features better.

```{r}
evaluation_tbl %>% plot_seasonal_diagnostics(date,data %>% log(),.feature_set = c('week','month.lbl','year'))
```

Week and month definitely seem to be interesting features. Let us keep that in mind.

### Setup Preprocessing Pipeline (Recipes)

```{r}
#training(splits)

recipe_spec <- recipe(data ~ ., training(splits)) %>% 
  step_timeseries_signature(date) %>% 
  # Cleaning up required since some features are not needed here when dealing with a decadal dataset.
  step_rm(
    ends_with(".iso"),ends_with(".xts"), contains("hour"),
      contains("minute"),contains("second"),contains("am.pm")
    ) %>%
  step_normalize(
      ends_with("index.num"),ends_with("_year")
    ) %>% 
  step_dummy(all_nominal())

recipe_spec %>% prep() %>% juice() %>% glimpse()
```

### Machine Learning Specs

```{r}
model_spec <- linear_reg() %>% 
  set_engine("lm")

# Now, we create workflow object
workflow_fit_lm <- workflow() %>% 
  add_model(model_spec) %>% 
  add_recipe(recipe_spec) %>% 
  fit(training(splits))

workflow_fit_lm
```

### Compare Models

```{r}
calibration_tbl <- modeltime_table(
  model_prophet_fit,
  workflow_fit_lm
) %>% 
  modeltime_calibrate(testing(splits))

calibration_tbl

calibration_tbl %>% modeltime_accuracy()

calibration_tbl %>% 
  modeltime_forecast(
    new_data = testing(splits), actual_data = evaluation_tbl
  ) %>% 
  plot_modeltime_forecast()
```
